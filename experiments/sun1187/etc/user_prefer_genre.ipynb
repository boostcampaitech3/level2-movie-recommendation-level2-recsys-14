{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7CfnRw7U59C"
      },
      "source": [
        "## 1. 초기 세팅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWWEf1mPKdnX",
        "outputId": "f85f098b-72f1-43d7-ff10-2e44357f9e42"
      },
      "outputs": [],
      "source": [
        "## 전처리과정에서 pandas의 버전에 다르게 동작하는 경향이 보여, 이 미션에서는 아래 버전으로 사용하도록하겠습니다.\n",
        "#!pip install pandas==1.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bQj6k1mSbxaz"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from scipy import sparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ3W0udmbxa3",
        "outputId": "4bad9428-5957-4e22-ea4f-1f6f67b1c25e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## 각종 파라미터 세팅\n",
        "parser = argparse.ArgumentParser(description='PyTorch Variational Autoencoders for Collaborative Filtering')\n",
        "\n",
        "\n",
        "parser.add_argument('--data', type=str, default='/opt/ml/input/data/train/',\n",
        "                    help='Movielens dataset location')\n",
        "\n",
        "parser.add_argument('--lr', type=float, default=1e-4,\n",
        "                    help='initial learning rate')\n",
        "parser.add_argument('--wd', type=float, default=0.00,\n",
        "                    help='weight decay coefficient')\n",
        "parser.add_argument('--batch_size', type=int, default=500,\n",
        "                    help='batch size')\n",
        "parser.add_argument('--epochs', type=int, default=20, #원래 20\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--total_anneal_steps', type=int, default=200000,\n",
        "                    help='the total number of gradient updates for annealing')\n",
        "parser.add_argument('--anneal_cap', type=float, default=0.2,\n",
        "                    help='largest annealing parameter')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_true',\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--log_interval', type=int, default=100, metavar='N',\n",
        "                    help='report interval')\n",
        "parser.add_argument('--save', type=str, default='model.pt',\n",
        "                    help='path to save the final model')\n",
        "args = parser.parse_args([])\n",
        "\n",
        "# Set the random seed manually for reproductibility.\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#만약 GPU가 사용가능한 환경이라면 GPU를 사용\n",
        "if torch.cuda.is_available():\n",
        "    args.cuda = True\n",
        "\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o1fvXqFWE_G"
      },
      "source": [
        "##2. 데이터 전처리\n",
        "\n",
        "이 부분에서 진행되는 과정은 저희가 일반적으로 알고있는 MovieLens (user, item, timestamp)데이터를 전처리하는 과정입니다. 전처리 과정의 다양한 옵션들을 구성하기 위해 약간 복잡하게 되었지만, \n",
        "결과적으로는, 유저들의 특정한 아이템들을 따로 분리를 해서, 그 분리된 값을 모델이 예측할 수 있냐를 확인하기 위한 전처리 과정이라고 보시면 되겠습니다.\n",
        "실제로 나오는 데이터셋을 확인하면 더욱 이해가 빠를것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cgvNoy1Ybxa6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "\n",
        "def get_count(tp, id):\n",
        "    playcount_groupbyid = tp[[id]].groupby(id)#, as_index=False)\n",
        "    count = playcount_groupbyid.size()\n",
        "\n",
        "    return count\n",
        "\n",
        "# 특정한 횟수 이상의 리뷰가 존재하는(사용자의 경우 min_uc 이상, 아이템의 경우 min_sc이상) \n",
        "# 데이터만을 추출할 때 사용하는 함수입니다.\n",
        "# 현재 데이터셋에서는 결과적으로 원본그대로 사용하게 됩니다.\n",
        "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
        "    if min_sc > 0:\n",
        "        itemcount = get_count(tp, 'item')\n",
        "        tp = tp[tp['item'].isin(itemcount.index[itemcount >= min_sc])]\n",
        "\n",
        "    if min_uc > 0:\n",
        "        usercount = get_count(tp, 'user')\n",
        "        tp = tp[tp['user'].isin(usercount.index[usercount >= min_uc])]\n",
        "\n",
        "    usercount, itemcount = get_count(tp, 'user'), get_count(tp, 'item')\n",
        "    return tp, usercount, itemcount\n",
        "\n",
        "#훈련된 모델을 이용해 검증할 데이터를 분리하는 함수입니다.\n",
        "#100개의 액션이 있다면, 그중에 test_prop 비율 만큼을 비워두고, 그것을 모델이 예측할 수 있는지를\n",
        "#확인하기 위함입니다.\n",
        "def split_train_test_proportion(data, test_prop=0.2): #원래 0.2\n",
        "    data_grouped_by_user = data.groupby('user')\n",
        "    tr_list, te_list = list(), list()\n",
        "\n",
        "    np.random.seed(98765)\n",
        "    \n",
        "    for _, group in data_grouped_by_user:\n",
        "        n_items_u = len(group)\n",
        "        \n",
        "        if n_items_u >= 5:\n",
        "            idx = np.zeros(n_items_u, dtype='bool')\n",
        "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "\n",
        "            tr_list.append(group[np.logical_not(idx)])\n",
        "            te_list.append(group[idx])\n",
        "        \n",
        "        else:\n",
        "            tr_list.append(group)\n",
        "    \n",
        "    data_tr = pd.concat(tr_list)\n",
        "    data_te = pd.concat(te_list)\n",
        "\n",
        "    return data_tr, data_te\n",
        "\n",
        "def numerize(tp, profile2id, show2id):\n",
        "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
        "    sid = tp['item'].apply(lambda x: show2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVFoRHrmVQsp",
        "outputId": "00dc5cf4-769a-4d56-b536-97280d00b99e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load and Preprocess Movielens dataset\n",
            "원본 데이터\n",
            "            user   item        time\n",
            "0            11   4643  1230782529\n",
            "1            11    170  1230782534\n",
            "2            11    531  1230782539\n",
            "3            11    616  1230782542\n",
            "4            11   2140  1230782563\n",
            "...         ...    ...         ...\n",
            "5154466  138493  44022  1260209449\n",
            "5154467  138493   4958  1260209482\n",
            "5154468  138493  68319  1260209720\n",
            "5154469  138493  40819  1260209726\n",
            "5154470  138493  27311  1260209807\n",
            "\n",
            "[5154471 rows x 3 columns]\n",
            "5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\n",
            "            user   item        time\n",
            "0            11   4643  1230782529\n",
            "1            11    170  1230782534\n",
            "2            11    531  1230782539\n",
            "3            11    616  1230782542\n",
            "4            11   2140  1230782563\n",
            "...         ...    ...         ...\n",
            "5154466  138493  44022  1260209449\n",
            "5154467  138493   4958  1260209482\n",
            "5154468  138493  68319  1260209720\n",
            "5154469  138493  40819  1260209726\n",
            "5154470  138493  27311  1260209807\n",
            "\n",
            "[5154471 rows x 3 columns]\n",
            "유저별 리뷰수\n",
            " user\n",
            "11        376\n",
            "14        180\n",
            "18         77\n",
            "25         91\n",
            "31        154\n",
            "         ... \n",
            "138473     63\n",
            "138475    124\n",
            "138486    137\n",
            "138492     68\n",
            "138493    314\n",
            "Length: 31360, dtype: int64\n",
            "아이템별 리뷰수\n",
            " item\n",
            "1         12217\n",
            "2          3364\n",
            "3           734\n",
            "4            43\n",
            "5           590\n",
            "          ...  \n",
            "118700       54\n",
            "118900       60\n",
            "118997       52\n",
            "119141      122\n",
            "119145       78\n",
            "Length: 6807, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Load and Preprocess Movielens dataset\")\n",
        "# Load Data\n",
        "\n",
        "DATA_DIR = args.data\n",
        "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train_ratings.csv'), header=0)\n",
        "print(\"원본 데이터\\n\", raw_data)\n",
        "\n",
        "# Filter Data\n",
        "raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=5, min_sc=0)\n",
        "#제공된 훈련데이터의 유저는 모두 5개 이상의 리뷰가 있습니다.\n",
        "print(\"5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\\n\",raw_data)\n",
        "\n",
        "print(\"유저별 리뷰수\\n\",user_activity)\n",
        "print(\"아이템별 리뷰수\\n\",item_popularity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T1dTsWUrffP",
        "outputId": "3aa98010-8b5f-4845-e405-fe9248bd614e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(BEFORE) unique_uid: Int64Index([    11,     14,     18,     25,     31,     35,     43,     50,\n",
            "                58,     60,\n",
            "            ...\n",
            "            138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486,\n",
            "            138492, 138493],\n",
            "           dtype='int64', name='user', length=31360)\n",
            "(AFTER) unique_uid: Int64Index([ 27968,  67764,   2581,  82969, 137831,  48639,  97870,  40424,\n",
            "             46835,  79570,\n",
            "            ...\n",
            "            114284,   9009,  21165,  33920,  22054, 135379, 125855,  41891,\n",
            "             15720,  17029],\n",
            "           dtype='int64', name='user', length=31360)\n",
            "훈련 데이터에 사용될 사용자 수: 25088\n",
            "검증 데이터에 사용될 사용자 수: 3136\n",
            "테스트 데이터에 사용될 사용자 수: 3136\n"
          ]
        }
      ],
      "source": [
        "# Shuffle User Indices\n",
        "unique_uid = user_activity.index\n",
        "print(\"(BEFORE) unique_uid:\",unique_uid)\n",
        "np.random.seed(98765)\n",
        "idx_perm = np.random.permutation(unique_uid.size)\n",
        "unique_uid = unique_uid[idx_perm]\n",
        "print(\"(AFTER) unique_uid:\",unique_uid)\n",
        "\n",
        "n_users = unique_uid.size #31360\n",
        "n_heldout_users = 3136#3000\n",
        "\n",
        "\n",
        "# Split Train/Validation/Test User Indices\n",
        "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
        "te_users = unique_uid[(n_users - n_heldout_users):]\n",
        "\n",
        "#주의: 데이터의 수가 아닌 사용자의 수입니다!\n",
        "print(\"훈련 데이터에 사용될 사용자 수:\", len(tr_users))\n",
        "print(\"검증 데이터에 사용될 사용자 수:\", len(vd_users))\n",
        "print(\"테스트 데이터에 사용될 사용자 수:\", len(te_users))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yBsRCRqtPz6",
        "outputId": "94fa6b78-63b2-4b70-d7e4-fab0ebaded58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "##훈련 데이터에 해당하는 아이템들\n",
        "#Train에는 전체 데이터를 사용합니다.\n",
        "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
        "\n",
        "##아이템 ID\n",
        "unique_sid = pd.unique(train_plays['item'])\n",
        "\n",
        "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
        "\n",
        "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "\n",
        "if not os.path.exists(pro_dir):\n",
        "    os.makedirs(pro_dir)\n",
        "\n",
        "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "    for sid in unique_sid:\n",
        "        f.write('%s\\n' % sid)\n",
        "\n",
        "#Validation과 Test에는 input으로 사용될 tr 데이터와 정답을 확인하기 위한 te 데이터로 분리되었습니다.\n",
        "vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
        "vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
        "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
        "\n",
        "test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
        "test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
        "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
        "\n",
        "\n",
        "\n",
        "train_data = numerize(train_plays, profile2id, show2id)\n",
        "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
        "\n",
        "\n",
        "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
        "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
        "\n",
        "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
        "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
        "\n",
        "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
        "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
        "\n",
        "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
        "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkdg2OkjqVUM",
        "outputId": "7e9694eb-ff67-4d5c-efd9-28be4b6f79bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           uid   sid\n",
            "0        11825     0\n",
            "1        11825     1\n",
            "2        11825     2\n",
            "3        11825     3\n",
            "4        11825     4\n",
            "...        ...   ...\n",
            "5154466  10783   477\n",
            "5154467  10783  1325\n",
            "5154468  10783   331\n",
            "5154469  10783   558\n",
            "5154470  10783  1922\n",
            "\n",
            "[4125303 rows x 2 columns]\n",
            "           uid   sid\n",
            "376      26554   440\n",
            "377      26554   741\n",
            "378      26554  1407\n",
            "379      26554   193\n",
            "380      26554  1041\n",
            "...        ...   ...\n",
            "5153247  26934   760\n",
            "5153248  26934   697\n",
            "5153249  26934  3232\n",
            "5153250  26934  1369\n",
            "5153251  26934  3679\n",
            "\n",
            "[415395 rows x 2 columns]\n",
            "           uid   sid\n",
            "382      26554  3012\n",
            "383      26554  1681\n",
            "384      26554   201\n",
            "399      26554  3177\n",
            "401      26554  3289\n",
            "...        ...   ...\n",
            "5153229  26934   737\n",
            "5153233  26934   228\n",
            "5153236  26934   235\n",
            "5153240  26934  3962\n",
            "5153243  26934  1086\n",
            "\n",
            "[102295 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "#데이터 셋 확인\n",
        "print(train_data)\n",
        "print(vad_data_tr)\n",
        "print(vad_data_te)\n",
        "# print(test_data_tr)\n",
        "# print(test_data_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMiq9leyWWL1"
      },
      "source": [
        "##3. 데이터 로더 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nxUADr9ibxa8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataLoader():\n",
        "    '''\n",
        "    Load Movielens dataset\n",
        "    '''\n",
        "    def __init__(self, path):\n",
        "        \n",
        "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
        "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
        "\n",
        "        self.n_items = self.load_n_items()\n",
        "    \n",
        "    def load_data(self, datatype='train'):\n",
        "        if datatype == 'train':\n",
        "            return self._load_train_data()\n",
        "        elif datatype == 'validation':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        elif datatype == 'test':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        else:\n",
        "            raise ValueError(\"datatype should be in [train, validation, test]\")\n",
        "        \n",
        "    def load_n_items(self):\n",
        "        unique_sid = list()\n",
        "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                unique_sid.append(line.strip())\n",
        "        n_items = len(unique_sid)\n",
        "        return n_items\n",
        "    \n",
        "    def _load_train_data(self):\n",
        "        path = os.path.join(self.pro_dir, 'train.csv')\n",
        "        \n",
        "        tp = pd.read_csv(path)\n",
        "        n_users = tp['uid'].max() + 1\n",
        "\n",
        "        rows, cols = tp['uid'], tp['sid']\n",
        "        data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                                 (rows, cols)), dtype='float64',\n",
        "                                 shape=(n_users, self.n_items))\n",
        "        return data\n",
        "    \n",
        "    def _load_tr_te_data(self, datatype='test'):\n",
        "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
        "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
        "\n",
        "        tp_tr = pd.read_csv(tr_path)\n",
        "        tp_te = pd.read_csv(te_path)\n",
        "\n",
        "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        return data_tr, data_te"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## side information 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델의 크기(shape) : (3000000, 300)\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "\n",
        "#urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
        "#                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "print('모델의 크기(shape) :',word2vec_model.vectors.shape) # 모델의 크기 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item</th>\n",
              "      <th>genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>318</td>\n",
              "      <td>Crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>318</td>\n",
              "      <td>Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2571</td>\n",
              "      <td>Action</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2571</td>\n",
              "      <td>Sci-Fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2571</td>\n",
              "      <td>Thriller</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   item     genre\n",
              "0   318     Crime\n",
              "1   318     Drama\n",
              "2  2571    Action\n",
              "3  2571    Sci-Fi\n",
              "4  2571  Thriller"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "gen = pd.read_csv(\"/opt/ml/input/data/train/genres.tsv\", delimiter='\\t')\n",
        "gen.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item</th>\n",
              "      <th>genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>198</td>\n",
              "      <td>Crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>198</td>\n",
              "      <td>Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>82</td>\n",
              "      <td>Action</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>82</td>\n",
              "      <td>Sci-Fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82</td>\n",
              "      <td>Thriller</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   item     genre\n",
              "0   198     Crime\n",
              "1   198     Drama\n",
              "2    82    Action\n",
              "3    82    Sci-Fi\n",
              "4    82  Thriller"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def gen_numerize(tp, show2id):\n",
        "    #uid = tp['user'].apply(lambda x: profile2id[x])\n",
        "    sid = tp['item'].apply(lambda x: show2id[x])\n",
        "    return sid\n",
        "gen['item'] = gen_numerize(gen, show2id)\n",
        "gen.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item</th>\n",
              "      <th>genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>198</td>\n",
              "      <td>Crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>198</td>\n",
              "      <td>Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>82</td>\n",
              "      <td>Action</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>82</td>\n",
              "      <td>Sci-Fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82</td>\n",
              "      <td>Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15928</th>\n",
              "      <td>6763</td>\n",
              "      <td>Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15929</th>\n",
              "      <td>5046</td>\n",
              "      <td>Action</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15930</th>\n",
              "      <td>5046</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15931</th>\n",
              "      <td>5508</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15932</th>\n",
              "      <td>5531</td>\n",
              "      <td>Documentary</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15933 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       item        genre\n",
              "0       198        Crime\n",
              "1       198        Drama\n",
              "2        82       Action\n",
              "3        82       Sci-Fi\n",
              "4        82     Thriller\n",
              "...     ...          ...\n",
              "15928  6763        Drama\n",
              "15929  5046       Action\n",
              "15930  5046       Comedy\n",
              "15931  5508       Comedy\n",
              "15932  5531  Documentary\n",
              "\n",
              "[15933 rows x 2 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Action</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      genre\n",
              "0     Drama\n",
              "1    Comedy\n",
              "2  Thriller\n",
              "3   Romance\n",
              "4    Action"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_emb = pd.DataFrame(gen.genre.value_counts().index.values, columns=['genre'])\n",
        "gen_emb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "emb_list = []\n",
        "for x in gen_emb.genre:\n",
        "    if x == 'Sci-Fi':\n",
        "        emb_list.append(word2vec_model['science_fiction'])\n",
        "    elif x == 'Film-Noir':\n",
        "        emb_list.append(word2vec_model['Film_Noir'])\n",
        "    else:\n",
        "        emb_list.append(word2vec_model[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genre</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Drama</th>\n",
              "      <td>-0.144531</td>\n",
              "      <td>-0.055420</td>\n",
              "      <td>0.013855</td>\n",
              "      <td>-0.111816</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.022095</td>\n",
              "      <td>-0.277344</td>\n",
              "      <td>-0.127930</td>\n",
              "      <td>-0.320312</td>\n",
              "      <td>0.032227</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.222656</td>\n",
              "      <td>-0.100098</td>\n",
              "      <td>-0.589844</td>\n",
              "      <td>-0.184570</td>\n",
              "      <td>0.189453</td>\n",
              "      <td>0.195312</td>\n",
              "      <td>-0.113281</td>\n",
              "      <td>-0.055908</td>\n",
              "      <td>0.052490</td>\n",
              "      <td>0.247070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Comedy</th>\n",
              "      <td>-0.032959</td>\n",
              "      <td>-0.077637</td>\n",
              "      <td>-0.065918</td>\n",
              "      <td>0.291016</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.043213</td>\n",
              "      <td>0.151367</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>0.097656</td>\n",
              "      <td>-0.054443</td>\n",
              "      <td>...</td>\n",
              "      <td>0.443359</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>-0.443359</td>\n",
              "      <td>-0.024048</td>\n",
              "      <td>-0.036621</td>\n",
              "      <td>0.253906</td>\n",
              "      <td>-0.046631</td>\n",
              "      <td>-0.045898</td>\n",
              "      <td>0.038818</td>\n",
              "      <td>0.074219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Thriller</th>\n",
              "      <td>0.217773</td>\n",
              "      <td>-0.064941</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.314453</td>\n",
              "      <td>-0.047363</td>\n",
              "      <td>-0.192383</td>\n",
              "      <td>-0.390625</td>\n",
              "      <td>0.135742</td>\n",
              "      <td>0.168945</td>\n",
              "      <td>0.019165</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.058594</td>\n",
              "      <td>0.003937</td>\n",
              "      <td>-0.267578</td>\n",
              "      <td>0.235352</td>\n",
              "      <td>0.271484</td>\n",
              "      <td>-0.000277</td>\n",
              "      <td>-0.034180</td>\n",
              "      <td>-0.396484</td>\n",
              "      <td>-0.028076</td>\n",
              "      <td>0.072266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Romance</th>\n",
              "      <td>0.041992</td>\n",
              "      <td>-0.075195</td>\n",
              "      <td>-0.341797</td>\n",
              "      <td>0.119629</td>\n",
              "      <td>0.185547</td>\n",
              "      <td>-0.005249</td>\n",
              "      <td>-0.057617</td>\n",
              "      <td>-0.179688</td>\n",
              "      <td>-0.090332</td>\n",
              "      <td>0.179688</td>\n",
              "      <td>...</td>\n",
              "      <td>0.116699</td>\n",
              "      <td>-0.078613</td>\n",
              "      <td>-0.322266</td>\n",
              "      <td>-0.013245</td>\n",
              "      <td>0.353516</td>\n",
              "      <td>-0.083008</td>\n",
              "      <td>-0.121094</td>\n",
              "      <td>0.022095</td>\n",
              "      <td>-0.035156</td>\n",
              "      <td>0.291016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Action</th>\n",
              "      <td>0.053955</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>0.242188</td>\n",
              "      <td>0.049316</td>\n",
              "      <td>0.023315</td>\n",
              "      <td>-0.080566</td>\n",
              "      <td>-0.059570</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.108887</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009277</td>\n",
              "      <td>-0.189453</td>\n",
              "      <td>-0.242188</td>\n",
              "      <td>0.067383</td>\n",
              "      <td>0.024170</td>\n",
              "      <td>0.016968</td>\n",
              "      <td>0.049072</td>\n",
              "      <td>0.011475</td>\n",
              "      <td>-0.025513</td>\n",
              "      <td>-0.099121</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0         1         2         3         4         5    \\\n",
              "genre                                                                  \n",
              "Drama    -0.144531 -0.055420  0.013855 -0.111816  0.187500  0.022095   \n",
              "Comedy   -0.032959 -0.077637 -0.065918  0.291016  0.041016  0.043213   \n",
              "Thriller  0.217773 -0.064941  0.187500  0.314453 -0.047363 -0.192383   \n",
              "Romance   0.041992 -0.075195 -0.341797  0.119629  0.185547 -0.005249   \n",
              "Action    0.053955 -0.031250  0.242188  0.049316  0.023315 -0.080566   \n",
              "\n",
              "               6         7         8         9    ...       290       291  \\\n",
              "genre                                             ...                       \n",
              "Drama    -0.277344 -0.127930 -0.320312  0.032227  ... -0.222656 -0.100098   \n",
              "Comedy    0.151367  0.273438  0.097656 -0.054443  ...  0.443359  0.078125   \n",
              "Thriller -0.390625  0.135742  0.168945  0.019165  ... -0.058594  0.003937   \n",
              "Romance  -0.057617 -0.179688 -0.090332  0.179688  ...  0.116699 -0.078613   \n",
              "Action   -0.059570  0.033203 -0.310547  0.108887  ...  0.009277 -0.189453   \n",
              "\n",
              "               292       293       294       295       296       297  \\\n",
              "genre                                                                  \n",
              "Drama    -0.589844 -0.184570  0.189453  0.195312 -0.113281 -0.055908   \n",
              "Comedy   -0.443359 -0.024048 -0.036621  0.253906 -0.046631 -0.045898   \n",
              "Thriller -0.267578  0.235352  0.271484 -0.000277 -0.034180 -0.396484   \n",
              "Romance  -0.322266 -0.013245  0.353516 -0.083008 -0.121094  0.022095   \n",
              "Action   -0.242188  0.067383  0.024170  0.016968  0.049072  0.011475   \n",
              "\n",
              "               298       299  \n",
              "genre                         \n",
              "Drama     0.052490  0.247070  \n",
              "Comedy    0.038818  0.074219  \n",
              "Thriller -0.028076  0.072266  \n",
              "Romance  -0.035156  0.291016  \n",
              "Action   -0.025513 -0.099121  \n",
              "\n",
              "[5 rows x 300 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = pd.concat([gen_emb, pd.DataFrame(emb_list)], axis=1)\n",
        "a = x.set_index('genre', drop=True)\n",
        "a.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen2emb = dict((x, a.loc[x].values) for (i, x) in enumerate(a.index))\n",
        "gen['emb'] = gen['genre'].apply(lambda x: gen2emb[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item</th>\n",
              "      <th>genre</th>\n",
              "      <th>emb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>198</td>\n",
              "      <td>Crime</td>\n",
              "      <td>[0.028076172, 0.0048828125, -0.09667969, -0.01...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>198</td>\n",
              "      <td>Drama</td>\n",
              "      <td>[-0.14453125, -0.055419922, 0.0138549805, -0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>82</td>\n",
              "      <td>Action</td>\n",
              "      <td>[0.053955078, -0.03125, 0.2421875, 0.049316406...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>82</td>\n",
              "      <td>Sci-Fi</td>\n",
              "      <td>[0.12988281, -0.12597656, 0.1796875, 0.2519531...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>[0.21777344, -0.064941406, 0.1875, 0.31445312,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>260</td>\n",
              "      <td>Action</td>\n",
              "      <td>[0.053955078, -0.03125, 0.2421875, 0.049316406...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>260</td>\n",
              "      <td>Crime</td>\n",
              "      <td>[0.028076172, 0.0048828125, -0.09667969, -0.01...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>260</td>\n",
              "      <td>Drama</td>\n",
              "      <td>[-0.14453125, -0.055419922, 0.0138549805, -0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>260</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>[0.21777344, -0.064941406, 0.1875, 0.31445312,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>264</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>[-0.032958984, -0.07763672, -0.06591797, 0.291...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   item     genre                                                emb\n",
              "0   198     Crime  [0.028076172, 0.0048828125, -0.09667969, -0.01...\n",
              "1   198     Drama  [-0.14453125, -0.055419922, 0.0138549805, -0.1...\n",
              "2    82    Action  [0.053955078, -0.03125, 0.2421875, 0.049316406...\n",
              "3    82    Sci-Fi  [0.12988281, -0.12597656, 0.1796875, 0.2519531...\n",
              "4    82  Thriller  [0.21777344, -0.064941406, 0.1875, 0.31445312,...\n",
              "5   260    Action  [0.053955078, -0.03125, 0.2421875, 0.049316406...\n",
              "6   260     Crime  [0.028076172, 0.0048828125, -0.09667969, -0.01...\n",
              "7   260     Drama  [-0.14453125, -0.055419922, 0.0138549805, -0.1...\n",
              "8   260  Thriller  [0.21777344, -0.064941406, 0.1875, 0.31445312,...\n",
              "9   264    Comedy  [-0.032958984, -0.07763672, -0.06591797, 0.291..."
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def item_genre_emb_mean(i):\n",
        "    total.append(np.mean(gen[gen['item'] == i].emb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.016510</td>\n",
              "      <td>-0.062561</td>\n",
              "      <td>0.094589</td>\n",
              "      <td>0.042084</td>\n",
              "      <td>0.043182</td>\n",
              "      <td>0.026520</td>\n",
              "      <td>-0.068420</td>\n",
              "      <td>0.137451</td>\n",
              "      <td>-0.155029</td>\n",
              "      <td>-0.025269</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.132629</td>\n",
              "      <td>-0.041382</td>\n",
              "      <td>-0.305176</td>\n",
              "      <td>-0.006836</td>\n",
              "      <td>0.197449</td>\n",
              "      <td>0.032135</td>\n",
              "      <td>-0.027195</td>\n",
              "      <td>0.064209</td>\n",
              "      <td>0.043518</td>\n",
              "      <td>0.061951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.081635</td>\n",
              "      <td>-0.032227</td>\n",
              "      <td>0.068909</td>\n",
              "      <td>0.082504</td>\n",
              "      <td>0.009857</td>\n",
              "      <td>-0.055420</td>\n",
              "      <td>-0.080139</td>\n",
              "      <td>0.152634</td>\n",
              "      <td>-0.122925</td>\n",
              "      <td>0.063751</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.075012</td>\n",
              "      <td>-0.030418</td>\n",
              "      <td>-0.158997</td>\n",
              "      <td>0.145264</td>\n",
              "      <td>0.174988</td>\n",
              "      <td>0.007255</td>\n",
              "      <td>0.039490</td>\n",
              "      <td>-0.155701</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>-0.040222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.103394</td>\n",
              "      <td>0.007202</td>\n",
              "      <td>-0.018585</td>\n",
              "      <td>0.072998</td>\n",
              "      <td>0.104065</td>\n",
              "      <td>0.027039</td>\n",
              "      <td>-0.024414</td>\n",
              "      <td>0.055664</td>\n",
              "      <td>-0.175781</td>\n",
              "      <td>-0.042969</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.228516</td>\n",
              "      <td>-0.050545</td>\n",
              "      <td>-0.402344</td>\n",
              "      <td>-0.223145</td>\n",
              "      <td>0.194824</td>\n",
              "      <td>0.049561</td>\n",
              "      <td>-0.035889</td>\n",
              "      <td>-0.060669</td>\n",
              "      <td>0.027115</td>\n",
              "      <td>0.212891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.081909</td>\n",
              "      <td>-0.063721</td>\n",
              "      <td>0.080933</td>\n",
              "      <td>0.164795</td>\n",
              "      <td>0.060608</td>\n",
              "      <td>0.061646</td>\n",
              "      <td>0.236816</td>\n",
              "      <td>0.079346</td>\n",
              "      <td>-0.057861</td>\n",
              "      <td>-0.184082</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.273438</td>\n",
              "      <td>-0.071785</td>\n",
              "      <td>-0.208496</td>\n",
              "      <td>-0.039551</td>\n",
              "      <td>0.143799</td>\n",
              "      <td>0.001709</td>\n",
              "      <td>-0.082275</td>\n",
              "      <td>0.070801</td>\n",
              "      <td>-0.091904</td>\n",
              "      <td>0.053711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.054626</td>\n",
              "      <td>-0.195557</td>\n",
              "      <td>-0.029028</td>\n",
              "      <td>-0.056213</td>\n",
              "      <td>0.179199</td>\n",
              "      <td>-0.013428</td>\n",
              "      <td>0.020264</td>\n",
              "      <td>0.259277</td>\n",
              "      <td>-0.032227</td>\n",
              "      <td>-0.060394</td>\n",
              "      <td>...</td>\n",
              "      <td>0.173462</td>\n",
              "      <td>0.118408</td>\n",
              "      <td>-0.044434</td>\n",
              "      <td>-0.103149</td>\n",
              "      <td>0.124237</td>\n",
              "      <td>-0.050903</td>\n",
              "      <td>-0.112061</td>\n",
              "      <td>-0.040283</td>\n",
              "      <td>-0.031189</td>\n",
              "      <td>0.062378</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  0.016510 -0.062561  0.094589  0.042084  0.043182  0.026520 -0.068420   \n",
              "1  0.081635 -0.032227  0.068909  0.082504  0.009857 -0.055420 -0.080139   \n",
              "2 -0.103394  0.007202 -0.018585  0.072998  0.104065  0.027039 -0.024414   \n",
              "3 -0.081909 -0.063721  0.080933  0.164795  0.060608  0.061646  0.236816   \n",
              "4  0.054626 -0.195557 -0.029028 -0.056213  0.179199 -0.013428  0.020264   \n",
              "\n",
              "        7         8         9    ...       290       291       292       293  \\\n",
              "0  0.137451 -0.155029 -0.025269  ... -0.132629 -0.041382 -0.305176 -0.006836   \n",
              "1  0.152634 -0.122925  0.063751  ... -0.075012 -0.030418 -0.158997  0.145264   \n",
              "2  0.055664 -0.175781 -0.042969  ... -0.228516 -0.050545 -0.402344 -0.223145   \n",
              "3  0.079346 -0.057861 -0.184082  ... -0.273438 -0.071785 -0.208496 -0.039551   \n",
              "4  0.259277 -0.032227 -0.060394  ...  0.173462  0.118408 -0.044434 -0.103149   \n",
              "\n",
              "        294       295       296       297       298       299  \n",
              "0  0.197449  0.032135 -0.027195  0.064209  0.043518  0.061951  \n",
              "1  0.174988  0.007255  0.039490 -0.155701  0.000488 -0.040222  \n",
              "2  0.194824  0.049561 -0.035889 -0.060669  0.027115  0.212891  \n",
              "3  0.143799  0.001709 -0.082275  0.070801 -0.091904  0.053711  \n",
              "4  0.124237 -0.050903 -0.112061 -0.040283 -0.031189  0.062378  \n",
              "\n",
              "[5 rows x 300 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total = []\n",
        "\n",
        "item_genre_emb_idx = pd.DataFrame(list(i for i in range(0, max(gen.item)+1)), columns=['item'])\n",
        "item_genre_emb_idx.item.apply(lambda x: item_genre_emb_mean(x))\n",
        "item_genre_emb = pd.DataFrame(total)\n",
        "item_genre_emb.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6807, 300)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "item_genre_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300, 6807)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "item_genre_emb = item_genre_emb.T\n",
        "item_genre_emb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# User별 장르 관점에서의 아이템 선호도"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 유저별 시청한 영화 장르 파악\n",
        "raw_data = pd.read_csv('/opt/ml/input/data/train/train_ratings.csv')\n",
        "users = raw_data.user.unique()\n",
        "user_genre = pd.DataFrame([users]).T\n",
        "user_genre.columns = ['users']\n",
        "\n",
        "# 유저별 장르 선호에 따른 추천\n",
        "for x in gen.genre.unique():\n",
        "    user_genre[x] = 0\n",
        "user_genre.set_index('users', inplace=True)\n",
        "\n",
        "\n",
        "users = raw_data.user.unique()\n",
        "\n",
        "for i in tqdm(range(len(users))):\n",
        "    user_item = raw_data[raw_data.user == users[i]].item.values\n",
        "    user_fav_genre = []\n",
        "    for x in user_item:\n",
        "        user_fav_genre.extend(gen[gen.item == x].genre.values)\n",
        "\n",
        "    counter_user = Counter(user_fav_genre).most_common()\n",
        "    for x, y in counter_user:\n",
        "        user_genre.loc[users[i], x] = y\n",
        "\n",
        "user_genre['total'] = user_genre.sum(axis=1).values\n",
        "user_genre_prefer = user_genre.iloc[:,0:].div(user_genre.total, axis=0)\n",
        "items = sorted(gen.item.unique())\n",
        "for x in items:\n",
        "    user_genre_prefer[x] = 0\n",
        "\n",
        "user_genre_prefer.to_csv('user_genre_prefer.csv')\n",
        "\n",
        "items = gen.item.unique()\n",
        "\n",
        "for i in tqdm(range(len(items))):\n",
        "    i_genres = gen[gen.item == items[i]].genre.values\n",
        "    for u in users:\n",
        "        user_item = 0\n",
        "        cnt = 0\n",
        "        for x in i_genres:\n",
        "            user_item += user_genre_prefer.loc[u, x]\n",
        "            cnt += 1\n",
        "        user_genre_prefer.loc[u, items[i]] = user_item / cnt\n",
        "\n",
        "user_genre_prefer.iloc[:,19:].to_csv('8h_user_genre_prefer.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_genre_prefer = pd.read_csv('later_ana/8h_user_genre_prefer.csv')\n",
        "user_genre_prefer['users'] = user_genre_prefer['users'].apply(lambda x: profile2id[x])\n",
        "user_genre_prefer = user_genre_prefer.sort_values('users')\n",
        "user_genre_prefer.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>users</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>6797</th>\n",
              "      <th>6798</th>\n",
              "      <th>6799</th>\n",
              "      <th>6800</th>\n",
              "      <th>6801</th>\n",
              "      <th>6802</th>\n",
              "      <th>6803</th>\n",
              "      <th>6804</th>\n",
              "      <th>6805</th>\n",
              "      <th>6806</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.095404</td>\n",
              "      <td>0.076602</td>\n",
              "      <td>0.108635</td>\n",
              "      <td>0.025070</td>\n",
              "      <td>0.047354</td>\n",
              "      <td>0.062674</td>\n",
              "      <td>0.192201</td>\n",
              "      <td>0.094011</td>\n",
              "      <td>0.033426</td>\n",
              "      <td>...</td>\n",
              "      <td>0.080223</td>\n",
              "      <td>0.063138</td>\n",
              "      <td>0.192201</td>\n",
              "      <td>0.169916</td>\n",
              "      <td>0.104921</td>\n",
              "      <td>0.140669</td>\n",
              "      <td>0.130919</td>\n",
              "      <td>0.130919</td>\n",
              "      <td>0.192201</td>\n",
              "      <td>0.169916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.106796</td>\n",
              "      <td>0.089806</td>\n",
              "      <td>0.101942</td>\n",
              "      <td>0.048544</td>\n",
              "      <td>0.048544</td>\n",
              "      <td>0.084951</td>\n",
              "      <td>0.155340</td>\n",
              "      <td>0.080097</td>\n",
              "      <td>0.053398</td>\n",
              "      <td>...</td>\n",
              "      <td>0.085437</td>\n",
              "      <td>0.090615</td>\n",
              "      <td>0.155340</td>\n",
              "      <td>0.106796</td>\n",
              "      <td>0.080906</td>\n",
              "      <td>0.126214</td>\n",
              "      <td>0.087379</td>\n",
              "      <td>0.087379</td>\n",
              "      <td>0.155340</td>\n",
              "      <td>0.106796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.093220</td>\n",
              "      <td>0.064972</td>\n",
              "      <td>0.110169</td>\n",
              "      <td>0.016949</td>\n",
              "      <td>0.048023</td>\n",
              "      <td>0.066384</td>\n",
              "      <td>0.203390</td>\n",
              "      <td>0.094633</td>\n",
              "      <td>0.048023</td>\n",
              "      <td>...</td>\n",
              "      <td>0.087006</td>\n",
              "      <td>0.056497</td>\n",
              "      <td>0.203390</td>\n",
              "      <td>0.186441</td>\n",
              "      <td>0.103578</td>\n",
              "      <td>0.149718</td>\n",
              "      <td>0.135593</td>\n",
              "      <td>0.135593</td>\n",
              "      <td>0.203390</td>\n",
              "      <td>0.186441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.096667</td>\n",
              "      <td>0.053333</td>\n",
              "      <td>0.126667</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.101333</td>\n",
              "      <td>0.048889</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.146667</td>\n",
              "      <td>0.071111</td>\n",
              "      <td>0.173333</td>\n",
              "      <td>0.093333</td>\n",
              "      <td>0.093333</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.146667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.065625</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.043750</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>0.018750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.067500</td>\n",
              "      <td>0.054167</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.104167</td>\n",
              "      <td>0.118750</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 6808 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   users         0         1         2         3         4         5  \\\n",
              "0      0  0.095404  0.076602  0.108635  0.025070  0.047354  0.062674   \n",
              "1      1  0.106796  0.089806  0.101942  0.048544  0.048544  0.084951   \n",
              "2      2  0.093220  0.064972  0.110169  0.016949  0.048023  0.066384   \n",
              "3      3  0.096667  0.053333  0.126667  0.006667  0.040000  0.070000   \n",
              "4      4  0.084375  0.065625  0.131250  0.075000  0.075000  0.043750   \n",
              "\n",
              "          6         7         8  ...      6797      6798      6799      6800  \\\n",
              "0  0.192201  0.094011  0.033426  ...  0.080223  0.063138  0.192201  0.169916   \n",
              "1  0.155340  0.080097  0.053398  ...  0.085437  0.090615  0.155340  0.106796   \n",
              "2  0.203390  0.094633  0.048023  ...  0.087006  0.056497  0.203390  0.186441   \n",
              "3  0.240000  0.110000  0.066667  ...  0.101333  0.048889  0.240000  0.146667   \n",
              "4  0.175000  0.078125  0.018750  ...  0.067500  0.054167  0.175000  0.200000   \n",
              "\n",
              "       6801      6802      6803      6804      6805      6806  \n",
              "0  0.104921  0.140669  0.130919  0.130919  0.192201  0.169916  \n",
              "1  0.080906  0.126214  0.087379  0.087379  0.155340  0.106796  \n",
              "2  0.103578  0.149718  0.135593  0.135593  0.203390  0.186441  \n",
              "3  0.071111  0.173333  0.093333  0.093333  0.240000  0.146667  \n",
              "4  0.104167  0.118750  0.125000  0.125000  0.175000  0.200000  \n",
              "\n",
              "[5 rows x 6808 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_genre_prefer.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(31360, 6808)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_genre_prefer.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-VAE + Multi-DAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 로드\n",
        "loader = DataLoader(args.data)\n",
        "\n",
        "n_items = loader.load_n_items()\n",
        "train_data = loader.load_data('train')\n",
        "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "test_data_tr, test_data_te = loader.load_data('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MultiDAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-DAE.\n",
        "\n",
        "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
        "        super(MultiDAE, self).__init__()\n",
        "        self.item_genre = torch.Tensor(item_genre_emb.values) ##### 추가\n",
        "\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "\n",
        "        self.dims = self.q_dims + self.p_dims[1:]\n",
        "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        \n",
        "        self.init_weights()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        #print('input.shape: ', input.shape)\n",
        "        h = F.normalize(input)\n",
        "        h = self.drop(h)\n",
        "        h = torch.cat((self.item_genre.to(device), h), 0) ###추가\n",
        "        #print('합친 h.shape: ', h.shape)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.layers) - 1:\n",
        "                h = F.tanh(h) #reluX\n",
        "                \n",
        "\n",
        "        item_genre_emb, reconstructed_h = h.split([self.item_genre.shape[0], input.shape[0]], 0) ##추가\n",
        "        #print('item_genre_emb.shape: ', item_genre_emb.shape)\n",
        "        #print('reconstructed_h.shape: ',reconstructed_h.shape)\n",
        "        return reconstructed_h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "\n",
        "\n",
        "class MultiVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-VAE.\n",
        "\n",
        "    Multi-VAE : Variational Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
        "        super(MultiVAE, self).__init__()\n",
        "        self.p_dims = p_dims\n",
        "        self.item_genre = torch.Tensor(item_genre_emb.values) ##### 추가\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "\n",
        "        # Last dimension of q- network is for mean and variance\n",
        "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
        "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
        "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
        "        \n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.init_weights()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        \n",
        "        mu, logvar = self.encode(input)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "    \n",
        "    def encode(self, input):\n",
        "        h = F.normalize(input)\n",
        "        h = self.drop(h)\n",
        "        \n",
        "        h = torch.cat((self.item_genre.to(device), h), 0) ###추가\n",
        "        for i, layer in enumerate(self.q_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.q_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "            else:\n",
        "                mu = h[:, :self.q_dims[-1]]\n",
        "                logvar = h[:, self.q_dims[-1]:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "    \n",
        "    def decode(self, z):\n",
        "        h = z\n",
        "        for i, layer in enumerate(self.p_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.p_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "        item_genre_emb, reconstructed_h = h.split([self.item_genre.shape[0], self.input.shape[0]], 0) ##추가\n",
        "        return reconstructed_h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.q_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "        \n",
        "        for layer in self.p_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def loss_function_vae(recon_x, x, mu, logvar, anneal=1.0):\n",
        "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
        "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
        "\n",
        "    return BCE + anneal * KLD\n",
        "\n",
        "def loss_function_dae(recon_x, x):\n",
        "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
        "    return BCE\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def sparse2torch_sparse(data):\n",
        "    \"\"\"\n",
        "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
        "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
        "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
        "    \"\"\"\n",
        "    samples = data.shape[0]\n",
        "    features = data.shape[1]\n",
        "    coo_data = data.tocoo()\n",
        "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
        "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
        "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
        "    values = np.array([row2val[r] for r in coo_data.row])\n",
        "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
        "    return t\n",
        "\n",
        "def naive_sparse2tensor(data):\n",
        "    return torch.FloatTensor(data.toarray())\n",
        "\n",
        "\n",
        "def train(model, criterion, optimizer, is_VAE = False):\n",
        "    # Turn on training mode\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    global update_count\n",
        "\n",
        "    np.random.shuffle(idxlist)\n",
        "    \n",
        "    for batch_idx, start_idx in enumerate(range(0, N, args.batch_size)):\n",
        "        end_idx = min(start_idx + args.batch_size, N)\n",
        "        data = train_data[idxlist[start_idx:end_idx]]\n",
        "        data = naive_sparse2tensor(data).to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if is_VAE:\n",
        "          if args.total_anneal_steps > 0:\n",
        "            anneal = min(args.anneal_cap, \n",
        "                            1. * update_count / args.total_anneal_steps)\n",
        "          else:\n",
        "              anneal = args.anneal_cap\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          recon_batch, mu, logvar = model(data)\n",
        "          \n",
        "          loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
        "        else:\n",
        "          recon_batch = model(data)\n",
        "          loss = criterion(recon_batch, data)\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        update_count += 1\n",
        "\n",
        "        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
        "                    'loss {:4.2f}'.format(\n",
        "                        epoch, batch_idx, len(range(0, N, args.batch_size)),\n",
        "                        elapsed * 1000 / args.log_interval,\n",
        "                        train_loss / args.log_interval))\n",
        "            \n",
        "\n",
        "            start_time = time.time()\n",
        "            train_loss = 0.0\n",
        "    train_loss /= len(range(0, N, args.batch_size))\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, data_tr, data_te, is_VAE=False):\n",
        "    # Turn on evaluation mode\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    global update_count\n",
        "    e_idxlist = list(range(data_tr.shape[0]))\n",
        "    e_N = data_tr.shape[0]\n",
        "    n100_list = []\n",
        "    r10_list= []\n",
        "    r20_list = []\n",
        "    r50_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for start_idx in range(0, e_N, args.batch_size):\n",
        "            end_idx = min(start_idx + args.batch_size, N)\n",
        "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
        "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
        "\n",
        "            data_tensor = naive_sparse2tensor(data).to(device)\n",
        "            if is_VAE :\n",
        "              \n",
        "              if args.total_anneal_steps > 0:\n",
        "                  anneal = min(args.anneal_cap, \n",
        "                                1. * update_count / args.total_anneal_steps)\n",
        "              else:\n",
        "                  anneal = args.anneal_cap\n",
        "\n",
        "              recon_batch, mu, logvar = model(data_tensor)\n",
        "\n",
        "              loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
        "\n",
        "            else :\n",
        "              recon_batch = model(data_tensor)\n",
        "              loss = criterion(recon_batch, data_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Exclude examples from training set\n",
        "            recon_batch = recon_batch.cpu().numpy()\n",
        "            recon_batch[data.nonzero()] = -np.inf\n",
        "\n",
        "            n100 = NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
        "            r20 = Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
        "            r10 = Recall_at_k_batch(recon_batch, heldout_data, 10)\n",
        "            r50 = Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
        "\n",
        "            n100_list.append(n100)\n",
        "            r20_list.append(r20)\n",
        "            r10_list.append(r10)\n",
        "            r50_list.append(r50)\n",
        " \n",
        "    total_loss /= len(range(0, e_N, args.batch_size))\n",
        "    n100_list = np.concatenate(n100_list)\n",
        "    r20_list = np.concatenate(r20_list)\n",
        "    r10_list = np.concatenate(r10_list)\n",
        "    r50_list = np.concatenate(r50_list)\n",
        "\n",
        "    return total_loss, np.mean(n100_list), np.mean(r10_list), np.mean(r20_list), np.mean(r50_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bottleneck as bn\n",
        "import numpy as np\n",
        "\n",
        "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    Normalized Discounted Cumulative Gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "    batch_users = X_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "    return DCG / IDCG\n",
        "\n",
        "\n",
        "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    batch_users = X_pred.shape[0]\n",
        "    \n",
        "    X_pred = X_pred + np.mean(item_genre_emb.values)*20\n",
        "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
        "    \n",
        "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
        "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    X_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install torch torchvision\n",
        "#import torch\n",
        "#from torch.utils.tensorboard import SummaryWriter\n",
        "#writer = SummaryWriter()\n",
        "\n",
        "#!pip install adabound\n",
        "import adabound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mesk1\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.12 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/input/code/wandb/run-20220406_042308-1mn83obq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/esk1/recmovie/runs/1mn83obq\" target=\"_blank\">nagus-spot-58</a></strong> to <a href=\"https://wandb.ai/esk1/recmovie\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"recmovie\", entity=\"esk1\")\n",
        "wandb.config.update(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "loader = DataLoader(args.data)\n",
        "\n",
        "n_items = loader.load_n_items()\n",
        "train_data = loader.load_data('train')\n",
        "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "N = train_data.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "#p_dims = [200, 600, 1600, 3200, n_items]\n",
        "p_dims = [200, 3000, n_items]\n",
        "model = MultiDAE(p_dims).to(device)\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
        "#https://github.com/Luolc/AdaBound\n",
        "#optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
        "criterion = loss_function_dae\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "best_r10 = -np.inf\n",
        "update_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/panda_down/lib/python3.8/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 2.95s | valid loss 968.81 | n100 0.344 | r10 0.281 | r20 0.257 | r50 0.312\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/panda_down/lib/python3.8/site-packages/torch/serialization.py:401: UserWarning: Couldn't retrieve source code for container of type MultiDAE. It won't be checked for correctness upon loading.\n",
            "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 2.83s | valid loss 946.18 | n100 0.385 | r10 0.315 | r20 0.290 | r50 0.350\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 2.94s | valid loss 936.94 | n100 0.402 | r10 0.330 | r20 0.305 | r50 0.367\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 2.90s | valid loss 930.41 | n100 0.415 | r10 0.347 | r20 0.319 | r50 0.381\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 2.92s | valid loss 925.29 | n100 0.420 | r10 0.346 | r20 0.322 | r50 0.388\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 2.86s | valid loss 920.94 | n100 0.429 | r10 0.361 | r20 0.332 | r50 0.394\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 2.89s | valid loss 917.78 | n100 0.433 | r10 0.367 | r20 0.338 | r50 0.400\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 2.90s | valid loss 914.76 | n100 0.436 | r10 0.366 | r20 0.340 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 2.88s | valid loss 912.20 | n100 0.434 | r10 0.354 | r20 0.334 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 2.96s | valid loss 909.64 | n100 0.443 | r10 0.377 | r20 0.344 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 2.77s | valid loss 907.69 | n100 0.439 | r10 0.362 | r20 0.340 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 2.81s | valid loss 906.99 | n100 0.442 | r10 0.377 | r20 0.346 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 2.88s | valid loss 904.66 | n100 0.444 | r10 0.375 | r20 0.347 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 2.73s | valid loss 903.32 | n100 0.445 | r10 0.380 | r20 0.346 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 2.79s | valid loss 902.90 | n100 0.446 | r10 0.384 | r20 0.350 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 2.86s | valid loss 899.93 | n100 0.438 | r10 0.362 | r20 0.339 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 2.82s | valid loss 898.90 | n100 0.441 | r10 0.367 | r20 0.340 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 2.84s | valid loss 899.16 | n100 0.441 | r10 0.375 | r20 0.344 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 2.88s | valid loss 896.72 | n100 0.443 | r10 0.371 | r20 0.342 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 2.86s | valid loss 896.08 | n100 0.443 | r10 0.374 | r20 0.343 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 2.84s | valid loss 895.95 | n100 0.429 | r10 0.340 | r20 0.323 | r50 0.399\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 2.83s | valid loss 893.85 | n100 0.441 | r10 0.365 | r20 0.341 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 2.87s | valid loss 893.21 | n100 0.441 | r10 0.365 | r20 0.340 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 2.82s | valid loss 893.94 | n100 0.446 | r10 0.379 | r20 0.346 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 2.90s | valid loss 891.41 | n100 0.443 | r10 0.367 | r20 0.341 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 3.00s | valid loss 891.42 | n100 0.443 | r10 0.371 | r20 0.342 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 2.83s | valid loss 898.24 | n100 0.446 | r10 0.381 | r20 0.345 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 2.86s | valid loss 890.53 | n100 0.447 | r10 0.375 | r20 0.344 | r50 0.411\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 2.69s | valid loss 903.85 | n100 0.440 | r10 0.373 | r20 0.340 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 2.92s | valid loss 890.28 | n100 0.434 | r10 0.348 | r20 0.328 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 2.77s | valid loss 888.73 | n100 0.443 | r10 0.371 | r20 0.345 | r50 0.412\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 2.79s | valid loss 887.92 | n100 0.441 | r10 0.365 | r20 0.339 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 2.76s | valid loss 887.54 | n100 0.443 | r10 0.369 | r20 0.343 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 2.89s | valid loss 887.27 | n100 0.442 | r10 0.368 | r20 0.341 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 2.89s | valid loss 886.45 | n100 0.444 | r10 0.366 | r20 0.343 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 2.93s | valid loss 887.84 | n100 0.444 | r10 0.371 | r20 0.343 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 2.91s | valid loss 887.89 | n100 0.445 | r10 0.375 | r20 0.346 | r50 0.411\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 2.97s | valid loss 886.01 | n100 0.445 | r10 0.371 | r20 0.342 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 2.88s | valid loss 885.74 | n100 0.444 | r10 0.371 | r20 0.342 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 2.90s | valid loss 885.03 | n100 0.441 | r10 0.363 | r20 0.339 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 2.93s | valid loss 885.75 | n100 0.446 | r10 0.375 | r20 0.345 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 2.86s | valid loss 884.73 | n100 0.443 | r10 0.367 | r20 0.343 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 2.75s | valid loss 883.75 | n100 0.444 | r10 0.366 | r20 0.340 | r50 0.411\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 2.90s | valid loss 884.04 | n100 0.447 | r10 0.377 | r20 0.346 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 2.81s | valid loss 884.68 | n100 0.446 | r10 0.379 | r20 0.348 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 2.87s | valid loss 883.58 | n100 0.442 | r10 0.365 | r20 0.340 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 2.84s | valid loss 883.16 | n100 0.442 | r10 0.363 | r20 0.340 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 2.89s | valid loss 883.16 | n100 0.447 | r10 0.375 | r20 0.346 | r50 0.411\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 2.96s | valid loss 884.83 | n100 0.428 | r10 0.337 | r20 0.321 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 2.95s | valid loss 883.99 | n100 0.445 | r10 0.372 | r20 0.345 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 2.86s | valid loss 882.46 | n100 0.446 | r10 0.371 | r20 0.345 | r50 0.411\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 2.80s | valid loss 882.45 | n100 0.446 | r10 0.375 | r20 0.345 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 2.85s | valid loss 883.14 | n100 0.449 | r10 0.381 | r20 0.347 | r50 0.414\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 2.78s | valid loss 881.78 | n100 0.445 | r10 0.367 | r20 0.343 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 2.81s | valid loss 881.87 | n100 0.446 | r10 0.370 | r20 0.342 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 2.82s | valid loss 881.44 | n100 0.445 | r10 0.370 | r20 0.344 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 2.93s | valid loss 881.59 | n100 0.442 | r10 0.361 | r20 0.337 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 2.94s | valid loss 880.82 | n100 0.443 | r10 0.366 | r20 0.342 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 2.90s | valid loss 880.95 | n100 0.446 | r10 0.371 | r20 0.343 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 2.85s | valid loss 880.98 | n100 0.446 | r10 0.370 | r20 0.343 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time: 2.82s | valid loss 881.10 | n100 0.447 | r10 0.376 | r20 0.348 | r50 0.412\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time: 2.80s | valid loss 880.75 | n100 0.446 | r10 0.371 | r20 0.345 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time: 2.88s | valid loss 880.18 | n100 0.442 | r10 0.360 | r20 0.336 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time: 2.86s | valid loss 879.70 | n100 0.443 | r10 0.367 | r20 0.342 | r50 0.411\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time: 2.93s | valid loss 880.20 | n100 0.444 | r10 0.368 | r20 0.343 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time: 2.92s | valid loss 879.52 | n100 0.442 | r10 0.362 | r20 0.337 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time: 2.93s | valid loss 879.61 | n100 0.441 | r10 0.361 | r20 0.337 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time: 2.94s | valid loss 880.19 | n100 0.439 | r10 0.354 | r20 0.332 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time: 2.84s | valid loss 879.59 | n100 0.442 | r10 0.362 | r20 0.336 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time: 2.82s | valid loss 880.53 | n100 0.442 | r10 0.368 | r20 0.342 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time: 2.89s | valid loss 879.91 | n100 0.446 | r10 0.374 | r20 0.345 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time: 2.82s | valid loss 879.05 | n100 0.443 | r10 0.365 | r20 0.340 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time: 2.89s | valid loss 878.61 | n100 0.442 | r10 0.360 | r20 0.335 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time: 2.73s | valid loss 878.88 | n100 0.442 | r10 0.365 | r20 0.339 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time: 2.85s | valid loss 879.32 | n100 0.444 | r10 0.370 | r20 0.342 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time: 2.77s | valid loss 878.50 | n100 0.443 | r10 0.367 | r20 0.340 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time: 2.90s | valid loss 878.64 | n100 0.440 | r10 0.363 | r20 0.336 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time: 2.90s | valid loss 879.77 | n100 0.443 | r10 0.369 | r20 0.340 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time: 2.86s | valid loss 878.40 | n100 0.442 | r10 0.362 | r20 0.338 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time: 2.85s | valid loss 878.29 | n100 0.444 | r10 0.366 | r20 0.341 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time: 2.88s | valid loss 878.01 | n100 0.441 | r10 0.362 | r20 0.338 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time: 2.86s | valid loss 879.14 | n100 0.445 | r10 0.372 | r20 0.344 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time: 2.89s | valid loss 878.00 | n100 0.444 | r10 0.368 | r20 0.343 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time: 2.94s | valid loss 877.83 | n100 0.442 | r10 0.363 | r20 0.338 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time: 2.88s | valid loss 877.72 | n100 0.441 | r10 0.362 | r20 0.339 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time: 2.92s | valid loss 878.01 | n100 0.441 | r10 0.361 | r20 0.336 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time: 2.87s | valid loss 877.81 | n100 0.443 | r10 0.367 | r20 0.339 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time: 2.89s | valid loss 878.16 | n100 0.446 | r10 0.369 | r20 0.342 | r50 0.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time: 2.87s | valid loss 878.03 | n100 0.441 | r10 0.362 | r20 0.336 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time: 2.87s | valid loss 879.37 | n100 0.442 | r10 0.368 | r20 0.341 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time: 2.84s | valid loss 877.95 | n100 0.441 | r10 0.364 | r20 0.339 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time: 2.82s | valid loss 878.62 | n100 0.442 | r10 0.365 | r20 0.340 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time: 2.81s | valid loss 877.59 | n100 0.441 | r10 0.363 | r20 0.338 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time: 2.73s | valid loss 877.60 | n100 0.439 | r10 0.362 | r20 0.337 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time: 2.85s | valid loss 878.58 | n100 0.442 | r10 0.364 | r20 0.338 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time: 2.78s | valid loss 878.20 | n100 0.441 | r10 0.362 | r20 0.338 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time: 2.87s | valid loss 878.49 | n100 0.439 | r10 0.358 | r20 0.335 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time: 2.90s | valid loss 877.58 | n100 0.441 | r10 0.359 | r20 0.338 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time: 2.90s | valid loss 877.99 | n100 0.441 | r10 0.363 | r20 0.338 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time: 2.85s | valid loss 877.73 | n100 0.439 | r10 0.357 | r20 0.335 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 907.04 | n100 0.44 | r10 0.38 | r20 0.34 | r50 0.41\n",
            "=========================================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "############batch 1600\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "r10_fin_list = []\n",
        "new_epochs = 100\n",
        "\n",
        "for epoch in range(1, new_epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_loss = train(model, criterion, optimizer, is_VAE=False)\n",
        "    val_loss, n100, r10, r20, r50 = evaluate(model, criterion, vad_data_tr, vad_data_te, is_VAE=False)\n",
        "    \n",
        "    train_loss_list.append(train_loss)\n",
        "    val_loss_list.append(val_loss)\n",
        "    r10_fin_list.append(r10)\n",
        "\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
        "            'n100 {:5.3f} | r10 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
        "                epoch, time.time() - epoch_start_time, val_loss,\n",
        "                n100, r10, r20, r50))\n",
        "    print('-' * 89)\n",
        "\n",
        "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
        "\n",
        "\n",
        "    # Save the model if the n100 is the best we've seen so far.\n",
        "    if r10 > best_r10:\n",
        "        with open('DAE_'+args.save, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_r10 = r10\n",
        "        print(\"Better performance! save best model...\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"n100\": n100,\n",
        "        \"r10\": r10, \n",
        "        \"r20\": r20,\n",
        "        \"r50\": r50\n",
        "    })\n",
        "\n",
        "# Load the best saved model.\n",
        "with open('DAE_'+args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss, n100, r10, r20, r50 = evaluate(model, criterion, test_data_tr, test_data_te, is_VAE=False)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r10 {:4.2f} | r20 {:4.2f} | '\n",
        "        'r50 {:4.2f}'.format(test_loss, n100, r10, r20, r50))\n",
        "print('=' * 89)\n",
        "wandb.watch(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multivae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:1mn83obq) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>n100</td><td>▁▅▇▇▇█▇█▇██████████▇█████████████████▇▇▇</td></tr><tr><td>r10</td><td>▁▅▇▇▇█▇█▅▇███▇████▇▅█▇▇██▇▇▇▇▇▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>r20</td><td>▁▅▇▇▇█▇█▆▇▇██▇████▇▆██████▇▇▇▇▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>r50</td><td>▁▅▇▇████▇██████████▇████████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>n100</td><td>0.43905</td></tr><tr><td>r10</td><td>0.35739</td></tr><tr><td>r20</td><td>0.33532</td></tr><tr><td>r50</td><td>0.40545</td></tr><tr><td>train_loss</td><td>1075.3526</td></tr><tr><td>val_loss</td><td>877.73273</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">nagus-spot-58</strong>: <a href=\"https://wandb.ai/esk1/recmovie/runs/1mn83obq\" target=\"_blank\">https://wandb.ai/esk1/recmovie/runs/1mn83obq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220406_042308-1mn83obq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:1mn83obq). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.12 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/input/code/wandb/run-20220406_042813-1rz3bxa3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/esk1/recmovie/runs/1rz3bxa3\" target=\"_blank\">borg-canamar-59</a></strong> to <a href=\"https://wandb.ai/esk1/recmovie\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"recmovie\", entity=\"esk1\")\n",
        "wandb.config.update(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "loader = DataLoader(args.data)\n",
        "\n",
        "n_items = loader.load_n_items()\n",
        "train_data = loader.load_data('train')\n",
        "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "N = train_data.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "p_dims = [200, 3000, n_items]\n",
        "model2 = MultiVAE(p_dims).to(device)\n",
        "\n",
        "optimizer2 = adabound.AdaBound(model2.parameters(), lr=1e-3, final_lr=0.1)\n",
        "#optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
        "criterion2 = loss_function_vae\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "best_n100 = -np.inf\n",
        "update_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/panda_down/lib/python3.8/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 3.01s | valid loss 972.23 | n100 0.345 | r10 0.281 | r20 0.258 | r50 0.312\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/panda_down/lib/python3.8/site-packages/torch/serialization.py:401: UserWarning: Couldn't retrieve source code for container of type MultiVAE. It won't be checked for correctness upon loading.\n",
            "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 2.83s | valid loss 952.94 | n100 0.383 | r10 0.323 | r20 0.291 | r50 0.344\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 2.83s | valid loss 941.27 | n100 0.405 | r10 0.340 | r20 0.311 | r50 0.367\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 2.77s | valid loss 934.64 | n100 0.415 | r10 0.349 | r20 0.318 | r50 0.378\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 2.74s | valid loss 929.41 | n100 0.424 | r10 0.362 | r20 0.329 | r50 0.386\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 2.84s | valid loss 925.61 | n100 0.430 | r10 0.368 | r20 0.333 | r50 0.390\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 2.92s | valid loss 920.56 | n100 0.434 | r10 0.367 | r20 0.336 | r50 0.397\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 2.79s | valid loss 917.28 | n100 0.439 | r10 0.373 | r20 0.341 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 2.79s | valid loss 913.99 | n100 0.439 | r10 0.370 | r20 0.341 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 2.83s | valid loss 911.36 | n100 0.437 | r10 0.364 | r20 0.338 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 2.82s | valid loss 908.58 | n100 0.438 | r10 0.363 | r20 0.339 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 2.78s | valid loss 906.13 | n100 0.441 | r10 0.375 | r20 0.343 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 2.85s | valid loss 903.81 | n100 0.441 | r10 0.371 | r20 0.343 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 2.84s | valid loss 901.80 | n100 0.441 | r10 0.373 | r20 0.342 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 2.94s | valid loss 900.19 | n100 0.439 | r10 0.365 | r20 0.339 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 2.88s | valid loss 898.70 | n100 0.441 | r10 0.371 | r20 0.343 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 3.07s | valid loss 897.44 | n100 0.440 | r10 0.368 | r20 0.338 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 2.98s | valid loss 895.57 | n100 0.438 | r10 0.362 | r20 0.338 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 2.89s | valid loss 894.34 | n100 0.438 | r10 0.367 | r20 0.337 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 2.83s | valid loss 893.41 | n100 0.438 | r10 0.364 | r20 0.338 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 2.84s | valid loss 892.45 | n100 0.439 | r10 0.366 | r20 0.339 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 3.00s | valid loss 891.45 | n100 0.440 | r10 0.363 | r20 0.339 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 2.97s | valid loss 890.82 | n100 0.437 | r10 0.361 | r20 0.334 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 2.97s | valid loss 889.77 | n100 0.439 | r10 0.364 | r20 0.338 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 2.99s | valid loss 889.80 | n100 0.437 | r10 0.359 | r20 0.332 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 3.03s | valid loss 889.14 | n100 0.433 | r10 0.354 | r20 0.331 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 2.94s | valid loss 887.91 | n100 0.437 | r10 0.364 | r20 0.338 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 2.87s | valid loss 887.56 | n100 0.438 | r10 0.363 | r20 0.336 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 2.94s | valid loss 887.56 | n100 0.439 | r10 0.365 | r20 0.337 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 3.00s | valid loss 887.02 | n100 0.438 | r10 0.362 | r20 0.339 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 2.93s | valid loss 887.37 | n100 0.437 | r10 0.366 | r20 0.338 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 2.98s | valid loss 886.19 | n100 0.438 | r10 0.364 | r20 0.337 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 2.94s | valid loss 885.96 | n100 0.438 | r10 0.364 | r20 0.339 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 2.95s | valid loss 886.00 | n100 0.440 | r10 0.367 | r20 0.339 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 2.96s | valid loss 885.43 | n100 0.437 | r10 0.360 | r20 0.333 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 2.95s | valid loss 885.78 | n100 0.437 | r10 0.364 | r20 0.337 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 2.93s | valid loss 885.78 | n100 0.438 | r10 0.364 | r20 0.336 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 2.93s | valid loss 885.24 | n100 0.438 | r10 0.364 | r20 0.338 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 2.95s | valid loss 884.35 | n100 0.439 | r10 0.362 | r20 0.337 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 2.94s | valid loss 884.59 | n100 0.438 | r10 0.361 | r20 0.336 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 2.95s | valid loss 884.68 | n100 0.439 | r10 0.365 | r20 0.338 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 2.85s | valid loss 884.70 | n100 0.439 | r10 0.366 | r20 0.337 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 2.94s | valid loss 883.31 | n100 0.439 | r10 0.364 | r20 0.337 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 2.91s | valid loss 883.17 | n100 0.440 | r10 0.361 | r20 0.337 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 2.88s | valid loss 883.68 | n100 0.438 | r10 0.358 | r20 0.332 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 2.87s | valid loss 883.43 | n100 0.438 | r10 0.360 | r20 0.335 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 2.95s | valid loss 883.84 | n100 0.439 | r10 0.364 | r20 0.337 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 2.89s | valid loss 883.96 | n100 0.442 | r10 0.369 | r20 0.340 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 3.01s | valid loss 883.66 | n100 0.439 | r10 0.361 | r20 0.334 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 2.90s | valid loss 883.35 | n100 0.440 | r10 0.363 | r20 0.335 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 2.95s | valid loss 882.72 | n100 0.438 | r10 0.359 | r20 0.336 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 2.90s | valid loss 883.26 | n100 0.438 | r10 0.361 | r20 0.333 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 2.90s | valid loss 882.59 | n100 0.438 | r10 0.360 | r20 0.334 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 2.93s | valid loss 882.72 | n100 0.440 | r10 0.359 | r20 0.337 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 2.93s | valid loss 882.89 | n100 0.440 | r10 0.362 | r20 0.337 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 2.98s | valid loss 883.10 | n100 0.436 | r10 0.357 | r20 0.333 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 2.94s | valid loss 882.89 | n100 0.438 | r10 0.361 | r20 0.336 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 2.92s | valid loss 882.83 | n100 0.440 | r10 0.362 | r20 0.336 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 2.83s | valid loss 882.89 | n100 0.438 | r10 0.358 | r20 0.333 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 2.82s | valid loss 882.62 | n100 0.439 | r10 0.359 | r20 0.334 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time: 2.92s | valid loss 882.68 | n100 0.440 | r10 0.360 | r20 0.336 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time: 2.87s | valid loss 882.75 | n100 0.436 | r10 0.355 | r20 0.333 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time: 2.87s | valid loss 882.52 | n100 0.440 | r10 0.362 | r20 0.337 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time: 2.89s | valid loss 884.06 | n100 0.437 | r10 0.358 | r20 0.334 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time: 2.82s | valid loss 882.73 | n100 0.440 | r10 0.359 | r20 0.334 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time: 2.94s | valid loss 883.08 | n100 0.436 | r10 0.351 | r20 0.334 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time: 2.92s | valid loss 884.12 | n100 0.436 | r10 0.355 | r20 0.332 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time: 2.96s | valid loss 883.48 | n100 0.437 | r10 0.356 | r20 0.333 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time: 2.99s | valid loss 882.92 | n100 0.440 | r10 0.361 | r20 0.336 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time: 2.90s | valid loss 882.81 | n100 0.438 | r10 0.358 | r20 0.333 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time: 2.88s | valid loss 884.00 | n100 0.435 | r10 0.354 | r20 0.330 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time: 2.80s | valid loss 882.96 | n100 0.437 | r10 0.357 | r20 0.334 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time: 2.81s | valid loss 883.06 | n100 0.436 | r10 0.355 | r20 0.330 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time: 2.90s | valid loss 883.48 | n100 0.438 | r10 0.357 | r20 0.331 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time: 2.83s | valid loss 884.34 | n100 0.434 | r10 0.356 | r20 0.331 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time: 2.98s | valid loss 883.09 | n100 0.435 | r10 0.352 | r20 0.330 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time: 2.98s | valid loss 882.87 | n100 0.438 | r10 0.354 | r20 0.330 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time: 2.82s | valid loss 884.51 | n100 0.430 | r10 0.347 | r20 0.325 | r50 0.397\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time: 2.85s | valid loss 884.16 | n100 0.434 | r10 0.350 | r20 0.330 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time: 2.81s | valid loss 884.69 | n100 0.437 | r10 0.355 | r20 0.333 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time: 2.85s | valid loss 884.70 | n100 0.429 | r10 0.346 | r20 0.325 | r50 0.397\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time: 2.89s | valid loss 882.90 | n100 0.434 | r10 0.350 | r20 0.328 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time: 2.98s | valid loss 883.58 | n100 0.435 | r10 0.351 | r20 0.330 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time: 2.89s | valid loss 883.82 | n100 0.435 | r10 0.352 | r20 0.330 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time: 2.88s | valid loss 884.24 | n100 0.435 | r10 0.353 | r20 0.329 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time: 2.88s | valid loss 884.46 | n100 0.434 | r10 0.355 | r20 0.332 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time: 2.93s | valid loss 884.33 | n100 0.431 | r10 0.347 | r20 0.327 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time: 2.91s | valid loss 884.21 | n100 0.435 | r10 0.350 | r20 0.330 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time: 2.88s | valid loss 884.23 | n100 0.432 | r10 0.347 | r20 0.326 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time: 2.88s | valid loss 884.35 | n100 0.432 | r10 0.348 | r20 0.328 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time: 2.82s | valid loss 884.38 | n100 0.430 | r10 0.346 | r20 0.326 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time: 2.91s | valid loss 884.62 | n100 0.434 | r10 0.349 | r20 0.329 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time: 2.87s | valid loss 884.36 | n100 0.432 | r10 0.347 | r20 0.324 | r50 0.400\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time: 2.94s | valid loss 884.64 | n100 0.431 | r10 0.345 | r20 0.326 | r50 0.400\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time: 2.95s | valid loss 885.32 | n100 0.429 | r10 0.342 | r20 0.326 | r50 0.399\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time: 2.90s | valid loss 884.70 | n100 0.432 | r10 0.348 | r20 0.326 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time: 2.82s | valid loss 885.44 | n100 0.430 | r10 0.344 | r20 0.325 | r50 0.399\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time: 2.87s | valid loss 884.90 | n100 0.429 | r10 0.343 | r20 0.324 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time: 2.93s | valid loss 885.39 | n100 0.428 | r10 0.345 | r20 0.324 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time: 2.79s | valid loss 885.14 | n100 0.432 | r10 0.346 | r20 0.327 | r50 0.400\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 919.90 | n100 0.44 | r10 0.36 | r20 0.34 | r50 0.40\n",
            "=========================================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "r10_fin_list = []\n",
        "\n",
        "best_r10 = -np.inf\n",
        "# save best model as r10\n",
        "for epoch in range(1, 100 + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_loss = train(model2, criterion2, optimizer2, is_VAE=True)\n",
        "    val_loss, n100, r10, r20, r50 = evaluate(model2, criterion2, vad_data_tr, vad_data_te, is_VAE=True)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
        "            'n100 {:5.3f} | r10 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
        "                epoch, time.time() - epoch_start_time, val_loss,\n",
        "                n100, r10, r20, r50))\n",
        "    print('-' * 89)\n",
        "\n",
        "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
        "    train_loss_list.append(train_loss)\n",
        "    val_loss_list.append(val_loss)\n",
        "    r10_fin_list.append(r10)\n",
        "\n",
        "    # Save the model if the r10 is the best we've seen so far.\n",
        "    if r10 > best_r10:\n",
        "        with open('VAE_'+args.save, 'wb') as f:\n",
        "            torch.save(model2, f)\n",
        "        best_r10 = r10\n",
        "        print(\"Better performance! save best model...\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"n100\": n100,\n",
        "        \"r10\": r10, \n",
        "        \"r20\": r20,\n",
        "        \"r50\": r50\n",
        "    })\n",
        "\n",
        "\n",
        "# Load the best saved model.\n",
        "with open('VAE_'+args.save, 'rb') as f:\n",
        "    model2 = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss, n100, r10, r20, r50 = evaluate(model2, criterion2, test_data_tr, test_data_te, is_VAE=True)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r10 {:4.2f} | r20 {:4.2f} | '\n",
        "        'r50 {:4.2f}'.format(test_loss, n100, r10, r20, r50))\n",
        "print('=' * 89)\n",
        "wandb.watch(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 배치사이즈 포함\n",
        "def numerize_for_infer(tp, profile2id, show2id):\n",
        "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
        "    sid = tp['item'].apply(lambda x: show2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
        "\n",
        "### 데이터 준비    \n",
        "infer_df = numerize_for_infer(raw_data, profile2id, show2id)\n",
        "\n",
        "loader = DataLoader(args.data)\n",
        "n_items = loader.load_n_items()\n",
        "\n",
        "n_users = infer_df['uid'].max() + 1\n",
        "\n",
        "rows, cols = infer_df['uid'], infer_df['sid']\n",
        "data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                                 (rows, cols)), dtype='float64',\n",
        "                                 shape=(n_users, n_items))\n",
        "\n",
        "N = data.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "model.eval()\n",
        "model2.eval()\n",
        "total_loss = 0.0\n",
        "e_idxlist = list(range(data.shape[0]))\n",
        "e_N = data.shape[0]\n",
        "pred_list = None\n",
        "user_genre_prefer.drop('users', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/panda_down/lib/python3.8/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(31360, 10)\n"
          ]
        }
      ],
      "source": [
        "total_loss = 0\n",
        "total_loss2 = 0\n",
        "with torch.no_grad():\n",
        "    for start_idx in range(0, e_N, args.batch_size):\n",
        "        end_idx = min(start_idx + args.batch_size, N)\n",
        "        data_batch = data[e_idxlist[start_idx:end_idx]]\n",
        "\n",
        "        data_tensor = naive_sparse2tensor(data_batch).to(device)\n",
        "        data_tensor2 = naive_sparse2tensor(data_batch).to(device)\n",
        "\n",
        "        if args.total_anneal_steps > 0:\n",
        "            anneal = min(args.anneal_cap, 1. * update_count / args.total_anneal_steps)\n",
        "        else:\n",
        "            anneal = args.anneal_cap\n",
        "        \n",
        "        recon_batch = model(data_tensor)\n",
        "        loss = criterion(recon_batch, data_tensor)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        recon_batch2, mu, logvar = model2(data_tensor2)\n",
        "        loss2 = criterion2(recon_batch2, data_tensor2, mu, logvar, anneal)\n",
        "        total_loss2 += loss2.item()\n",
        "\n",
        "        # Exclude examples from training set\n",
        "        recon_batch2 = recon_batch2.cpu().numpy()\n",
        "        recon_batch = recon_batch.cpu().numpy()\n",
        "        \n",
        "        recon_batch = np.add(recon_batch, recon_batch2) # 1:1로 앙상블\n",
        "        recon_batch = recon_batch + np.mean(item_genre_emb.values)*20 + user_genre_prefer.values[e_idxlist[start_idx:end_idx]]*20\n",
        "        recon_batch[data_batch.nonzero()] = -np.inf\n",
        "  \n",
        "        ##Recall\n",
        "        batch_users = recon_batch.shape[0]\n",
        "        idx = bn.argpartition(-recon_batch, 10, axis=1)[:, :10]\n",
        "        if start_idx == 0:\n",
        "            pred_list = idx\n",
        "        else:\n",
        "            pred_list = np.append(pred_list, idx, axis=0)\n",
        "\n",
        "print(pred_list.shape)\n",
        "## sample_submission에 맞게끔 바꾸기\n",
        "user2 = []\n",
        "item2 = []\n",
        "for i_idx, arr_10 in enumerate(pred_list):\n",
        "    user2.extend([i_idx]*10)\n",
        "    item2.extend(arr_10)\n",
        "\n",
        "u2 = pd.DataFrame(user2, columns=['user'])\n",
        "i2 = pd.DataFrame(item2, columns=['item'])\n",
        "all2 = pd.concat([u2, i2], axis=1)\n",
        "\n",
        "re_p2id = dict((v, k) for k, v in profile2id.items())\n",
        "re_s2id = dict((v, k) for k, v in show2id.items())\n",
        "\n",
        "def de_numerize(tp, re_p2id, re_s2id):\n",
        "    uid2 = tp['user'].apply(lambda x: re_p2id[x])\n",
        "    sid2 = tp['item'].apply(lambda x: re_s2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid2, 'sid': sid2}, columns=['uid', 'sid'])\n",
        "\n",
        "ans2 = de_numerize(all2, re_p2id, re_s2id)\n",
        "ans2.columns = ['user', 'item']\n",
        "new_ans2 = ans2.sort_values('user')\n",
        "\n",
        "### 확인용\n",
        "submit_data = pd.read_csv('/opt/ml/input/data/eval/sample_submission.csv', sep=',')\n",
        "sum(new_ans2.user.values == submit_data.user.values)\n",
        "new_ans2.reset_index(drop=True, inplace=True)\n",
        "new_ans2.to_csv('0405_user_item_prefer.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/panda_down/lib/python3.8/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ -1.7009397   -2.4967341   -1.6319823  ... -16.350296    -8.83639\n",
            "  -10.603864  ]\n",
            " [  5.370331     6.2508354    0.9975498  ... -11.320208   -11.89768\n",
            "   -4.7585497 ]\n",
            " [  2.1687698    3.367939     0.51937413 ...  -6.740138   -13.162594\n",
            "   -4.848978  ]\n",
            " ...\n",
            " [ -1.8735077   -0.25055498  -3.3098485  ... -10.564044    -2.3500547\n",
            "   -8.045005  ]\n",
            " [ -3.3119338   -2.2483325   -0.8973126  ...  -8.552555    -1.1070881\n",
            "   -8.202363  ]\n",
            " [ -3.106039    -0.7413424    6.3965263  ...  -7.470235    -8.992666\n",
            "   -8.2287855 ]]\n"
          ]
        }
      ],
      "source": [
        "##배치없이 -> 오류\n",
        "### 예측하기\n",
        "total_loss = 0\n",
        "total_loss2 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    data_tensor = naive_sparse2tensor(data).to(device)\n",
        "    data_tensor2 = naive_sparse2tensor(data).to(device)\n",
        "\n",
        "    if args.total_anneal_steps > 0:\n",
        "        anneal = min(args.anneal_cap, 1. * update_count / args.total_anneal_steps)\n",
        "    else:\n",
        "        anneal = args.anneal_cap\n",
        "        \n",
        "    recon_batch = model(data_tensor)\n",
        "    loss = criterion(recon_batch, data_tensor)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    recon_batch2, mu, logvar = model2(data_tensor2)\n",
        "    loss2 = criterion2(recon_batch2, data_tensor2, mu, logvar, anneal)\n",
        "    total_loss2 += loss2.item()\n",
        "\n",
        "# Exclude examples from training set\n",
        "recon_batch2 = recon_batch2.cpu().numpy()\n",
        "recon_batch = recon_batch.cpu().numpy()\n",
        "        \n",
        "recon_batch = np.add(recon_batch, recon_batch2) # 1:1로 앙상블\n",
        "print(recon_batch)\n",
        "recon_batch = recon_batch + np.mean(item_genre_emb.values)*20 + user_genre_prefer.values*20\n",
        "recon_batch[data.nonzero()] = -np.inf\n",
        "\n",
        "idx = bn.argpartition(-recon_batch, 10, axis=1)[:, :10]\n",
        "\n",
        "\n",
        "### sample_submission 형태로 바꾸고 id 다시 바꾸기\n",
        "user = []\n",
        "item = []\n",
        "for i_idx, arr_10 in enumerate(idx):\n",
        "    user.extend([i_idx]*10)\n",
        "    item.extend(arr_10)\n",
        "\n",
        "u = pd.DataFrame(user, columns=['user'])\n",
        "i = pd.DataFrame(item, columns=['item'])\n",
        "all = pd.concat([u, i], axis=1)\n",
        "\n",
        "re_p2id = dict((v, k) for k, v in profile2id.items())\n",
        "re_s2id = dict((v, k) for k, v in show2id.items())\n",
        "\n",
        "def de_numerize(tp, re_p2id, re_s2id):\n",
        "    uid = tp['user'].apply(lambda x: re_p2id[x])\n",
        "    sid = tp['item'].apply(lambda x: re_s2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
        "\n",
        "ans = de_numerize(all, re_p2id, re_s2id)\n",
        "ans.columns = ['user', 'item']\n",
        "new_ans = ans.sort_values('user')\n",
        "\n",
        "### 확인용\n",
        "submit_data = pd.read_csv('/opt/ml/input/data/eval/sample_submission.csv', sep=',')\n",
        "sum(new_ans.user.values == submit_data.user.values)\n",
        "new_ans.reset_index(drop=True, inplace=True)\n",
        "new_ans.to_csv('0405_second.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Mission2_Multi-VAE-정답.ipynb의 사본",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
