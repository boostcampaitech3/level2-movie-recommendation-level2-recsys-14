{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험 내용\n",
    "- Multi-VAE와 Multi-DAE를 ensemble한 것을 다루고 있었을 때, sun1187님의 side information을 적용 실험을 보고, year side information을 사용하고자 했습니다.\n",
    "- 따라서, 학습을 진행할 때, genre emb, title emb, year emb을 넣어주었고, loss 계산할 때도 반영해주면서 학습될 수 있게끔 실험을 진행했습니다.\n",
    "- 데이터 EDA를 통해 유저들이 1993년 이전에 개봉한 영화들은 많이 소비하지 않는 것을 확인했고, 따라서 1993년 이후에 개봉한 영화들에 가중을 둔 emb을 적용해보았습니다.\n",
    "\n",
    "# 실험 결과\n",
    "- year side information을 적용했을 때에는 genre와 title을 적용했을 때와 달리 성능 향상으로 이루어지지 않았습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 초기 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import bottleneck as bn\n",
    "import adabound\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## 각종 파라미터 세팅\n",
    "parser = argparse.ArgumentParser(description='PyTorch Variational Autoencoders for Collaborative Filtering')\n",
    "\n",
    "\n",
    "parser.add_argument('--data', type=str, default='/opt/ml/input/data/train/',\n",
    "                    help='Movielens dataset location')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--wd', type=float, default=0.00,\n",
    "                    help='weight decay coefficient')\n",
    "parser.add_argument('--batch_size', type=int, default=500,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=50, #원래 20\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--total_anneal_steps', type=int, default=200000,\n",
    "                    help='the total number of gradient updates for annealing')\n",
    "parser.add_argument('--anneal_cap', type=float, default=0.2,\n",
    "                    help='largest annealing parameter')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--log_interval', type=int, default=100, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--dae_save', type=str, default='multi_dae_model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--vae_save', type=str, default='multi_vae_model.pt',\n",
    "                    help='path to save the final model')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproductibility.\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "#만약 GPU가 사용가능한 환경이라면 GPU를 사용\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'item')\n",
    "        tp = tp[tp['item'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'user')\n",
    "        tp = tp[tp['user'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    usercount, itemcount = get_count(tp, 'user'), get_count(tp, 'item')\n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2): #원래 0.2\n",
    "    data_grouped_by_user = data.groupby('user')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for _, group in data_grouped_by_user:\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def numerize(tp, profile2id, show2id):\n",
    "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and Preprocess Movielens dataset\n",
      "원본 데이터\n",
      "            user   item        time\n",
      "0            11   4643  1230782529\n",
      "1            11    170  1230782534\n",
      "2            11    531  1230782539\n",
      "3            11    616  1230782542\n",
      "4            11   2140  1230782563\n",
      "...         ...    ...         ...\n",
      "5154466  138493  44022  1260209449\n",
      "5154467  138493   4958  1260209482\n",
      "5154468  138493  68319  1260209720\n",
      "5154469  138493  40819  1260209726\n",
      "5154470  138493  27311  1260209807\n",
      "\n",
      "[5154471 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "print(\"Load and Preprocess Movielens dataset\")\n",
    "DATA_DIR = args.data\n",
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train_ratings.csv'), header=0)\n",
    "print(\"원본 데이터\\n\", raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\n",
      "            user   item        time\n",
      "0            11   4643  1230782529\n",
      "1            11    170  1230782534\n",
      "2            11    531  1230782539\n",
      "3            11    616  1230782542\n",
      "4            11   2140  1230782563\n",
      "...         ...    ...         ...\n",
      "5154466  138493  44022  1260209449\n",
      "5154467  138493   4958  1260209482\n",
      "5154468  138493  68319  1260209720\n",
      "5154469  138493  40819  1260209726\n",
      "5154470  138493  27311  1260209807\n",
      "\n",
      "[5154471 rows x 3 columns]\n",
      "유저별 리뷰수\n",
      " user\n",
      "11        376\n",
      "14        180\n",
      "18         77\n",
      "25         91\n",
      "31        154\n",
      "         ... \n",
      "138473     63\n",
      "138475    124\n",
      "138486    137\n",
      "138492     68\n",
      "138493    314\n",
      "Length: 31360, dtype: int64\n",
      "아이템별 리뷰수\n",
      " item\n",
      "1         12217\n",
      "2          3364\n",
      "3           734\n",
      "4            43\n",
      "5           590\n",
      "          ...  \n",
      "118700       54\n",
      "118900       60\n",
      "118997       52\n",
      "119141      122\n",
      "119145       78\n",
      "Length: 6807, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter Data\n",
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=5, min_sc=0)\n",
    "#제공된 훈련데이터의 유저는 모두 5개 이상의 리뷰가 있습니다.\n",
    "print(\"5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\\n\",raw_data)\n",
    "\n",
    "print(\"유저별 리뷰수\\n\",user_activity)\n",
    "print(\"아이템별 리뷰수\\n\",item_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BEFORE) unique_uid: Int64Index([    11,     14,     18,     25,     31,     35,     43,     50,\n",
      "                58,     60,\n",
      "            ...\n",
      "            138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486,\n",
      "            138492, 138493],\n",
      "           dtype='int64', name='user', length=31360)\n",
      "(AFTER) unique_uid: Int64Index([ 27968,  67764,   2581,  82969, 137831,  48639,  97870,  40424,\n",
      "             46835,  79570,\n",
      "            ...\n",
      "            114284,   9009,  21165,  33920,  22054, 135379, 125855,  41891,\n",
      "             15720,  17029],\n",
      "           dtype='int64', name='user', length=31360)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle User Indices\n",
    "unique_uid = user_activity.index\n",
    "print(\"(BEFORE) unique_uid:\", unique_uid)\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]\n",
    "print(\"(AFTER) unique_uid:\",unique_uid)\n",
    "\n",
    "n_users = unique_uid.size #31360\n",
    "n_heldout_users = 3136 #3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에 사용될 사용자 수: 25088\n",
      "검증 데이터에 사용될 사용자 수: 3136\n",
      "테스트 데이터에 사용될 사용자 수: 3136\n"
     ]
    }
   ],
   "source": [
    "# Split Train/Validation/Test User Indices\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "\n",
    "#주의: 데이터의 수가 아닌 사용자의 수입니다.\n",
    "print(\"훈련 데이터에 사용될 사용자 수:\", len(tr_users))\n",
    "print(\"검증 데이터에 사용될 사용자 수:\", len(vd_users))\n",
    "print(\"테스트 데이터에 사용될 사용자 수:\", len(te_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##훈련 데이터에 해당하는 아이템들\n",
    "#Train에는 전체 데이터를 사용합니다.\n",
    "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
    "\n",
    "##아이템 ID\n",
    "unique_sid = pd.unique(train_plays['item'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
    "\n",
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Validation과 Test에는 input으로 사용될 tr 데이터와 정답을 확인하기 위한 te 데이터로 분리되었습니다.\n",
    "vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "\n",
    "test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_data = numerize(train_plays, profile2id, show2id)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
    "\n",
    "\n",
    "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           uid   sid\n",
      "0        11825     0\n",
      "1        11825     1\n",
      "2        11825     2\n",
      "3        11825     3\n",
      "4        11825     4\n",
      "...        ...   ...\n",
      "5154466  10783   477\n",
      "5154467  10783  1325\n",
      "5154468  10783   331\n",
      "5154469  10783   558\n",
      "5154470  10783  1922\n",
      "\n",
      "[4125303 rows x 2 columns]\n",
      "           uid   sid\n",
      "376      26554   440\n",
      "377      26554   741\n",
      "378      26554  1407\n",
      "379      26554   193\n",
      "380      26554  1041\n",
      "...        ...   ...\n",
      "5153247  26934   760\n",
      "5153248  26934   697\n",
      "5153249  26934  3232\n",
      "5153250  26934  1369\n",
      "5153251  26934  3679\n",
      "\n",
      "[415395 rows x 2 columns]\n",
      "           uid   sid\n",
      "382      26554  3012\n",
      "383      26554  1681\n",
      "384      26554   201\n",
      "399      26554  3177\n",
      "401      26554  3289\n",
      "...        ...   ...\n",
      "5153229  26934   737\n",
      "5153233  26934   228\n",
      "5153236  26934   235\n",
      "5153240  26934  3962\n",
      "5153243  26934  1086\n",
      "\n",
      "[102295 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#데이터 셋 확인\n",
    "print(train_data)\n",
    "print(vad_data_tr)\n",
    "print(vad_data_te)\n",
    "# print(test_data_tr)\n",
    "# print(test_data_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Loader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    '''\n",
    "    Load Movielens dataset\n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
    "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
    "\n",
    "        self.n_items = self.load_n_items()\n",
    "\n",
    "    def load_data(self, datatype='train'):\n",
    "        if datatype == 'train':\n",
    "            return self._load_train_data()\n",
    "        elif datatype == 'validation':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        elif datatype == 'test':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        else:\n",
    "            raise ValueError(\"datatype should be in [train, validation, test]\")\n",
    "\n",
    "    def load_n_items(self):\n",
    "        unique_sid = list()\n",
    "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                unique_sid.append(line.strip())\n",
    "        n_items = len(unique_sid)\n",
    "        return n_items\n",
    "\n",
    "    def _load_train_data(self):\n",
    "        path = os.path.join(self.pro_dir, 'train.csv')\n",
    "\n",
    "        tp = pd.read_csv(path)\n",
    "        n_users = tp['uid'].max() + 1\n",
    "\n",
    "        rows, cols = tp['uid'], tp['sid']\n",
    "        data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, self.n_items))\n",
    "        return data\n",
    "\n",
    "    def _load_tr_te_data(self, datatype='test'):\n",
    "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
    "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
    "\n",
    "        tp_tr = pd.read_csv(tr_path)\n",
    "        tp_te = pd.read_csv(te_path)\n",
    "\n",
    "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        return data_tr, data_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word2vec를 활용하여 side information 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Genre Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from urllib.request import urlretrieve, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", filename=\"GoogleNews-vectors-negative300.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 크기(shape) : (3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "print('모델의 크기(shape) :',word2vec_model.vectors.shape) # 모델의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>318</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>318</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2571</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2571</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2571</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item     genre\n",
       "0   318     Crime\n",
       "1   318     Drama\n",
       "2  2571    Action\n",
       "3  2571    Sci-Fi\n",
       "4  2571  Thriller"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = pd.read_csv(\"/opt/ml/input/data/train/genres.tsv\", delimiter='\\t')\n",
    "gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gen_numerize(tp, show2id):\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item     genre\n",
       "0   198     Crime\n",
       "1   198     Drama\n",
       "2    82    Action\n",
       "3    82    Sci-Fi\n",
       "4    82  Thriller"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen['item'] = gen_numerize(gen, show2id)\n",
    "gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      genre\n",
       "0     Drama\n",
       "1    Comedy\n",
       "2  Thriller\n",
       "3   Romance\n",
       "4    Action"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_emb = pd.DataFrame(gen['genre'].value_counts().index.values, columns=['genre'])\n",
    "gen_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emb_list = []\n",
    "for x in gen_emb.genre:\n",
    "    if x == 'Sci-Fi':\n",
    "        emb_list.append(word2vec_model['science_fiction'])\n",
    "    elif x == 'Film-Noir':\n",
    "        emb_list.append(word2vec_model['Film_Noir'])\n",
    "    else:\n",
    "        emb_list.append(word2vec_model[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genre</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drama</th>\n",
       "      <td>-0.144531</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>0.013855</td>\n",
       "      <td>-0.111816</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>-0.127930</td>\n",
       "      <td>-0.320312</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222656</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>-0.589844</td>\n",
       "      <td>-0.184570</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>-0.055908</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>0.247070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comedy</th>\n",
       "      <td>-0.032959</td>\n",
       "      <td>-0.077637</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.043213</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>-0.054443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443359</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>-0.443359</td>\n",
       "      <td>-0.024048</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thriller</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.314453</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.192383</td>\n",
       "      <td>-0.390625</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>0.019165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>-0.396484</td>\n",
       "      <td>-0.028076</td>\n",
       "      <td>0.072266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romance</th>\n",
       "      <td>0.041992</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>-0.005249</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116699</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.322266</td>\n",
       "      <td>-0.013245</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>-0.083008</td>\n",
       "      <td>-0.121094</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>0.291016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Action</th>\n",
       "      <td>0.053955</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>0.049316</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>-0.059570</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>0.108887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>-0.242188</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.024170</td>\n",
       "      <td>0.016968</td>\n",
       "      <td>0.049072</td>\n",
       "      <td>0.011475</td>\n",
       "      <td>-0.025513</td>\n",
       "      <td>-0.099121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5    \\\n",
       "genre                                                                  \n",
       "Drama    -0.144531 -0.055420  0.013855 -0.111816  0.187500  0.022095   \n",
       "Comedy   -0.032959 -0.077637 -0.065918  0.291016  0.041016  0.043213   \n",
       "Thriller  0.217773 -0.064941  0.187500  0.314453 -0.047363 -0.192383   \n",
       "Romance   0.041992 -0.075195 -0.341797  0.119629  0.185547 -0.005249   \n",
       "Action    0.053955 -0.031250  0.242188  0.049316  0.023315 -0.080566   \n",
       "\n",
       "               6         7         8         9    ...       290       291  \\\n",
       "genre                                             ...                       \n",
       "Drama    -0.277344 -0.127930 -0.320312  0.032227  ... -0.222656 -0.100098   \n",
       "Comedy    0.151367  0.273438  0.097656 -0.054443  ...  0.443359  0.078125   \n",
       "Thriller -0.390625  0.135742  0.168945  0.019165  ... -0.058594  0.003937   \n",
       "Romance  -0.057617 -0.179688 -0.090332  0.179688  ...  0.116699 -0.078613   \n",
       "Action   -0.059570  0.033203 -0.310547  0.108887  ...  0.009277 -0.189453   \n",
       "\n",
       "               292       293       294       295       296       297  \\\n",
       "genre                                                                  \n",
       "Drama    -0.589844 -0.184570  0.189453  0.195312 -0.113281 -0.055908   \n",
       "Comedy   -0.443359 -0.024048 -0.036621  0.253906 -0.046631 -0.045898   \n",
       "Thriller -0.267578  0.235352  0.271484 -0.000277 -0.034180 -0.396484   \n",
       "Romance  -0.322266 -0.013245  0.353516 -0.083008 -0.121094  0.022095   \n",
       "Action   -0.242188  0.067383  0.024170  0.016968  0.049072  0.011475   \n",
       "\n",
       "               298       299  \n",
       "genre                         \n",
       "Drama     0.052490  0.247070  \n",
       "Comedy    0.038818  0.074219  \n",
       "Thriller -0.028076  0.072266  \n",
       "Romance  -0.035156  0.291016  \n",
       "Action   -0.025513 -0.099121  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.concat([gen_emb, pd.DataFrame(emb_list)], axis=1)\n",
    "a = x.set_index('genre', drop=True)\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gen2emb = dict((x, a.loc[x].values) for (i, x) in enumerate(a.index))\n",
    "gen['emb'] = gen['genre'].apply(lambda x: gen2emb[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>genre</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198</td>\n",
       "      <td>Crime</td>\n",
       "      <td>[0.028076171875, 0.0048828125, -0.0966796875, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[-0.14453125, -0.055419921875, 0.0138549804687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Action</td>\n",
       "      <td>[0.053955078125, -0.03125, 0.2421875, 0.049316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>[0.1298828125, -0.1259765625, 0.1796875, 0.251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>[0.2177734375, -0.06494140625, 0.1875, 0.31445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>260</td>\n",
       "      <td>Action</td>\n",
       "      <td>[0.053955078125, -0.03125, 0.2421875, 0.049316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>260</td>\n",
       "      <td>Crime</td>\n",
       "      <td>[0.028076171875, 0.0048828125, -0.0966796875, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>260</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[-0.14453125, -0.055419921875, 0.0138549804687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>260</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>[0.2177734375, -0.06494140625, 0.1875, 0.31445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>264</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>[-0.032958984375, -0.07763671875, -0.065917968...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item     genre                                                emb\n",
       "0   198     Crime  [0.028076171875, 0.0048828125, -0.0966796875, ...\n",
       "1   198     Drama  [-0.14453125, -0.055419921875, 0.0138549804687...\n",
       "2    82    Action  [0.053955078125, -0.03125, 0.2421875, 0.049316...\n",
       "3    82    Sci-Fi  [0.1298828125, -0.1259765625, 0.1796875, 0.251...\n",
       "4    82  Thriller  [0.2177734375, -0.06494140625, 0.1875, 0.31445...\n",
       "5   260    Action  [0.053955078125, -0.03125, 0.2421875, 0.049316...\n",
       "6   260     Crime  [0.028076171875, 0.0048828125, -0.0966796875, ...\n",
       "7   260     Drama  [-0.14453125, -0.055419921875, 0.0138549804687...\n",
       "8   260  Thriller  [0.2177734375, -0.06494140625, 0.1875, 0.31445...\n",
       "9   264    Comedy  [-0.032958984375, -0.07763671875, -0.065917968..."
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 평균을 취하지 않고 각 아이템 별로 장르를 하나씩만 남기면 어떻게 될까요?\n",
    "# gen = gen.drop_duplicates(subset=['item'])\n",
    "# gen.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total = []\n",
    "def item_genre_emb_mean(i):\n",
    "    total.append(np.mean(gen[gen['item'] == i].emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016510</td>\n",
       "      <td>-0.062561</td>\n",
       "      <td>0.094589</td>\n",
       "      <td>0.042084</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>0.026520</td>\n",
       "      <td>-0.068420</td>\n",
       "      <td>0.137451</td>\n",
       "      <td>-0.155029</td>\n",
       "      <td>-0.025269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132629</td>\n",
       "      <td>-0.041382</td>\n",
       "      <td>-0.305176</td>\n",
       "      <td>-0.006836</td>\n",
       "      <td>0.197449</td>\n",
       "      <td>0.032135</td>\n",
       "      <td>-0.027195</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.043518</td>\n",
       "      <td>0.061951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081635</td>\n",
       "      <td>-0.032227</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>0.082504</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>-0.080139</td>\n",
       "      <td>0.152634</td>\n",
       "      <td>-0.122925</td>\n",
       "      <td>0.063751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075012</td>\n",
       "      <td>-0.030418</td>\n",
       "      <td>-0.158997</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>0.174988</td>\n",
       "      <td>0.007255</td>\n",
       "      <td>0.039490</td>\n",
       "      <td>-0.155701</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.040222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.103394</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>-0.018585</td>\n",
       "      <td>0.072998</td>\n",
       "      <td>0.104065</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>-0.024414</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>-0.175781</td>\n",
       "      <td>-0.042969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228516</td>\n",
       "      <td>-0.050545</td>\n",
       "      <td>-0.402344</td>\n",
       "      <td>-0.223145</td>\n",
       "      <td>0.194824</td>\n",
       "      <td>0.049561</td>\n",
       "      <td>-0.035889</td>\n",
       "      <td>-0.060669</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.212891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.081909</td>\n",
       "      <td>-0.063721</td>\n",
       "      <td>0.080933</td>\n",
       "      <td>0.164795</td>\n",
       "      <td>0.060608</td>\n",
       "      <td>0.061646</td>\n",
       "      <td>0.236816</td>\n",
       "      <td>0.079346</td>\n",
       "      <td>-0.057861</td>\n",
       "      <td>-0.184082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273438</td>\n",
       "      <td>-0.071785</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>-0.039551</td>\n",
       "      <td>0.143799</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>-0.082275</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>-0.091904</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.054626</td>\n",
       "      <td>-0.195557</td>\n",
       "      <td>-0.029028</td>\n",
       "      <td>-0.056213</td>\n",
       "      <td>0.179199</td>\n",
       "      <td>-0.013428</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.259277</td>\n",
       "      <td>-0.032227</td>\n",
       "      <td>-0.060394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173462</td>\n",
       "      <td>0.118408</td>\n",
       "      <td>-0.044434</td>\n",
       "      <td>-0.103149</td>\n",
       "      <td>0.124237</td>\n",
       "      <td>-0.050903</td>\n",
       "      <td>-0.112061</td>\n",
       "      <td>-0.040283</td>\n",
       "      <td>-0.031189</td>\n",
       "      <td>0.062378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.016510 -0.062561  0.094589  0.042084  0.043182  0.026520 -0.068420   \n",
       "1  0.081635 -0.032227  0.068909  0.082504  0.009857 -0.055420 -0.080139   \n",
       "2 -0.103394  0.007202 -0.018585  0.072998  0.104065  0.027039 -0.024414   \n",
       "3 -0.081909 -0.063721  0.080933  0.164795  0.060608  0.061646  0.236816   \n",
       "4  0.054626 -0.195557 -0.029028 -0.056213  0.179199 -0.013428  0.020264   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0  0.137451 -0.155029 -0.025269  ... -0.132629 -0.041382 -0.305176 -0.006836   \n",
       "1  0.152634 -0.122925  0.063751  ... -0.075012 -0.030418 -0.158997  0.145264   \n",
       "2  0.055664 -0.175781 -0.042969  ... -0.228516 -0.050545 -0.402344 -0.223145   \n",
       "3  0.079346 -0.057861 -0.184082  ... -0.273438 -0.071785 -0.208496 -0.039551   \n",
       "4  0.259277 -0.032227 -0.060394  ...  0.173462  0.118408 -0.044434 -0.103149   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.197449  0.032135 -0.027195  0.064209  0.043518  0.061951  \n",
       "1  0.174988  0.007255  0.039490 -0.155701  0.000488 -0.040222  \n",
       "2  0.194824  0.049561 -0.035889 -0.060669  0.027115  0.212891  \n",
       "3  0.143799  0.001709 -0.082275  0.070801 -0.091904  0.053711  \n",
       "4  0.124237 -0.050903 -0.112061 -0.040283 -0.031189  0.062378  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_genre_emb_idx = pd.DataFrame(list(i for i in range(0, max(gen.item)+1)), columns=['item'])\n",
    "item_genre_emb_idx.item.apply(lambda x: item_genre_emb_mean(x))\n",
    "item_genre_emb = pd.DataFrame(total)\n",
    "item_genre_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 300)\n"
     ]
    }
   ],
   "source": [
    "print(item_genre_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6807)\n"
     ]
    }
   ],
   "source": [
    "item_genre_emb = item_genre_emb.T\n",
    "print(item_genre_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Title Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import sister\n",
    "sentence_embedding = sister.MeanEmbedding(lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "title = pd.read_csv(\"/opt/ml/input/data/train/titles.tsv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        item                                              title\n",
      "0        318                   Shawshank Redemption, The (1994)\n",
      "1       2571                                 Matrix, The (1999)\n",
      "2       2959                                  Fight Club (1999)\n",
      "3        296                                Pulp Fiction (1994)\n",
      "4        356                                Forrest Gump (1994)\n",
      "...      ...                                                ...\n",
      "6802   73106  American Pie Presents: The Book of Love (Ameri...\n",
      "6803  109850                              Need for Speed (2014)\n",
      "6804    8605                                      Taxi 3 (2003)\n",
      "6805    3689                    Porky's II: The Next Day (1983)\n",
      "6806    8130                         Girl Next Door, The (1999)\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def title_numerize(tp, show2id):\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item                                              title\n",
      "0      198                   Shawshank Redemption, The (1994)\n",
      "1       82                                 Matrix, The (1999)\n",
      "2      260                                  Fight Club (1999)\n",
      "3      264                                Pulp Fiction (1994)\n",
      "4      265                                Forrest Gump (1994)\n",
      "...    ...                                                ...\n",
      "6802  3396  American Pie Presents: The Book of Love (Ameri...\n",
      "6803  6763                              Need for Speed (2014)\n",
      "6804  5046                                      Taxi 3 (2003)\n",
      "6805  5508                    Porky's II: The Next Day (1983)\n",
      "6806  5531                         Girl Next Door, The (1999)\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "title['item'] = title_numerize(title, show2id)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fawlty Towers \n",
      "Big Bang Theory, The \n",
      "      item                                              title\n",
      "0      198                         Shawshank Redemption, The \n",
      "1       82                                       Matrix, The \n",
      "2      260                                        Fight Club \n",
      "3      264                                      Pulp Fiction \n",
      "4      265                                      Forrest Gump \n",
      "...    ...                                                ...\n",
      "6802  3396  American Pie Presents: The Book of Love (Ameri...\n",
      "6803  6763                                    Need for Speed \n",
      "6804  5046                                            Taxi 3 \n",
      "6805  5508                          Porky's II: The Next Day \n",
      "6806  5531                               Girl Next Door, The \n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "new_title = []\n",
    "\n",
    "for item, text in title.values:\n",
    "    if re.search(r'\\([0-9]{4}\\)', text[-6:]):\n",
    "        new_title.append([item, text[:-6]])\n",
    "    else:\n",
    "        print(text[:text.rfind('(')])\n",
    "        new_title.append([item, text[:text.rfind('(')]])\n",
    "\n",
    "new_title_df = pd.DataFrame(new_title, columns = ['item', 'title'])\n",
    "print(new_title_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emb_title_dict = {}\n",
    "for item, title in new_title_df.values:\n",
    "    emb_title_dict[item] = sentence_embedding(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item                                              title\n",
      "0      198  [0.09887906, 0.0020635799, 0.1023672, -0.16108...\n",
      "1       82  [0.20249972, -0.116814286, 0.038145967, -0.091...\n",
      "2      260  [0.15916233, -0.013797939, -0.06912231, -0.066...\n",
      "3      264  [-0.042834193, -0.026048277, 0.048151728, -0.0...\n",
      "4      265  [0.18826124, 0.06215177, 0.04759519, -0.018898...\n",
      "...    ...                                                ...\n",
      "6802  3396  [0.05855634, -0.07672383, 0.056784138, -0.1529...\n",
      "6803  6763  [0.0046750098, 0.07040122, 0.19956714, -0.1759...\n",
      "6804  5046  [0.1844265, -0.07056019, -0.04190911, 0.080615...\n",
      "6805  5508  [0.24258502, -0.008186941, 0.0679334, -0.11687...\n",
      "6806  5531  [0.16492175, -0.087725565, -0.018908694, -0.05...\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "emb_title_df = pd.DataFrame(list(emb_title_dict.items()), columns=['item', 'title'])\n",
    "print(emb_title_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item                                              title\n",
      "3492     0  [0.060705453, -0.17232932, 0.07193938, -0.0160...\n",
      "4567     1  [-0.048258446, 0.29175144, 0.024340292, -0.248...\n",
      "737      2  [0.16013275, 0.01752352, 0.09133945, -0.174270...\n",
      "731      3  [0.14779359, -0.1571878, 0.11643047, -0.173786...\n",
      "574      4  [0.22029698, 0.039300818, 0.08298388, -0.19853...\n",
      "...    ...                                                ...\n",
      "6289  6802  [0.09505787, 0.00903964, -0.09849598, -0.06931...\n",
      "6772  6803  [0.13124995, -0.05749653, 0.0032128196, -0.057...\n",
      "6760  6804  [0.20940778, -0.06612897, 0.14894299, -0.14761...\n",
      "6516  6805  [0.11617702, -0.23807585, -0.010553868, -0.435...\n",
      "6762  6806  [0.05673375, -0.018610748, -0.15926576, -0.130...\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "emb_title_df2 = emb_title_df.sort_values(by=['item'])\n",
    "print(emb_title_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6    \\\n",
      "0     0.060705 -0.172329  0.071939 -0.016023  0.019821  0.172676 -0.098369   \n",
      "1    -0.048258  0.291751  0.024340 -0.248048  0.019833 -0.042012  0.269257   \n",
      "2     0.160133  0.017524  0.091339 -0.174270  0.144272  0.108190 -0.209995   \n",
      "3     0.147794 -0.157188  0.116430 -0.173786  0.058035 -0.050341 -0.161892   \n",
      "4     0.220297  0.039301  0.082984 -0.198539  0.107078  0.112428 -0.267611   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "6802  0.095058  0.009040 -0.098496 -0.069320  0.111002 -0.080911 -0.205108   \n",
      "6803  0.131250 -0.057497  0.003213 -0.057943  0.102392  0.084665 -0.090912   \n",
      "6804  0.209408 -0.066129  0.148943 -0.147610  0.045850  0.033999 -0.186204   \n",
      "6805  0.116177 -0.238076 -0.010554 -0.435312  0.071545 -0.130239 -0.190550   \n",
      "6806  0.056734 -0.018611 -0.159266 -0.130187  0.004537  0.113627 -0.185436   \n",
      "\n",
      "           7         8         9    ...       290       291       292  \\\n",
      "0    -0.015639 -0.050379 -0.051945  ... -0.125337  0.011902  0.045414   \n",
      "1    -0.177424  0.416401  0.327475  ...  0.162379  0.000952  0.293767   \n",
      "2     0.058064  0.010407  0.159249  ... -0.077829 -0.099417 -0.094958   \n",
      "3    -0.013702  0.000790 -0.029409  ...  0.046678 -0.103129  0.011876   \n",
      "4    -0.094678  0.091287  0.039513  ...  0.021672  0.059619 -0.070881   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "6802 -0.040041  0.324735 -0.052425  ...  0.069953  0.190423 -0.086988   \n",
      "6803 -0.063237 -0.035511  0.075327  ... -0.066396  0.044142 -0.014393   \n",
      "6804 -0.088744  0.131297 -0.030582  ...  0.077845 -0.098284  0.021381   \n",
      "6805 -0.153314  0.235201 -0.091391  ...  0.006570 -0.277720 -0.003019   \n",
      "6806 -0.101376  0.000113  0.069646  ...  0.075978 -0.108760  0.108718   \n",
      "\n",
      "           293       294       295       296       297       298       299  \n",
      "0    -0.069140  0.058895  0.013648 -0.098602  0.001309 -0.170907 -0.064850  \n",
      "1    -0.234219  0.011926  0.178708 -0.051436  0.067603  0.019331  0.097408  \n",
      "2    -0.117981 -0.024513 -0.030071  0.088674  0.111612  0.068765 -0.044972  \n",
      "3    -0.138554 -0.025504  0.148756  0.031759  0.033528  0.088699  0.012696  \n",
      "4    -0.010764  0.089532  0.061731 -0.114149  0.035243 -0.023510  0.031119  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "6802 -0.140079  0.012981 -0.094743 -0.277142 -0.065319 -0.396279  0.121280  \n",
      "6803 -0.034365  0.011282  0.091712 -0.028294  0.010583  0.021816 -0.055442  \n",
      "6804 -0.179527  0.022492  0.048297 -0.070032  0.033208  0.058636 -0.032276  \n",
      "6805  0.165771  0.128352 -0.066401  0.071748  0.052234  0.060959 -0.125069  \n",
      "6806  0.011436 -0.023316  0.036824 -0.096715  0.024286 -0.007445 -0.067098  \n",
      "\n",
      "[6807 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "total_emb_title = []\n",
    "\n",
    "for text in emb_title_df2['title'].values:\n",
    "    total_emb_title.append(text)\n",
    "\n",
    "item_title_emb = pd.DataFrame(total_emb_title)\n",
    "print(item_title_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 300)\n"
     ]
    }
   ],
   "source": [
    "print(item_title_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6807)\n"
     ]
    }
   ],
   "source": [
    "item_title_emb = item_title_emb.T\n",
    "print(item_title_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### year data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1348</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44587</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4768</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8235</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8609</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6802</th>\n",
       "      <td>7065</td>\n",
       "      <td>1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6803</th>\n",
       "      <td>7243</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>119145</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>8511</td>\n",
       "      <td>1917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6806</th>\n",
       "      <td>6988</td>\n",
       "      <td>1919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6807 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item  year\n",
       "0       1348  1922\n",
       "1      44587  1922\n",
       "2       4768  1922\n",
       "3       8235  1923\n",
       "4       8609  1923\n",
       "...      ...   ...\n",
       "6802    7065  1915\n",
       "6803    7243  1916\n",
       "6804  119145  2015\n",
       "6805    8511  1917\n",
       "6806    6988  1919\n",
       "\n",
       "[6807 rows x 2 columns]"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_data = pd.read_csv(os.path.join(DATA_DIR, 'years.tsv'), sep='\\t')\n",
    "year_data.loc[6799] = [6987, 1920]\n",
    "year_data.loc[6800] = [3310, 1921]\n",
    "year_data.loc[6801] = [32898, 1902]\n",
    "year_data.loc[6802] = [7065, 1915]\n",
    "year_data.loc[6803] = [7243, 1916]\n",
    "year_data.loc[6804] = [119145, 2015]\n",
    "year_data.loc[6805] = [8511, 1917]\n",
    "year_data.loc[6806] = [6988, 1919]\n",
    "year_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_numerize(tp, show2id):\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item  year\n",
      "0      990  1922\n",
      "1     2575  1922\n",
      "2     6127  1922\n",
      "3     3013  1923\n",
      "4     3801  1923\n",
      "...    ...   ...\n",
      "6802  4374  1915\n",
      "6803  3603  1916\n",
      "6804  4474  2015\n",
      "6805  3713  1917\n",
      "6806  6425  1919\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "year_data['item'] = year_numerize(year_data, show2id)\n",
    "print(year_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_year_dict = {}\n",
    "for item, year in year_data.values:\n",
    "    emb_year_dict[item] = 1 if year >= 1993 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_year_df = pd.DataFrame(list(emb_year_dict.items()), columns=['item','year'])\n",
    "print(emb_year_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_year_df2 = emb_year_df.sort_values(by = ['item'])\n",
    "print(emb_year_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_emb_year = []\n",
    "for year in emb_year_df2['year'].values:\n",
    "    total_emb_year.append(year)\n",
    "item_year_emb = pd.DataFrame(total_emb_year)\n",
    "print(item_year_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 1)\n"
     ]
    }
   ],
   "source": [
    "print(item_year_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_year_emb = item_year_emb.T\n",
    "print(item_year_emb.shape)\n",
    "print(item_year_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      -0.002037\n",
      "1      -0.005243\n",
      "2      -0.002291\n",
      "3      -0.004488\n",
      "4       0.005975\n",
      "          ...   \n",
      "6802   -0.003603\n",
      "6803    0.003379\n",
      "6804    0.003379\n",
      "6805   -0.005051\n",
      "6806    0.010760\n",
      "Length: 6807, dtype: float64\n",
      "0      -0.001810\n",
      "1       0.012518\n",
      "2      -0.004329\n",
      "3      -0.010981\n",
      "4      -0.006752\n",
      "          ...   \n",
      "6802   -0.006579\n",
      "6803   -0.006462\n",
      "6804   -0.000675\n",
      "6805   -0.007131\n",
      "6806   -0.010628\n",
      "Length: 6807, dtype: float64\n",
      "0       1.0\n",
      "1       1.0\n",
      "2       1.0\n",
      "3      -1.0\n",
      "4      -1.0\n",
      "       ... \n",
      "6802   -1.0\n",
      "6803    1.0\n",
      "6804    1.0\n",
      "6805    1.0\n",
      "6806    1.0\n",
      "Length: 6807, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(item_genre_emb))\n",
    "print(np.mean(item_title_emb))\n",
    "print(np.mean(item_year_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi VAE + Multi DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "loader = DataLoader(args.data)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiDAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module for Multi-DAE.\n",
    "\n",
    "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
    "    See Variational Autoencoders for Collaborative Filtering\n",
    "    https://arxiv.org/abs/1802.05814\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
    "        super(MultiDAE, self).__init__()\n",
    "        self.item_genre = torch.Tensor(item_genre_emb.values) ##### 추가\n",
    "        self.item_title = torch.Tensor(item_title_emb.values) ##### 추가\n",
    "        self.item_year = torch.Tensor(item_year_emb.values) ##### 추가\n",
    "\n",
    "        self.p_dims = p_dims\n",
    "        if q_dims:\n",
    "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        else:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print('input.shape: ', input.shape)\n",
    "        h = F.normalize(input)\n",
    "        h = self.drop(h)\n",
    "        # h = torch.cat((self.item_genre.to(device), h), 0) ###추가\n",
    "        h = torch.cat((self.item_genre.to(device), self.item_title.to(device), self.item_year.to(device), h), 0) ###추가\n",
    "        #print('합친 h.shape: ', h.shape)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.layers) - 1:\n",
    "                h = F.tanh(h) #reluX\n",
    "\n",
    "        # reconstructed_genre, reconstructed_h = h.split([self.item_genre.shape[0], input.shape[0]], 0) ##추가\n",
    "        reconstructed_genre, reconstructed_title, reconstructed_year, reconstructed_h = h.split([self.item_genre.shape[0], self.item_title.shape[0], self.item_year.shape[0], input.shape[0]], 0) ##추가\n",
    "        #print('item_genre_emb.shape: ', item_genre_emb.shape)\n",
    "        #print('reconstructed_h.shape: ',reconstructed_h.shape)\n",
    "        return reconstructed_genre, reconstructed_title, reconstructed_year, reconstructed_h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module for Multi-VAE.\n",
    "\n",
    "    Multi-VAE : Variational Autoencoder with Multinomial Likelihood\n",
    "    See Variational Autoencoders for Collaborative Filtering\n",
    "    https://arxiv.org/abs/1802.05814\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
    "        super(MultiVAE, self).__init__()\n",
    "        self.p_dims = p_dims\n",
    "        self.item_genre = torch.Tensor(item_genre_emb.values) ##### 추가\n",
    "        self.item_title = torch.Tensor(item_title_emb.values) ##### 추가\n",
    "        self.item_year = torch.Tensor(item_year_emb.values) ##### 추가\n",
    "        if q_dims:\n",
    "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        else:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "\n",
    "        # Last dimension of q- network is for mean and variance\n",
    "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
    "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
    "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "\n",
    "        mu, logvar = self.encode(input)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_gen, recon_tit, recon_yr, recon_batch = self.decode(z)\n",
    "        return recon_gen, recon_tit, recon_yr, recon_batch, mu, logvar\n",
    "\n",
    "    def encode(self, input):\n",
    "        h = F.normalize(input)\n",
    "        h = self.drop(h)\n",
    "\n",
    "        # h = torch.cat((self.item_genre.to(device), h), 0) ###추가\n",
    "        h = torch.cat((self.item_genre.to(device), self.item_title.to(device), self.item_year.to(device), h), 0) ###추가\n",
    "        for i, layer in enumerate(self.q_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.q_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "            else:\n",
    "                mu = h[:, :self.q_dims[-1]]\n",
    "                logvar = h[:, self.q_dims[-1]:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = z\n",
    "        for i, layer in enumerate(self.p_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.p_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "        #reconstructed_genre, reconstructed_h = h.split([self.item_genre.shape[0], self.input.shape[0]], 0) ##추가\n",
    "        reconstructed_genre, reconstructed_title, reconstructed_year, reconstructed_h = h.split([self.item_genre.shape[0], self.item_title.shape[0], self.item_year.shape[0], self.input.shape[0]], 0) ##추가\n",
    "        return reconstructed_genre, reconstructed_title, reconstructed_year, reconstructed_h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.q_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "        for layer in self.p_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function_vae(recon_x, x, mu, logvar, anneal=1.0):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "\n",
    "    return BCE + anneal * KLD\n",
    "\n",
    "def loss_function_dae(recon_x, x):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "    return BCE\n",
    "\n",
    "def loss_function_vae_genre(recon_genre, recon_title, recon_year, recon_x, x, mu, logvar, anneal=1.0):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "\n",
    "    mseloss = nn.MSELoss()\n",
    "    mseloss2 = nn.MSELoss()\n",
    "    mseloss3 = nn.MSELoss()\n",
    "\n",
    "    RMSE = torch.sqrt(mseloss(recon_genre, torch.Tensor(item_genre_emb.values).to(device))) * 10\n",
    "    RMSE2 = torch.sqrt(mseloss2(recon_title, torch.Tensor(item_title_emb.values).to(device))) * 10\n",
    "    RMSE3 = torch.sqrt(mseloss3(recon_year, torch.Tensor(item_year_emb.values).to(device)))\n",
    "\n",
    "    return BCE + anneal * KLD + RMSE + RMSE2 + RMSE3\n",
    "\n",
    "def loss_function_dae_genre(recon_genre, recon_title, recon_year, recon_x, x):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "\n",
    "    mseloss = nn.MSELoss()\n",
    "    mseloss2 = nn.MSELoss()\n",
    "    mseloss3 = nn.MSELoss()\n",
    "\n",
    "    RMSE = torch.sqrt(mseloss(recon_genre, torch.Tensor(item_genre_emb.values).to(device))) * 10\n",
    "    RMSE2 = torch.sqrt(mseloss2(recon_title, torch.Tensor(item_title_emb.values).to(device))) * 10\n",
    "    RMSE3 = torch.sqrt(mseloss3(recon_year, torch.Tensor(item_year_emb.values).to(device)))\n",
    "\n",
    "    return BCE + RMSE + RMSE2 + RMSE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "best_r10 = -np.inf\n",
    "update_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    Normalized Discounted Cumulative Gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def Recall_at_k_batch(genre_pred, title_pred, year_pred, X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "    # year은 emb이 아니긴 한데 .. 흠..\n",
    "    X_pred = X_pred + np.mean(genre_pred) * 20 + np.mean(title_pred) * 20 + np.mean(year_pred)*0.1\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, epoch, is_VAE = False):\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    global update_count\n",
    "\n",
    "    np.random.shuffle(idxlist)\n",
    "\n",
    "    for batch_idx, start_idx in enumerate(range(0, N, args.batch_size)):\n",
    "        end_idx = min(start_idx + args.batch_size, N)\n",
    "        data = train_data[idxlist[start_idx:end_idx]]\n",
    "        data = naive_sparse2tensor(data).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if is_VAE:\n",
    "          if args.total_anneal_steps > 0:\n",
    "            anneal = min(args.anneal_cap,\n",
    "                            1. * update_count / args.total_anneal_steps)\n",
    "          else:\n",
    "              anneal = args.anneal_cap\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          #recon_batch, mu, logvar = model(data)\n",
    "          recon_genre, recon_title, recon_year, recon_batch, mu, logvar = model(data)\n",
    "\n",
    "          #loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "          loss = criterion(recon_genre, recon_title, recon_year, recon_batch, data, mu, logvar, anneal)\n",
    "        else:\n",
    "          #recon_batch = model(data)\n",
    "          recon_genre, recon_title, recon_year, recon_batch = model(data)\n",
    "          loss = criterion(recon_genre, recon_title, recon_year, recon_batch, data)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                    'loss {:4.2f}'.format(\n",
    "                        epoch, batch_idx, len(range(0, N, args.batch_size)),\n",
    "                        elapsed * 1000 / args.log_interval,\n",
    "                        train_loss / args.log_interval))\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0\n",
    "    train_loss /= len(range(0, N, args.batch_size))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, data_tr, data_te, is_VAE=False):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n100_list = []\n",
    "    r10_list= []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, e_N, args.batch_size):\n",
    "            end_idx = min(start_idx + args.batch_size, N)\n",
    "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "            data_tensor = naive_sparse2tensor(data).to(device)\n",
    "            if is_VAE :\n",
    "\n",
    "              if args.total_anneal_steps > 0:\n",
    "                  anneal = min(args.anneal_cap,\n",
    "                                1. * update_count / args.total_anneal_steps)\n",
    "              else:\n",
    "                  anneal = args.anneal_cap\n",
    "\n",
    "              #recon_batch, mu, logvar = model(data_tensor)\n",
    "              #recon_genre, recon_batch, mu, logvar = model(data_tensor)\n",
    "              recon_genre, recon_title, recon_year, recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "              #loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "              #loss = criterion(recon_genre, recon_batch, data_tensor, mu, logvar, anneal)\n",
    "              loss = criterion(recon_genre, recon_title, recon_year, recon_batch, data_tensor, mu, logvar, anneal)\n",
    "\n",
    "            else :\n",
    "              #recon_batch = model(data_tensor)\n",
    "              #recon_genre, recon_batch = model(data_tensor)\n",
    "              recon_genre, recon_title, recon_year, recon_batch = model(data_tensor)\n",
    "\n",
    "              #loss = criterion(recon_batch, data_tensor)\n",
    "              #loss = criterion(recon_genre, recon_batch, data_tensor)\n",
    "              loss = criterion(recon_genre, recon_title, recon_year, recon_batch, data_tensor)\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            recon_batch = recon_batch.cpu().numpy()\n",
    "            recon_genre = recon_genre.cpu().numpy()\n",
    "            recon_title = recon_title.cpu().numpy()\n",
    "            recon_year = recon_year.cpu().numpy()\n",
    "\n",
    "            recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "            n100 = NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
    "            # r10 = Recall_at_k_batch(recon_genre, recon_batch, heldout_data, 10)\n",
    "            # r20 = Recall_at_k_batch(recon_genre, recon_batch, heldout_data, 20)\n",
    "            # r50 = Recall_at_k_batch(recon_genre, recon_batch, heldout_data, 50)\n",
    "\n",
    "            r10 = Recall_at_k_batch(recon_genre, recon_title, recon_year, recon_batch, heldout_data, 10)\n",
    "            r20 = Recall_at_k_batch(recon_genre, recon_title, recon_year, recon_batch, heldout_data, 20)\n",
    "            r50 = Recall_at_k_batch(recon_genre, recon_title, recon_year, recon_batch, heldout_data, 50)\n",
    "\n",
    "            n100_list.append(n100)\n",
    "            r20_list.append(r20)\n",
    "            r10_list.append(r10)\n",
    "            r50_list.append(r50)\n",
    "\n",
    "    total_loss /= len(range(0, e_N, args.batch_size))\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r10_list = np.concatenate(r10_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, np.mean(n100_list), np.mean(r10_list), np.mean(r20_list), np.mean(r50_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi-DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2j24clra) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266c977cfd144d59bce12cb25c066102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>n100</td><td>▁▁▁▁</td></tr><tr><td>r10</td><td>▁▁▁▁</td></tr><tr><td>r20</td><td>▁▁▁▁</td></tr><tr><td>r50</td><td>▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>n100</td><td>0.00204</td></tr><tr><td>r10</td><td>0.00115</td></tr><tr><td>r20</td><td>0.00125</td></tr><tr><td>r50</td><td>0.00171</td></tr><tr><td>train_loss</td><td>nan</td></tr><tr><td>val_loss</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">neat-thunder-36</strong>: <a href=\"https://wandb.ai/juk1329/recmovie/runs/2j24clra\" target=\"_blank\">https://wandb.ai/juk1329/recmovie/runs/2j24clra</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220403_145320-2j24clra/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2j24clra). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/ml/input/code/wandb/run-20220403_151648-36msjlh7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/juk1329/recmovie/runs/36msjlh7\" target=\"_blank\">mild-salad-37</a></strong> to <a href=\"https://wandb.ai/juk1329/recmovie\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"recmovie\", entity=\"juk1329\")\n",
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "loader = DataLoader(args.data)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "#p_dims = [200, 600, 1600, 3200, n_items]\n",
    "p_dims = [200, 3000, n_items]\n",
    "model_dae = MultiDAE(p_dims).to(device)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
    "optimizer = adabound.AdaBound(model_dae.parameters(), lr=1e-3, final_lr=0.1)\n",
    "#https://github.com/Luolc/AdaBound\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
    "#criterion = loss_function_dae\n",
    "criterion = loss_function_dae_genre\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "best_r10 = -np.inf\n",
    "update_count = 0\n",
    "\n",
    "############batch 1600\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "r10_fin_list = []\n",
    "new_epochs = args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/notebook/lib/python3.8/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 3.52s | valid loss 978.44 | n100 0.333 | r10 0.271 | r20 0.249 | r50 0.303\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 3.56s | valid loss 954.05 | n100 0.382 | r10 0.311 | r20 0.288 | r50 0.346\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 3.58s | valid loss 945.12 | n100 0.400 | r10 0.334 | r20 0.305 | r50 0.364\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 3.56s | valid loss 938.10 | n100 0.414 | r10 0.351 | r20 0.320 | r50 0.379\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 3.54s | valid loss 933.87 | n100 0.415 | r10 0.337 | r20 0.317 | r50 0.384\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 3.45s | valid loss 929.31 | n100 0.424 | r10 0.355 | r20 0.329 | r50 0.391\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 3.49s | valid loss 927.57 | n100 0.430 | r10 0.365 | r20 0.334 | r50 0.396\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 3.51s | valid loss 923.71 | n100 0.427 | r10 0.352 | r20 0.330 | r50 0.397\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 3.50s | valid loss 921.49 | n100 0.433 | r10 0.357 | r20 0.333 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 3.61s | valid loss 919.03 | n100 0.440 | r10 0.373 | r20 0.342 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 3.51s | valid loss 918.49 | n100 0.431 | r10 0.347 | r20 0.328 | r50 0.401\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 3.60s | valid loss 915.77 | n100 0.442 | r10 0.375 | r20 0.345 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 3.52s | valid loss 913.10 | n100 0.440 | r10 0.368 | r20 0.341 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 3.56s | valid loss 911.12 | n100 0.442 | r10 0.367 | r20 0.342 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 3.72s | valid loss 913.03 | n100 0.427 | r10 0.335 | r20 0.322 | r50 0.399\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 3.50s | valid loss 908.08 | n100 0.442 | r10 0.370 | r20 0.341 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 3.57s | valid loss 909.69 | n100 0.440 | r10 0.367 | r20 0.340 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 3.60s | valid loss 907.06 | n100 0.444 | r10 0.372 | r20 0.345 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 3.56s | valid loss 905.63 | n100 0.439 | r10 0.358 | r20 0.335 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 3.59s | valid loss 902.92 | n100 0.444 | r10 0.375 | r20 0.344 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 3.56s | valid loss 903.41 | n100 0.438 | r10 0.354 | r20 0.334 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 3.47s | valid loss 902.08 | n100 0.444 | r10 0.370 | r20 0.344 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 3.54s | valid loss 901.80 | n100 0.441 | r10 0.365 | r20 0.339 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 3.62s | valid loss 899.20 | n100 0.444 | r10 0.370 | r20 0.344 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 3.55s | valid loss 898.72 | n100 0.444 | r10 0.368 | r20 0.342 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 3.67s | valid loss 898.89 | n100 0.445 | r10 0.374 | r20 0.346 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 3.58s | valid loss 897.36 | n100 0.441 | r10 0.362 | r20 0.339 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 3.54s | valid loss 896.93 | n100 0.445 | r10 0.371 | r20 0.345 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 3.61s | valid loss 899.26 | n100 0.446 | r10 0.379 | r20 0.348 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 3.59s | valid loss 895.85 | n100 0.441 | r10 0.364 | r20 0.338 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 3.47s | valid loss 895.30 | n100 0.446 | r10 0.372 | r20 0.344 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 3.61s | valid loss 895.25 | n100 0.446 | r10 0.374 | r20 0.345 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 3.70s | valid loss 893.81 | n100 0.441 | r10 0.362 | r20 0.340 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 3.52s | valid loss 893.73 | n100 0.442 | r10 0.367 | r20 0.342 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 3.60s | valid loss 892.70 | n100 0.442 | r10 0.365 | r20 0.341 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 3.61s | valid loss 893.57 | n100 0.444 | r10 0.372 | r20 0.344 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 3.56s | valid loss 893.28 | n100 0.447 | r10 0.377 | r20 0.346 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 3.57s | valid loss 891.48 | n100 0.445 | r10 0.373 | r20 0.344 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 3.53s | valid loss 891.24 | n100 0.445 | r10 0.372 | r20 0.346 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 3.74s | valid loss 890.76 | n100 0.445 | r10 0.369 | r20 0.344 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 3.61s | valid loss 890.02 | n100 0.445 | r10 0.370 | r20 0.344 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 3.49s | valid loss 889.65 | n100 0.445 | r10 0.374 | r20 0.344 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 3.60s | valid loss 889.65 | n100 0.442 | r10 0.363 | r20 0.340 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 3.58s | valid loss 890.56 | n100 0.447 | r10 0.376 | r20 0.346 | r50 0.412\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 3.55s | valid loss 888.83 | n100 0.446 | r10 0.373 | r20 0.346 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 3.66s | valid loss 888.91 | n100 0.444 | r10 0.368 | r20 0.343 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 3.60s | valid loss 888.63 | n100 0.444 | r10 0.367 | r20 0.343 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 3.55s | valid loss 888.02 | n100 0.446 | r10 0.371 | r20 0.346 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 3.52s | valid loss 888.02 | n100 0.442 | r10 0.366 | r20 0.342 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 3.54s | valid loss 888.37 | n100 0.445 | r10 0.370 | r20 0.345 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, new_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(model_dae, criterion, optimizer, epoch, is_VAE=False)\n",
    "    val_loss, n100, r10, r20, r50 = evaluate(model_dae, criterion, vad_data_tr, vad_data_te, is_VAE=False)\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    r10_fin_list.append(r10)\n",
    "\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "            'n100 {:5.3f} | r10 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                epoch, time.time() - epoch_start_time, val_loss,\n",
    "                n100, r10, r20, r50))\n",
    "    print('-' * 89)\n",
    "\n",
    "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
    "\n",
    "\n",
    "    # Save the model if the r10 is the best we've seen so far.\n",
    "    if r10 > best_r10:\n",
    "        with open(args.dae_save, 'wb') as f:\n",
    "            torch.save({'state_dict':model_dae.state_dict()},f)\n",
    "        best_r10 = r10\n",
    "        print(\"Better performance! save best model...\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"n100\": n100,\n",
    "        \"r10\": r10,\n",
    "        \"r20\": r20,\n",
    "        \"r50\": r50\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 903.414 | n100 0.444 | r10 0.374 | r20 0.344 | r50 0.41\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.dae_save, 'rb') as f:\n",
    "    model_dae = MultiDAE(p_dims).to(device)\n",
    "    checkpoint = torch.load(f)\n",
    "    model_dae.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Run on test data.\n",
    "test_loss, n100, r10, r20, r50 = evaluate(model_dae, criterion, test_data_tr, test_data_te, is_VAE=False)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.3f} | n100 {:4.3f} | r10 {:4.3f} | r20 {:4.3f} | '\n",
    "        'r50 {:4.2f}'.format(test_loss, n100, r10, r20, r50))\n",
    "print('=' * 89)\n",
    "wandb.watch(model_dae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multi-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:36msjlh7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a327cd02c349cb939ea34d97dd942f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>n100</td><td>▁▄▅▆▇▇▇▇▇██████▇▇███████████████████████</td></tr><tr><td>r10</td><td>▁▄▅▆▆▇▆▇▆█▇▇▇▇█▇▆▇▇▇█▇████▇▇████▇█▇█▇▇█▇</td></tr><tr><td>r20</td><td>▁▄▅▆▇▇▇▇▇████▇█▇▇█▇██▇████▇███████▇█████</td></tr><tr><td>r50</td><td>▁▄▅▆▇▇▇▇▇███████████████████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>n100</td><td>0.44499</td></tr><tr><td>r10</td><td>0.37041</td></tr><tr><td>r20</td><td>0.34474</td></tr><tr><td>r50</td><td>0.41084</td></tr><tr><td>train_loss</td><td>1106.55274</td></tr><tr><td>val_loss</td><td>888.3716</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">mild-salad-37</strong>: <a href=\"https://wandb.ai/juk1329/recmovie/runs/36msjlh7\" target=\"_blank\">https://wandb.ai/juk1329/recmovie/runs/36msjlh7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220403_151648-36msjlh7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:36msjlh7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/ml/input/code/wandb/run-20220403_152007-3j17rh2o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/juk1329/recmovie/runs/3j17rh2o\" target=\"_blank\">wise-cloud-38</a></strong> to <a href=\"https://wandb.ai/juk1329/recmovie\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"recmovie\", entity=\"juk1329\")\n",
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiVAE(\n",
       "  (q_layers): ModuleList(\n",
       "    (0): Linear(in_features=6807, out_features=3000, bias=True)\n",
       "    (1): Linear(in_features=3000, out_features=400, bias=True)\n",
       "  )\n",
       "  (p_layers): ModuleList(\n",
       "    (0): Linear(in_features=200, out_features=3000, bias=True)\n",
       "    (1): Linear(in_features=3000, out_features=6807, bias=True)\n",
       "  )\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "loader = DataLoader(args.data)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "p_dims = [200, 3000, n_items]\n",
    "model_vae = MultiVAE(p_dims).to(device)\n",
    "\n",
    "optimizer2 = adabound.AdaBound(model_vae.parameters(), lr=1e-3, final_lr=0.1)\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
    "#criterion2 = loss_function_vae\n",
    "criterion2 = loss_function_vae_genre\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "r10_fin_list = []\n",
    "\n",
    "best_r10 = -np.inf\n",
    "update_count = 0\n",
    "new_epochs = args.epochs\n",
    "model_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 3.60s | valid loss 1004.49 | n100 0.319 | r10 0.252 | r20 0.235 | r50 0.290\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 3.56s | valid loss 968.64 | n100 0.374 | r10 0.307 | r20 0.281 | r50 0.339\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 3.60s | valid loss 955.17 | n100 0.395 | r10 0.328 | r20 0.299 | r50 0.358\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 3.73s | valid loss 947.59 | n100 0.404 | r10 0.332 | r20 0.308 | r50 0.371\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 3.56s | valid loss 942.08 | n100 0.415 | r10 0.346 | r20 0.317 | r50 0.380\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 3.54s | valid loss 938.92 | n100 0.417 | r10 0.343 | r20 0.321 | r50 0.385\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 3.54s | valid loss 934.89 | n100 0.425 | r10 0.356 | r20 0.327 | r50 0.392\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 3.65s | valid loss 933.18 | n100 0.424 | r10 0.353 | r20 0.324 | r50 0.392\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 3.49s | valid loss 930.84 | n100 0.432 | r10 0.358 | r20 0.330 | r50 0.397\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 3.61s | valid loss 927.52 | n100 0.434 | r10 0.361 | r20 0.334 | r50 0.398\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 3.57s | valid loss 924.90 | n100 0.431 | r10 0.360 | r20 0.334 | r50 0.400\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 3.60s | valid loss 922.94 | n100 0.434 | r10 0.360 | r20 0.336 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 3.49s | valid loss 920.88 | n100 0.432 | r10 0.352 | r20 0.333 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 3.70s | valid loss 919.40 | n100 0.437 | r10 0.363 | r20 0.337 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 3.60s | valid loss 918.27 | n100 0.433 | r10 0.358 | r20 0.335 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 3.56s | valid loss 916.67 | n100 0.433 | r10 0.354 | r20 0.330 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 3.64s | valid loss 915.72 | n100 0.428 | r10 0.345 | r20 0.327 | r50 0.400\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 3.63s | valid loss 913.85 | n100 0.432 | r10 0.350 | r20 0.330 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 3.60s | valid loss 913.38 | n100 0.434 | r10 0.355 | r20 0.332 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 3.53s | valid loss 911.72 | n100 0.432 | r10 0.349 | r20 0.330 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 3.55s | valid loss 910.77 | n100 0.428 | r10 0.342 | r20 0.326 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 3.75s | valid loss 909.39 | n100 0.429 | r10 0.343 | r20 0.326 | r50 0.401\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 3.57s | valid loss 907.92 | n100 0.429 | r10 0.343 | r20 0.328 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 3.65s | valid loss 907.13 | n100 0.429 | r10 0.343 | r20 0.326 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 3.62s | valid loss 907.18 | n100 0.422 | r10 0.329 | r20 0.317 | r50 0.397\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 3.57s | valid loss 905.82 | n100 0.429 | r10 0.346 | r20 0.326 | r50 0.398\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 3.61s | valid loss 904.46 | n100 0.428 | r10 0.342 | r20 0.324 | r50 0.399\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 3.68s | valid loss 904.48 | n100 0.428 | r10 0.341 | r20 0.322 | r50 0.396\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 3.62s | valid loss 904.05 | n100 0.425 | r10 0.333 | r20 0.320 | r50 0.399\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 3.72s | valid loss 903.13 | n100 0.424 | r10 0.335 | r20 0.323 | r50 0.397\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 3.56s | valid loss 904.27 | n100 0.420 | r10 0.330 | r20 0.316 | r50 0.393\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 3.66s | valid loss 903.29 | n100 0.427 | r10 0.342 | r20 0.323 | r50 0.398\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 3.59s | valid loss 902.37 | n100 0.424 | r10 0.336 | r20 0.319 | r50 0.396\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 3.69s | valid loss 902.36 | n100 0.428 | r10 0.340 | r20 0.320 | r50 0.397\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 3.59s | valid loss 901.60 | n100 0.421 | r10 0.329 | r20 0.316 | r50 0.395\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 3.61s | valid loss 901.78 | n100 0.417 | r10 0.324 | r20 0.313 | r50 0.392\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 3.62s | valid loss 900.29 | n100 0.420 | r10 0.331 | r20 0.316 | r50 0.394\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 3.59s | valid loss 899.99 | n100 0.423 | r10 0.336 | r20 0.318 | r50 0.395\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 3.68s | valid loss 899.92 | n100 0.419 | r10 0.326 | r20 0.312 | r50 0.394\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 3.62s | valid loss 899.92 | n100 0.420 | r10 0.332 | r20 0.316 | r50 0.392\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 3.63s | valid loss 905.76 | n100 0.418 | r10 0.329 | r20 0.313 | r50 0.391\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 3.59s | valid loss 900.27 | n100 0.421 | r10 0.330 | r20 0.313 | r50 0.392\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 3.65s | valid loss 900.05 | n100 0.415 | r10 0.325 | r20 0.313 | r50 0.388\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 3.63s | valid loss 899.65 | n100 0.418 | r10 0.323 | r20 0.313 | r50 0.391\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 3.79s | valid loss 900.02 | n100 0.412 | r10 0.314 | r20 0.305 | r50 0.386\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 3.74s | valid loss 899.17 | n100 0.419 | r10 0.329 | r20 0.315 | r50 0.392\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 3.56s | valid loss 899.18 | n100 0.417 | r10 0.322 | r20 0.308 | r50 0.389\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 3.54s | valid loss 899.01 | n100 0.416 | r10 0.321 | r20 0.307 | r50 0.390\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 3.25s | valid loss  nan | n100 0.002 | r10 0.001 | r20 0.001 | r50 0.002\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 3.31s | valid loss  nan | n100 0.002 | r10 0.001 | r20 0.001 | r50 0.002\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, new_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(model_vae, criterion2, optimizer2, epoch, is_VAE=True)\n",
    "    #print(train_loss)\n",
    "    val_loss, n100, r10, r20, r50 = evaluate(model_vae, criterion2, vad_data_tr, vad_data_te, is_VAE=True)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "            'n100 {:5.3f} | r10 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                epoch, time.time() - epoch_start_time, val_loss,\n",
    "                n100, r10, r20, r50))\n",
    "    print('-' * 89)\n",
    "\n",
    "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    r10_fin_list.append(r10)\n",
    "\n",
    "    # Save the model if the r10 is the best we've seen so far.\n",
    "    if r10 > best_r10:\n",
    "        with open(args.vae_save, 'wb') as f:\n",
    "            torch.save({'state_dict': model_vae.state_dict()}, f)\n",
    "        best_r10 = r10\n",
    "        print(\"Better performance! save best model...\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"n100\": n100,\n",
    "        \"r10\": r10,\n",
    "        \"r20\": r20,\n",
    "        \"r50\": r50\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 927.02 | n100 0.43 | r10 0.36 | r20 0.33 | r50 0.40\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.vae_save, 'rb') as f:\n",
    "    model_vae = MultiVAE(p_dims).to(device)\n",
    "    checkpoint = torch.load(f)\n",
    "    model_vae.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Run on test data.\n",
    "test_loss, n100, r10, r20, r50 = evaluate(model_vae, criterion2, test_data_tr, test_data_te, is_VAE=True)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r10 {:4.2f} | r20 {:4.2f} | '\n",
    "        'r50 {:4.2f}'.format(test_loss, n100, r10, r20, r50))\n",
    "print('=' * 89)\n",
    "wandb.watch(model_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## 배치사이즈 포함\n",
    "def numerize_for_infer(tp, profile2id, show2id):\n",
    "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### 데이터 준비\n",
    "infer_df = numerize_for_infer(raw_data, profile2id, show2id)\n",
    "\n",
    "loader = DataLoader(args.data)\n",
    "n_items = loader.load_n_items()\n",
    "\n",
    "n_users = infer_df['uid'].max() + 1\n",
    "\n",
    "rows, cols = infer_df['uid'], infer_df['sid']\n",
    "data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, n_items))\n",
    "\n",
    "N = data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "model_dae.eval()\n",
    "model_vae.eval()\n",
    "total_loss = 0.0\n",
    "e_idxlist = list(range(data.shape[0]))\n",
    "e_N = data.shape[0]\n",
    "pred_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04073035, -0.10485938, -0.04581048, ...,  0.06757813,\n",
       "       -0.10102921,  0.21519038])"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(item_genre_emb).values*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/notebook/lib/python3.8/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31360, 10)\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "total_loss2 = 0\n",
    "with torch.no_grad():\n",
    "    for start_idx in range(0, e_N, args.batch_size):\n",
    "        end_idx = min(start_idx + args.batch_size, N)\n",
    "        data_batch = data[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "        data_tensor = naive_sparse2tensor(data_batch).to(device)\n",
    "        data_tensor2 = naive_sparse2tensor(data_batch).to(device)\n",
    "\n",
    "        if args.total_anneal_steps > 0:\n",
    "            anneal = min(args.anneal_cap, 1. * update_count / args.total_anneal_steps)\n",
    "        else:\n",
    "            anneal = args.anneal_cap\n",
    "\n",
    "        # recon_batch = model_dae(data_tensor)\n",
    "        # loss = criterion(recon_batch, data_tensor)\n",
    "\n",
    "        #recon_genre, recon_batch = model_dae(data_tensor)\n",
    "        #loss = criterion(recon_genre, recon_batch, data_tensor)\n",
    "\n",
    "        recon_genre, recon_title, recon_year, recon_batch = model_dae(data_tensor)\n",
    "        loss = criterion(recon_genre, recon_title, recon_year, recon_batch, data_tensor)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # recon_batch2, mu, logvar = model_vae(data_tensor2)\n",
    "        # loss2 = criterion2(recon_batch2, data_tensor2, mu, logvar, anneal)\n",
    "\n",
    "        # recon_genre2, recon_batch2, mu, logvar = model_vae(data_tensor2)\n",
    "        # loss2 = criterion2(recon_genre2, recon_batch2, data_tensor2, mu, logvar, anneal)\n",
    "\n",
    "        recon_genre2, recon_title2, recon_year2, recon_batch2, mu, logvar = model_vae(data_tensor2)\n",
    "        loss2 = criterion2(recon_genre2, recon_title2, recon_year2, recon_batch2, data_tensor2, mu, logvar, anneal)\n",
    "\n",
    "        total_loss2 += loss2.item()\n",
    "\n",
    "        # Exclude examples from training set\n",
    "        recon_batch = recon_batch.cpu().numpy()\n",
    "        recon_batch2 = recon_batch2.cpu().numpy()\n",
    "\n",
    "        recon_genre = recon_genre.cpu().numpy()\n",
    "        recon_genre2 = recon_genre2.cpu().numpy()\n",
    "\n",
    "        recon_title = recon_title.cpu().numpy()\n",
    "        recon_title2 = recon_title2.cpu().numpy()\n",
    "\n",
    "        recon_year = recon_year.cpu().numpy()\n",
    "        recon_year2 = recon_year2.cpu().numpy()\n",
    "\n",
    "\n",
    "        recon_batch = np.add(recon_batch, recon_batch2) # 1:1로 앙상블\n",
    "        #아래 추가적으로 더해주는 부분은 지그시의 코드로 원상복구\n",
    "        recon_batch = recon_batch + np.mean(recon_genre)*10 + np.mean(recon_genre2) * 10 + np.mean(recon_title) * 10 + np.mean(recon_title2) * 10#+ np.mean(recon_year) + np.mean(recon_year2)을 넣어서 실험했었음.\n",
    "        recon_batch[data_batch.nonzero()] = -np.inf\n",
    "\n",
    "        ##Recall\n",
    "        batch_users = recon_batch.shape[0]\n",
    "        idx = bn.argpartition(-recon_batch, 10, axis=1)[:, :10]\n",
    "        if start_idx == 0:\n",
    "            pred_list = idx\n",
    "        else:\n",
    "            pred_list = np.append(pred_list, idx, axis=0)\n",
    "\n",
    "print(pred_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## sample_submission에 맞게끔 바꾸기\n",
    "user2 = []\n",
    "item2 = []\n",
    "for i_idx, arr_10 in enumerate(pred_list):\n",
    "    user2.extend([i_idx]*10)\n",
    "    item2.extend(arr_10)\n",
    "\n",
    "u2 = pd.DataFrame(user2, columns=['user'])\n",
    "i2 = pd.DataFrame(item2, columns=['item'])\n",
    "all2 = pd.concat([u2, i2], axis=1)\n",
    "\n",
    "re_p2id = dict((v, k) for k, v in profile2id.items())\n",
    "re_s2id = dict((v, k) for k, v in show2id.items())\n",
    "\n",
    "def de_numerize(tp, re_p2id, re_s2id):\n",
    "    uid2 = tp['user'].apply(lambda x: re_p2id[x])\n",
    "    sid2 = tp['item'].apply(lambda x: re_s2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid2, 'sid': sid2}, columns=['uid', 'sid'])\n",
    "\n",
    "ans2 = de_numerize(all2, re_p2id, re_s2id)\n",
    "ans2.columns = ['user', 'item']\n",
    "new_ans2 = ans2.sort_values('user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313600"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 확인용\n",
    "submit_data = pd.read_csv('/opt/ml/input/data/eval/sample_submission.csv', sep=',')\n",
    "sum(new_ans2.user.values == submit_data.user.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_ans2.reset_index(drop=True, inplace=True)\n",
    "new_ans2.to_csv('0403_double_submit_multi_item_embedding.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4226      5608\n",
      "2571      5401\n",
      "356       5030\n",
      "318       4858\n",
      "296       4539\n",
      "          ... \n",
      "2735         1\n",
      "74750        1\n",
      "102123       1\n",
      "82463        1\n",
      "8188         1\n",
      "Name: item, Length: 4150, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(new_ans2['item'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa610923100>]"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(new_ans2['item'].value_counts().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
