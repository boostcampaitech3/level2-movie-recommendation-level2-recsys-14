{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# model setting\n",
    "max_len = 50\n",
    "hidden_units = 50\n",
    "num_heads = 1\n",
    "num_layers = 2\n",
    "dropout_rate=0.5\n",
    "num_workers = 1\n",
    "device = 'cuda'\n",
    "\n",
    "# training setting\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "mask_prob = 0.15 # for cloze task\n",
    "\n",
    "model_save_dir = '/opt/ml/input/experiment/'\n",
    "model_save_file = 'bert4rec_model.pt'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num users: 31360, num items: 6807\n"
     ]
    }
   ],
   "source": [
    "############# 중요 #############\n",
    "# data_path는 사용자의 디렉토리에 맞게 설정해야 합니다.\n",
    "data_path = './input/data/train/train_ratings.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "item_ids = df['item'].unique()\n",
    "user_ids = df['user'].unique()\n",
    "num_item, num_user = len(item_ids), len(user_ids)\n",
    "num_batch = num_user // batch_size\n",
    "\n",
    "# user, item indexing\n",
    "item2idx = pd.Series(data=np.arange(len(item_ids))+1, index=item_ids) # item re-indexing (1~num_item), num_item+1: mask idx\n",
    "user2idx = pd.Series(data=np.arange(len(user_ids)), index=user_ids) # user re-indexing (0~num_user-1)\n",
    "\n",
    "# dataframe indexing\n",
    "df = pd.merge(df, pd.DataFrame({'item': item_ids, 'item_idx': item2idx[item_ids].values}), on='item', how='inner')\n",
    "df = pd.merge(df, pd.DataFrame({'user': user_ids, 'user_idx': user2idx[user_ids].values}), on='user', how='inner')\n",
    "df.sort_values(['user_idx', 'time'], inplace=True)\n",
    "del df['item'], df['user']\n",
    "\n",
    "# train set, valid set 생성\n",
    "users = defaultdict(list) # defaultdict은 dictionary의 key가 없을때 default 값을 value로 반환\n",
    "user_train = {}\n",
    "user_valid = {}\n",
    "for u, i, t in zip(df['user_idx'], df['item_idx'], df['time']):\n",
    "    users[u].append(i)\n",
    "\n",
    "for user in users:\n",
    "    user_train[user] = users[user][:-1]\n",
    "    user_valid[user] = [users[user][-1]]\n",
    "\n",
    "print(f'num users: {num_user}, num items: {num_item}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, user_train, num_user, num_item, max_len, mask_prob):\n",
    "        self.user_train = user_train\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        # 총 user의 수 = 학습에 사용할 sequence의 수\n",
    "        return self.num_user\n",
    "\n",
    "    def __getitem__(self, user):\n",
    "        # iterator를 구동할 때 사용됩니다.\n",
    "        seq = self.user_train[user]\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for s in seq:\n",
    "            prob = np.random.random() # TODO1: numpy를 사용해서 0~1 사이의 임의의 값을 샘플링하세요.\n",
    "            if prob < self.mask_prob:\n",
    "                prob /= self.mask_prob\n",
    "\n",
    "                # BERT 학습\n",
    "                if prob < 0.8:\n",
    "                    # masking\n",
    "                    # 해당 아이템이 마스킹이 되었음을 표시합니다.\n",
    "                    tokens.append(self.num_item + 1)  # mask_index: num_item + 1, 0: pad, 1~num_item: item index\n",
    "                elif prob < 0.9:\n",
    "                    # 10%의 단어들은 단어가 랜덤으로 변경됩니다. 1 ~ num_item에 masking까지\n",
    "                    tokens.append(np.random.randint(1, self.num_item+1))  # item random sampling\n",
    "                else:\n",
    "                    # 10%의 단어들은 그대로 둡니다.\n",
    "                    tokens.append(s)\n",
    "                labels.append(s)  # 학습에 사용\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "                labels.append(0)  # 학습에 사용 X, trivial\n",
    "        tokens = tokens[-self.max_len:]\n",
    "        labels = labels[-self.max_len:]\n",
    "        mask_len = self.max_len - len(tokens)\n",
    "\n",
    "        # zero padding\n",
    "        tokens = [0] * mask_len + tokens\n",
    "        labels = [0] * mask_len + labels\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate) # scaled dot product attention module을 사용하여 attention 계산\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "\n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seqlen, -1)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
    "        return output, attn_dist\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        # SASRec과의 dimension 차이가 있습니다.\n",
    "        self.W_1 = nn.Linear(hidden_units, 4 * hidden_units)\n",
    "        self.W_2 = nn.Linear(4 * hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.gelu(self.dropout(self.W_1(x)))) # activation: relu -> gelu\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "class BERT4RecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(BERT4RecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.item_emb = nn.Embedding(num_item + 2, hidden_units, padding_idx=0) # TODO2: mask와 padding을 고려하여 embedding을 생성해보세요.\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_units) # learnable positional encoding\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
    "\n",
    "        self.blocks = nn.ModuleList([BERT4RecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "        self.out = nn.Linear(hidden_units, num_item + 1) # TODO3: 예측을 위한 output layer를 구현해보세요. (num_item 주의)\n",
    "\n",
    "    def forward(self, log_seqs):\n",
    "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device))\n",
    "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
    "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))\n",
    "        seqs = self.emb_layernorm(self.dropout(seqs))\n",
    "\n",
    "        mask = torch.BoolTensor(log_seqs > 0).unsqueeze(1).repeat(1, log_seqs.shape[1], 1).unsqueeze(1).to(self.device) # mask for zero pad\n",
    "        for block in self.blocks:\n",
    "            seqs, attn_dist = block(seqs, mask)\n",
    "        out = self.out(seqs)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_bert4rec(data_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    tbar = tqdm(data_loader)\n",
    "    for step, (log_seqs, labels) in enumerate(tbar):\n",
    "        logits = model(log_seqs)\n",
    "\n",
    "        # size matching\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tbar.set_description(f'Epoch: {epoch:3d}| Step: {step:3d}| Train loss: {loss:.5f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def random_neg(l, r, s):\n",
    "    # log에 존재하는 아이템과 겹치지 않도록 sampling\n",
    "    sample = np.random.randint(l, r)\n",
    "    while sample in s:\n",
    "        sample = np.random.randint(l, r)\n",
    "    return sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def evaluate_bert4rec(model, user_train, user_valid, num_user, num_item, max_len):\n",
    "    model.eval()\n",
    "\n",
    "    NDCG = 0.0 # NDCG@10\n",
    "    HIT = 0.0 # HIT@10\n",
    "\n",
    "    num_item_sample = 100\n",
    "    num_user_sample = 1000\n",
    "    users = np.random.randint(0, num_user, num_user_sample) # 1000개만 sampling 하여 evaluation\n",
    "\n",
    "    for u in users:\n",
    "        seq = (user_train[u] + [num_item + 1])[-max_len:] # TODO5: 다음 아이템을 예측하기 위한 input token을 추가해주세요.\n",
    "        rated = set(user_train[u] + user_valid[u])\n",
    "        item_idx = [user_valid[u][0]] + [random_neg(1, num_item + 1, rated) for _ in range(num_item_sample)]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = - model(np.array([seq]))\n",
    "            predictions = predictions[0][-1][item_idx] # sampling\n",
    "            rank = predictions.argsort().argsort()[0].item()\n",
    "\n",
    "        if rank < 10: # 10\n",
    "            NDCG += 1 / np.log2(rank + 2)\n",
    "            HIT += 1\n",
    "\n",
    "    ndcg10 = NDCG/num_user_sample\n",
    "    hit10 = HIT/num_user_sample\n",
    "\n",
    "    print(f'NDCG@10: {ndcg10}| HIT@10: {hit10}')\n",
    "    return ndcg10, hit10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def evaluate_bert4rec_all(model, user_train, user_valid, num_user, num_item, max_len):\n",
    "    model.eval()\n",
    "\n",
    "    NDCG = 0.0 # NDCG@10\n",
    "    HIT = 0.0 # HIT@10\n",
    "\n",
    "    num_item_sample = 100\n",
    "    num_user_sample = 1000\n",
    "    # users = np.random.randint(0, num_user, num_user_sample) # 1000개만 sampling 하여 evaluation\n",
    "    users = np.arange(0, num_user)\n",
    "\n",
    "    for u in users:\n",
    "        seq = (user_train[u] + [num_item + 1])[-max_len:] # TODO5: 다음 아이템을 예측하기 위한 input token을 추가해주세요.\n",
    "        rated = set(user_train[u] + user_valid[u])\n",
    "        #item_idx = [user_valid[u][0]] + [random_neg(1, num_item + 1, rated) for _ in range(num_item_sample)]\n",
    "        item_idx = [user_valid[u][0]] + list(set(np.arange(1, num_item + 1)) - rated)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = - model(np.array([seq]))\n",
    "            predictions = predictions[0][-1][item_idx] # sampling\n",
    "\n",
    "            predictions = predictions.cpu()\n",
    "            pred_sort = predictions.argsort()\n",
    "            rev_pred_sort = np.empty(pred_sort.shape, dtype=np.intp)\n",
    "            rev_pred_sort[pred_sort] = np.arange(len(predictions))\n",
    "            rank = rev_pred_sort[0].item()\n",
    "            # rank = predictions.argsort().argsort()[0].item()\n",
    "\n",
    "        if rank < 10: # 10\n",
    "            NDCG += 1 / np.log2(rank + 2)\n",
    "            HIT += 1\n",
    "\n",
    "    # ndcg10 = NDCG/num_user_sample\n",
    "    # hit10 = HIT/num_user_sample\n",
    "\n",
    "    ndcg10 = NDCG / num_user\n",
    "    hit10 = HIT / num_user\n",
    "\n",
    "    print(f'NDCG@10: {ndcg10}| HIT@10: {hit10}')\n",
    "    return ndcg10, hit10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "model = BERT4Rec(num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n",
    "seq_dataset = SeqDataset(user_train, num_user, num_item, max_len, mask_prob)\n",
    "data_loader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, pin_memory=True) # TODO4: pytorch의 DataLoader와 seq_dataset을 사용하여 학습 파이프라인을 구현해보세요.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Step: 244| Train loss: 7.79916: 100%|██████████| 245/245 [00:08<00:00, 27.57it/s]\n",
      "Epoch:   2| Step: 244| Train loss: 7.69407: 100%|██████████| 245/245 [00:08<00:00, 29.20it/s]\n",
      "Epoch:   3| Step: 244| Train loss: 7.66612: 100%|██████████| 245/245 [00:08<00:00, 29.58it/s]\n",
      "Epoch:   4| Step: 244| Train loss: 7.47522: 100%|██████████| 245/245 [00:08<00:00, 29.72it/s]\n",
      "Epoch:   5| Step: 244| Train loss: 7.64924: 100%|██████████| 245/245 [00:08<00:00, 30.53it/s]\n",
      "Epoch:   6| Step: 244| Train loss: 7.52551: 100%|██████████| 245/245 [00:08<00:00, 30.36it/s]\n",
      "Epoch:   7| Step: 244| Train loss: 7.46688: 100%|██████████| 245/245 [00:08<00:00, 30.41it/s]\n",
      "Epoch:   8| Step: 244| Train loss: 7.44460: 100%|██████████| 245/245 [00:08<00:00, 30.14it/s]\n",
      "Epoch:   9| Step: 244| Train loss: 7.42204: 100%|██████████| 245/245 [00:08<00:00, 29.57it/s]\n",
      "Epoch:  10| Step: 244| Train loss: 7.45493: 100%|██████████| 245/245 [00:08<00:00, 28.34it/s]\n",
      "Epoch:  11| Step: 244| Train loss: 7.31858: 100%|██████████| 245/245 [00:08<00:00, 28.77it/s]\n",
      "Epoch:  12| Step: 244| Train loss: 7.40152: 100%|██████████| 245/245 [00:08<00:00, 27.33it/s]\n",
      "Epoch:  13| Step: 244| Train loss: 7.42655: 100%|██████████| 245/245 [00:08<00:00, 29.04it/s]\n",
      "Epoch:  14| Step: 244| Train loss: 7.22451: 100%|██████████| 245/245 [00:08<00:00, 29.42it/s]\n",
      "Epoch:  15| Step: 244| Train loss: 7.26712: 100%|██████████| 245/245 [00:08<00:00, 29.53it/s]\n",
      "Epoch:  16| Step: 244| Train loss: 7.19930: 100%|██████████| 245/245 [00:08<00:00, 29.18it/s]\n",
      "Epoch:  17| Step: 244| Train loss: 7.02947: 100%|██████████| 245/245 [00:08<00:00, 30.29it/s]\n",
      "Epoch:  18| Step: 244| Train loss: 7.08736: 100%|██████████| 245/245 [00:07<00:00, 31.11it/s]\n",
      "Epoch:  19| Step: 244| Train loss: 7.14605: 100%|██████████| 245/245 [00:07<00:00, 30.73it/s]\n",
      "Epoch:  20| Step: 244| Train loss: 7.00892: 100%|██████████| 245/245 [00:08<00:00, 29.51it/s]\n",
      "Epoch:  21| Step: 244| Train loss: 7.06655: 100%|██████████| 245/245 [00:08<00:00, 29.01it/s]\n",
      "Epoch:  22| Step: 244| Train loss: 6.87269: 100%|██████████| 245/245 [00:08<00:00, 28.74it/s]\n",
      "Epoch:  23| Step: 244| Train loss: 6.82110: 100%|██████████| 245/245 [00:08<00:00, 29.36it/s]\n",
      "Epoch:  24| Step: 244| Train loss: 7.00844: 100%|██████████| 245/245 [00:08<00:00, 28.16it/s]\n",
      "Epoch:  25| Step: 244| Train loss: 7.04971: 100%|██████████| 245/245 [00:08<00:00, 27.24it/s]\n",
      "Epoch:  26| Step: 244| Train loss: 6.98107: 100%|██████████| 245/245 [00:08<00:00, 28.80it/s]\n",
      "Epoch:  27| Step: 244| Train loss: 6.90134: 100%|██████████| 245/245 [00:08<00:00, 29.36it/s]\n",
      "Epoch:  28| Step: 244| Train loss: 7.00697: 100%|██████████| 245/245 [00:08<00:00, 28.06it/s]\n",
      "Epoch:  29| Step: 244| Train loss: 6.87540: 100%|██████████| 245/245 [00:08<00:00, 27.35it/s]\n",
      "Epoch:  30| Step: 244| Train loss: 6.87681: 100%|██████████| 245/245 [00:08<00:00, 28.71it/s]\n",
      "Epoch:  31| Step: 244| Train loss: 6.87848: 100%|██████████| 245/245 [00:08<00:00, 27.41it/s]\n",
      "Epoch:  32| Step: 244| Train loss: 6.80548: 100%|██████████| 245/245 [00:09<00:00, 27.20it/s]\n",
      "Epoch:  33| Step: 244| Train loss: 6.62534: 100%|██████████| 245/245 [00:09<00:00, 26.67it/s]\n",
      "Epoch:  34| Step: 244| Train loss: 6.91675: 100%|██████████| 245/245 [00:08<00:00, 28.87it/s]\n",
      "Epoch:  35| Step: 244| Train loss: 6.74250: 100%|██████████| 245/245 [00:08<00:00, 29.36it/s]\n",
      "Epoch:  36| Step: 244| Train loss: 6.63139: 100%|██████████| 245/245 [00:08<00:00, 27.57it/s]\n",
      "Epoch:  37| Step: 244| Train loss: 6.87484: 100%|██████████| 245/245 [00:09<00:00, 26.92it/s]\n",
      "Epoch:  38| Step: 244| Train loss: 6.82805: 100%|██████████| 245/245 [00:08<00:00, 27.65it/s]\n",
      "Epoch:  39| Step: 244| Train loss: 6.78907: 100%|██████████| 245/245 [00:09<00:00, 27.20it/s]\n",
      "Epoch:  40| Step: 244| Train loss: 6.73599: 100%|██████████| 245/245 [00:08<00:00, 27.40it/s]\n",
      "Epoch:  41| Step: 244| Train loss: 6.72493: 100%|██████████| 245/245 [00:08<00:00, 27.34it/s]\n",
      "Epoch:  42| Step: 244| Train loss: 6.72695: 100%|██████████| 245/245 [00:08<00:00, 27.92it/s]\n",
      "Epoch:  43| Step: 244| Train loss: 6.77622: 100%|██████████| 245/245 [00:08<00:00, 29.11it/s]\n",
      "Epoch:  44| Step: 244| Train loss: 6.69652: 100%|██████████| 245/245 [00:08<00:00, 28.80it/s]\n",
      "Epoch:  45| Step: 244| Train loss: 6.81333: 100%|██████████| 245/245 [00:08<00:00, 28.93it/s]\n",
      "Epoch:  46| Step: 244| Train loss: 6.74144: 100%|██████████| 245/245 [00:08<00:00, 29.25it/s]\n",
      "Epoch:  47| Step: 244| Train loss: 6.81539: 100%|██████████| 245/245 [00:08<00:00, 29.13it/s]\n",
      "Epoch:  48| Step: 244| Train loss: 6.66322: 100%|██████████| 245/245 [00:09<00:00, 26.99it/s]\n",
      "Epoch:  49| Step: 244| Train loss: 6.72723: 100%|██████████| 245/245 [00:08<00:00, 29.16it/s]\n",
      "Epoch:  50| Step: 244| Train loss: 6.88700: 100%|██████████| 245/245 [00:08<00:00, 27.96it/s]\n",
      "Epoch:  51| Step: 244| Train loss: 6.87560: 100%|██████████| 245/245 [00:08<00:00, 27.84it/s]\n",
      "Epoch:  52| Step: 244| Train loss: 6.73126: 100%|██████████| 245/245 [00:08<00:00, 27.99it/s]\n",
      "Epoch:  53| Step: 244| Train loss: 6.72835: 100%|██████████| 245/245 [00:08<00:00, 27.54it/s]\n",
      "Epoch:  54| Step: 244| Train loss: 6.77020: 100%|██████████| 245/245 [00:08<00:00, 27.92it/s]\n",
      "Epoch:  55| Step: 244| Train loss: 6.75631: 100%|██████████| 245/245 [00:08<00:00, 28.17it/s]\n",
      "Epoch:  56| Step: 244| Train loss: 6.67197: 100%|██████████| 245/245 [00:08<00:00, 27.27it/s]\n",
      "Epoch:  57| Step: 244| Train loss: 6.77528: 100%|██████████| 245/245 [00:08<00:00, 27.31it/s]\n",
      "Epoch:  58| Step: 244| Train loss: 6.50234: 100%|██████████| 245/245 [00:08<00:00, 27.52it/s]\n",
      "Epoch:  59| Step: 244| Train loss: 6.74730: 100%|██████████| 245/245 [00:09<00:00, 27.16it/s]\n",
      "Epoch:  60| Step: 244| Train loss: 6.52382: 100%|██████████| 245/245 [00:08<00:00, 29.54it/s]\n",
      "Epoch:  61| Step: 244| Train loss: 6.81160: 100%|██████████| 245/245 [00:08<00:00, 28.82it/s]\n",
      "Epoch:  62| Step: 244| Train loss: 6.65790: 100%|██████████| 245/245 [00:09<00:00, 27.22it/s]\n",
      "Epoch:  63| Step: 244| Train loss: 6.75561: 100%|██████████| 245/245 [00:08<00:00, 28.60it/s]\n",
      "Epoch:  64| Step: 244| Train loss: 6.66496: 100%|██████████| 245/245 [00:08<00:00, 28.51it/s]\n",
      "Epoch:  65| Step: 244| Train loss: 6.74912: 100%|██████████| 245/245 [00:08<00:00, 28.53it/s]\n",
      "Epoch:  66| Step: 244| Train loss: 6.71971: 100%|██████████| 245/245 [00:08<00:00, 28.46it/s]\n",
      "Epoch:  67| Step: 244| Train loss: 6.74589: 100%|██████████| 245/245 [00:08<00:00, 29.36it/s]\n",
      "Epoch:  68| Step: 244| Train loss: 6.72053: 100%|██████████| 245/245 [00:08<00:00, 27.84it/s]\n",
      "Epoch:  69| Step: 244| Train loss: 6.71357: 100%|██████████| 245/245 [00:08<00:00, 28.38it/s]\n",
      "Epoch:  70| Step: 244| Train loss: 6.78972: 100%|██████████| 245/245 [00:08<00:00, 29.09it/s]\n",
      "Epoch:  71| Step: 244| Train loss: 6.51784: 100%|██████████| 245/245 [00:08<00:00, 28.86it/s]\n",
      "Epoch:  72| Step: 244| Train loss: 6.67654: 100%|██████████| 245/245 [00:08<00:00, 29.63it/s]\n",
      "Epoch:  73| Step: 244| Train loss: 6.91080: 100%|██████████| 245/245 [00:08<00:00, 28.09it/s]\n",
      "Epoch:  74| Step: 244| Train loss: 6.74846: 100%|██████████| 245/245 [00:08<00:00, 29.56it/s]\n",
      "Epoch:  75| Step: 244| Train loss: 6.56393: 100%|██████████| 245/245 [00:08<00:00, 29.67it/s]\n",
      "Epoch:  76| Step: 244| Train loss: 6.76799: 100%|██████████| 245/245 [00:08<00:00, 29.46it/s]\n",
      "Epoch:  77| Step: 244| Train loss: 6.72021: 100%|██████████| 245/245 [00:08<00:00, 28.54it/s]\n",
      "Epoch:  78| Step: 244| Train loss: 6.78415: 100%|██████████| 245/245 [00:08<00:00, 29.28it/s]\n",
      "Epoch:  79| Step: 244| Train loss: 6.74641: 100%|██████████| 245/245 [00:08<00:00, 29.37it/s]\n",
      "Epoch:  80| Step: 244| Train loss: 6.67810: 100%|██████████| 245/245 [00:08<00:00, 28.63it/s]\n",
      "Epoch:  81| Step: 244| Train loss: 6.75662: 100%|██████████| 245/245 [00:08<00:00, 29.34it/s]\n",
      "Epoch:  82| Step: 244| Train loss: 6.59714: 100%|██████████| 245/245 [00:08<00:00, 29.24it/s]\n",
      "Epoch:  83| Step: 244| Train loss: 6.49934: 100%|██████████| 245/245 [00:08<00:00, 29.45it/s]\n",
      "Epoch:  84| Step: 244| Train loss: 6.59791: 100%|██████████| 245/245 [00:09<00:00, 27.22it/s]\n",
      "Epoch:  85| Step: 244| Train loss: 6.74191: 100%|██████████| 245/245 [00:08<00:00, 27.82it/s]\n",
      "Epoch:  86| Step: 244| Train loss: 6.64221: 100%|██████████| 245/245 [00:09<00:00, 27.10it/s]\n",
      "Epoch:  87| Step: 244| Train loss: 6.73782: 100%|██████████| 245/245 [00:08<00:00, 28.91it/s]\n",
      "Epoch:  88| Step: 244| Train loss: 6.87734: 100%|██████████| 245/245 [00:08<00:00, 28.00it/s]\n",
      "Epoch:  89| Step: 244| Train loss: 6.57174: 100%|██████████| 245/245 [00:08<00:00, 29.01it/s]\n",
      "Epoch:  90| Step: 244| Train loss: 6.66508: 100%|██████████| 245/245 [00:08<00:00, 27.24it/s]\n",
      "Epoch:  91| Step: 244| Train loss: 6.61554: 100%|██████████| 245/245 [00:09<00:00, 27.21it/s]\n",
      "Epoch:  92| Step: 244| Train loss: 6.63254: 100%|██████████| 245/245 [00:08<00:00, 28.82it/s]\n",
      "Epoch:  93| Step: 244| Train loss: 6.49034: 100%|██████████| 245/245 [00:08<00:00, 28.77it/s]\n",
      "Epoch:  94| Step: 244| Train loss: 6.65154: 100%|██████████| 245/245 [00:08<00:00, 28.29it/s]\n",
      "Epoch:  95| Step: 244| Train loss: 6.53789: 100%|██████████| 245/245 [00:08<00:00, 29.12it/s]\n",
      "Epoch:  96| Step: 244| Train loss: 6.44012: 100%|██████████| 245/245 [00:08<00:00, 28.58it/s]\n",
      "Epoch:  97| Step: 244| Train loss: 6.79246: 100%|██████████| 245/245 [00:09<00:00, 26.80it/s]\n",
      "Epoch:  98| Step: 244| Train loss: 6.47717: 100%|██████████| 245/245 [00:08<00:00, 27.46it/s]\n",
      "Epoch:  99| Step: 244| Train loss: 6.69132: 100%|██████████| 245/245 [00:08<00:00, 27.56it/s]\n",
      "Epoch: 100| Step: 244| Train loss: 6.70592: 100%|██████████| 245/245 [00:08<00:00, 27.42it/s]\n",
      "Epoch: 101| Step: 244| Train loss: 6.63434: 100%|██████████| 245/245 [00:08<00:00, 28.37it/s]\n",
      "Epoch: 102| Step: 244| Train loss: 6.83758: 100%|██████████| 245/245 [00:08<00:00, 28.97it/s]\n",
      "Epoch: 103| Step: 244| Train loss: 6.46072: 100%|██████████| 245/245 [00:08<00:00, 29.16it/s]\n",
      "Epoch: 104| Step: 244| Train loss: 6.63144: 100%|██████████| 245/245 [00:08<00:00, 28.85it/s]\n",
      "Epoch: 105| Step: 244| Train loss: 6.76796: 100%|██████████| 245/245 [00:08<00:00, 28.92it/s]\n",
      "Epoch: 106| Step: 244| Train loss: 6.74769: 100%|██████████| 245/245 [00:08<00:00, 29.83it/s]\n",
      "Epoch: 107| Step: 244| Train loss: 6.58958: 100%|██████████| 245/245 [00:08<00:00, 30.08it/s]\n",
      "Epoch: 108| Step: 244| Train loss: 6.64407: 100%|██████████| 245/245 [00:08<00:00, 28.16it/s]\n",
      "Epoch: 109| Step: 244| Train loss: 6.50434: 100%|██████████| 245/245 [00:08<00:00, 29.03it/s]\n",
      "Epoch: 110| Step: 244| Train loss: 6.54052: 100%|██████████| 245/245 [00:08<00:00, 28.03it/s]\n",
      "Epoch: 111| Step: 244| Train loss: 6.63884: 100%|██████████| 245/245 [00:08<00:00, 27.32it/s]\n",
      "Epoch: 112| Step: 244| Train loss: 6.69187: 100%|██████████| 245/245 [00:09<00:00, 27.09it/s]\n",
      "Epoch: 113| Step: 244| Train loss: 6.54907: 100%|██████████| 245/245 [00:09<00:00, 26.78it/s]\n",
      "Epoch: 114| Step: 244| Train loss: 6.70268: 100%|██████████| 245/245 [00:08<00:00, 27.58it/s]\n",
      "Epoch: 115| Step: 244| Train loss: 6.49646: 100%|██████████| 245/245 [00:08<00:00, 29.35it/s]\n",
      "Epoch: 116| Step: 244| Train loss: 6.60038: 100%|██████████| 245/245 [00:08<00:00, 27.88it/s]\n",
      "Epoch: 117| Step: 244| Train loss: 6.61353: 100%|██████████| 245/245 [00:08<00:00, 27.95it/s]\n",
      "Epoch: 118| Step: 244| Train loss: 6.50990: 100%|██████████| 245/245 [00:08<00:00, 29.58it/s]\n",
      "Epoch: 119| Step: 244| Train loss: 6.53939: 100%|██████████| 245/245 [00:08<00:00, 28.81it/s]\n",
      "Epoch: 120| Step: 244| Train loss: 6.48013: 100%|██████████| 245/245 [00:08<00:00, 29.34it/s]\n",
      "Epoch: 121| Step: 244| Train loss: 6.55647: 100%|██████████| 245/245 [00:08<00:00, 28.24it/s]\n",
      "Epoch: 122| Step: 244| Train loss: 6.57825: 100%|██████████| 245/245 [00:09<00:00, 27.01it/s]\n",
      "Epoch: 123| Step: 244| Train loss: 6.51034: 100%|██████████| 245/245 [00:08<00:00, 27.27it/s]\n",
      "Epoch: 124| Step: 244| Train loss: 6.66511: 100%|██████████| 245/245 [00:08<00:00, 27.85it/s]\n",
      "Epoch: 125| Step: 244| Train loss: 6.51756: 100%|██████████| 245/245 [00:08<00:00, 29.22it/s]\n",
      "Epoch: 126| Step: 244| Train loss: 6.40341: 100%|██████████| 245/245 [00:08<00:00, 28.19it/s]\n",
      "Epoch: 127| Step: 244| Train loss: 6.67603: 100%|██████████| 245/245 [00:08<00:00, 27.71it/s]\n",
      "Epoch: 128| Step: 244| Train loss: 6.65434: 100%|██████████| 245/245 [00:08<00:00, 28.14it/s]\n",
      "Epoch: 129| Step: 244| Train loss: 6.67296: 100%|██████████| 245/245 [00:08<00:00, 27.55it/s]\n",
      "Epoch: 130| Step: 244| Train loss: 6.59686: 100%|██████████| 245/245 [00:08<00:00, 27.72it/s]\n",
      "Epoch: 131| Step: 244| Train loss: 6.62031: 100%|██████████| 245/245 [00:08<00:00, 28.21it/s]\n",
      "Epoch: 132| Step: 244| Train loss: 6.60418: 100%|██████████| 245/245 [00:08<00:00, 28.70it/s]\n",
      "Epoch: 133| Step: 244| Train loss: 6.61852: 100%|██████████| 245/245 [00:09<00:00, 27.08it/s]\n",
      "Epoch: 134| Step: 244| Train loss: 6.43420: 100%|██████████| 245/245 [00:08<00:00, 27.84it/s]\n",
      "Epoch: 135| Step: 244| Train loss: 6.55352: 100%|██████████| 245/245 [00:08<00:00, 28.84it/s]\n",
      "Epoch: 136| Step: 244| Train loss: 6.50689: 100%|██████████| 245/245 [00:08<00:00, 28.05it/s]\n",
      "Epoch: 137| Step: 244| Train loss: 6.51775: 100%|██████████| 245/245 [00:08<00:00, 28.96it/s]\n",
      "Epoch: 138| Step: 244| Train loss: 6.55166: 100%|██████████| 245/245 [00:08<00:00, 28.49it/s]\n",
      "Epoch: 139| Step: 244| Train loss: 6.37684: 100%|██████████| 245/245 [00:08<00:00, 29.18it/s]\n",
      "Epoch: 140| Step: 244| Train loss: 6.73275: 100%|██████████| 245/245 [00:08<00:00, 28.14it/s]\n",
      "Epoch: 141| Step: 244| Train loss: 6.58315: 100%|██████████| 245/245 [00:08<00:00, 29.47it/s]\n",
      "Epoch: 142| Step: 244| Train loss: 6.67934: 100%|██████████| 245/245 [00:08<00:00, 29.72it/s]\n",
      "Epoch: 143| Step: 244| Train loss: 6.48618: 100%|██████████| 245/245 [00:08<00:00, 28.93it/s]\n",
      "Epoch: 144| Step: 244| Train loss: 6.46903: 100%|██████████| 245/245 [00:08<00:00, 29.27it/s]\n",
      "Epoch: 145| Step: 244| Train loss: 6.48837: 100%|██████████| 245/245 [00:08<00:00, 29.18it/s]\n",
      "Epoch: 146| Step: 244| Train loss: 6.57245: 100%|██████████| 245/245 [00:08<00:00, 28.16it/s]\n",
      "Epoch: 147| Step: 244| Train loss: 6.42134: 100%|██████████| 245/245 [00:08<00:00, 28.91it/s]\n",
      "Epoch: 148| Step: 244| Train loss: 6.63552: 100%|██████████| 245/245 [00:08<00:00, 28.00it/s]\n",
      "Epoch: 149| Step: 244| Train loss: 6.42960: 100%|██████████| 245/245 [00:09<00:00, 26.67it/s]\n",
      "Epoch: 150| Step: 244| Train loss: 6.57334: 100%|██████████| 245/245 [00:08<00:00, 27.87it/s]\n",
      "Epoch: 151| Step: 244| Train loss: 6.63963: 100%|██████████| 245/245 [00:08<00:00, 28.49it/s]\n",
      "Epoch: 152| Step: 244| Train loss: 6.49236: 100%|██████████| 245/245 [00:08<00:00, 27.86it/s]\n",
      "Epoch: 153| Step: 244| Train loss: 6.71193: 100%|██████████| 245/245 [00:08<00:00, 28.91it/s]\n",
      "Epoch: 154| Step: 244| Train loss: 6.60057: 100%|██████████| 245/245 [00:08<00:00, 28.03it/s]\n",
      "Epoch: 155| Step: 244| Train loss: 6.55151: 100%|██████████| 245/245 [00:08<00:00, 30.33it/s]\n",
      "Epoch: 156| Step: 244| Train loss: 6.63833: 100%|██████████| 245/245 [00:08<00:00, 30.48it/s]\n",
      "Epoch: 157| Step: 244| Train loss: 6.50943: 100%|██████████| 245/245 [00:08<00:00, 28.80it/s]\n",
      "Epoch: 158| Step: 244| Train loss: 6.59203: 100%|██████████| 245/245 [00:09<00:00, 27.00it/s]\n",
      "Epoch: 159| Step: 244| Train loss: 6.48335: 100%|██████████| 245/245 [00:08<00:00, 27.50it/s]\n",
      "Epoch: 160| Step: 244| Train loss: 6.63245: 100%|██████████| 245/245 [00:08<00:00, 29.15it/s]\n",
      "Epoch: 161| Step: 244| Train loss: 6.39076: 100%|██████████| 245/245 [00:08<00:00, 29.03it/s]\n",
      "Epoch: 162| Step: 244| Train loss: 6.42601: 100%|██████████| 245/245 [00:08<00:00, 28.63it/s]\n",
      "Epoch: 163| Step: 244| Train loss: 6.47840: 100%|██████████| 245/245 [00:08<00:00, 28.83it/s]\n",
      "Epoch: 164| Step: 244| Train loss: 6.40230: 100%|██████████| 245/245 [00:08<00:00, 29.18it/s]\n",
      "Epoch: 165| Step: 244| Train loss: 6.47098: 100%|██████████| 245/245 [00:08<00:00, 28.14it/s]\n",
      "Epoch: 166| Step: 244| Train loss: 6.63838: 100%|██████████| 245/245 [00:08<00:00, 28.38it/s]\n",
      "Epoch: 167| Step: 244| Train loss: 6.57622: 100%|██████████| 245/245 [00:08<00:00, 27.67it/s]\n",
      "Epoch: 168| Step: 244| Train loss: 6.48520: 100%|██████████| 245/245 [00:09<00:00, 26.39it/s]\n",
      "Epoch: 169| Step: 244| Train loss: 6.69243: 100%|██████████| 245/245 [00:08<00:00, 28.21it/s]\n",
      "Epoch: 170| Step: 244| Train loss: 6.73406: 100%|██████████| 245/245 [00:09<00:00, 27.17it/s]\n",
      "Epoch: 171| Step: 244| Train loss: 6.79201: 100%|██████████| 245/245 [00:08<00:00, 28.78it/s]\n",
      "Epoch: 172| Step: 244| Train loss: 6.62126: 100%|██████████| 245/245 [00:08<00:00, 29.19it/s]\n",
      "Epoch: 173| Step: 244| Train loss: 6.53052: 100%|██████████| 245/245 [00:08<00:00, 28.95it/s]\n",
      "Epoch: 174| Step: 244| Train loss: 6.41543: 100%|██████████| 245/245 [00:08<00:00, 28.74it/s]\n",
      "Epoch: 175| Step: 244| Train loss: 6.54028: 100%|██████████| 245/245 [00:08<00:00, 28.41it/s]\n",
      "Epoch: 176| Step: 244| Train loss: 6.37284: 100%|██████████| 245/245 [00:09<00:00, 26.57it/s]\n",
      "Epoch: 177| Step: 244| Train loss: 6.55109: 100%|██████████| 245/245 [00:09<00:00, 26.92it/s]\n",
      "Epoch: 178| Step: 244| Train loss: 6.57813: 100%|██████████| 245/245 [00:08<00:00, 28.76it/s]\n",
      "Epoch: 179| Step: 244| Train loss: 6.47185: 100%|██████████| 245/245 [00:08<00:00, 30.23it/s]\n",
      "Epoch: 180| Step: 244| Train loss: 6.52997: 100%|██████████| 245/245 [00:08<00:00, 29.28it/s]\n",
      "Epoch: 181| Step: 244| Train loss: 6.75908: 100%|██████████| 245/245 [00:08<00:00, 29.34it/s]\n",
      "Epoch: 182| Step: 244| Train loss: 6.47762: 100%|██████████| 245/245 [00:08<00:00, 29.16it/s]\n",
      "Epoch: 183| Step: 244| Train loss: 6.40918: 100%|██████████| 245/245 [00:08<00:00, 27.78it/s]\n",
      "Epoch: 184| Step: 244| Train loss: 6.49869: 100%|██████████| 245/245 [00:09<00:00, 27.15it/s]\n",
      "Epoch: 185| Step: 244| Train loss: 6.64790: 100%|██████████| 245/245 [00:08<00:00, 27.40it/s]\n",
      "Epoch: 186| Step: 244| Train loss: 6.48180: 100%|██████████| 245/245 [00:09<00:00, 26.84it/s]\n",
      "Epoch: 187| Step: 244| Train loss: 6.60562: 100%|██████████| 245/245 [00:09<00:00, 26.86it/s]\n",
      "Epoch: 188| Step: 244| Train loss: 6.61604: 100%|██████████| 245/245 [00:08<00:00, 27.42it/s]\n",
      "Epoch: 189| Step: 244| Train loss: 6.66788: 100%|██████████| 245/245 [00:08<00:00, 27.24it/s]\n",
      "Epoch: 190| Step: 244| Train loss: 6.67064: 100%|██████████| 245/245 [00:08<00:00, 28.63it/s]\n",
      "Epoch: 191| Step: 244| Train loss: 6.55365: 100%|██████████| 245/245 [00:08<00:00, 28.25it/s]\n",
      "Epoch: 192| Step: 244| Train loss: 6.60676: 100%|██████████| 245/245 [00:09<00:00, 26.58it/s]\n",
      "Epoch: 193| Step: 244| Train loss: 6.47593: 100%|██████████| 245/245 [00:08<00:00, 27.29it/s]\n",
      "Epoch: 194| Step: 244| Train loss: 6.51626: 100%|██████████| 245/245 [00:08<00:00, 27.92it/s]\n",
      "Epoch: 195| Step: 244| Train loss: 6.50128: 100%|██████████| 245/245 [00:08<00:00, 28.66it/s]\n",
      "Epoch: 196| Step: 244| Train loss: 6.44230: 100%|██████████| 245/245 [00:08<00:00, 27.78it/s]\n",
      "Epoch: 197| Step: 244| Train loss: 6.61586: 100%|██████████| 245/245 [00:08<00:00, 28.70it/s]\n",
      "Epoch: 198| Step: 244| Train loss: 6.50869: 100%|██████████| 245/245 [00:08<00:00, 28.80it/s]\n",
      "Epoch: 199| Step: 244| Train loss: 6.78609: 100%|██████████| 245/245 [00:08<00:00, 29.16it/s]\n",
      "Epoch: 200| Step: 244| Train loss: 6.55937: 100%|██████████| 245/245 [00:09<00:00, 27.02it/s]\n"
     ]
    }
   ],
   "source": [
    "best_ndcg10 = -np.inf\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_bert4rec(data_loader, model, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "with open(os.path.join(model_save_dir, model_save_file), 'wb') as f:\n",
    "    torch.save(model, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "with open(os.path.join(model_save_dir, model_save_file), 'rb') as f:\n",
    "    model = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def generate_sparse_matrix(user_seq, num_user, num_item):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    # 사용자가 본 아이템은 1로 표시하는 sparse matrix를 만들 것입니다.\n",
    "    for user_id, item_list in enumerate(user_seq):\n",
    "        for item in item_list:\n",
    "            row.append(user_id)\n",
    "            col.append(item)\n",
    "            data.append(1)\n",
    "\n",
    "    row = np.array(row)\n",
    "    col = np.array(col)\n",
    "    data = np.array(data)\n",
    "    sparse_matrix = csr_matrix((data, (row, col)), shape=(num_user, num_item))\n",
    "\n",
    "    return sparse_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "item_lists = df.groupby('user_idx')['item_idx'].apply(list)\n",
    "user_seq = []\n",
    "\n",
    "for item_list in item_lists:\n",
    "    items = item_list\n",
    "    user_seq.append(items)\n",
    "\n",
    "sparse_matrix = generate_sparse_matrix(user_seq, num_user, num_item + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "user_item = np.zeros(shape=(num_user, num_item + 1))\n",
    "\n",
    "user_to_submit = []\n",
    "item_to_submit = []\n",
    "\n",
    "for u in range(num_user):\n",
    "    pad_len = max_len - (len(user_train[u] + user_valid[u]))\n",
    "    seq = [0] * pad_len + user_train[u] + user_valid[u]\n",
    "    seq = seq[-max_len:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(np.array([seq]))\n",
    "\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        pred = pred[0][-1]\n",
    "        pred = np.expand_dims(pred, axis=0)\n",
    "\n",
    "        max_value = max(pred[0])\n",
    "        min_value = min(pred[0])\n",
    "\n",
    "        pred[0] = (pred[0] - min_value) / (max_value - min_value)\n",
    "\n",
    "        pred[sparse_matrix[u].toarray() > 0] = 0\n",
    "        user_item[u] = pred\n",
    "\n",
    "        item_indices = np.flip(np.argsort(pred[0])[-10:])\n",
    "\n",
    "        item_to_submit.extend(item_indices)\n",
    "        user_to_submit.extend([u] * 10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "np.save('BERT4Rec', user_item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "u_to_s = pd.DataFrame(user_to_submit, columns=['user'])\n",
    "i_to_s = pd.DataFrame(item_to_submit, columns=['item'])\n",
    "all_to_s = pd.concat([u_to_s, i_to_s], axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def de_numerize(tp, re_p2id, re_s2id):\n",
    "    uid2 = tp['user'].apply(lambda x: re_p2id[x])\n",
    "    sid2 = tp['item'].apply(lambda x: re_s2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid2, 'sid': sid2}, columns=['uid', 'sid'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "re_p2id = dict((v,k) for k,v in user2idx.items())\n",
    "re_s2id = dict((v,k) for k,v in item2idx.items())\n",
    "\n",
    "ans_to_s = de_numerize(all_to_s, re_p2id, re_s2id)\n",
    "ans_to_s.columns = ['user','item']\n",
    "new_ans = ans_to_s.sort_values('user')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          user   item\n",
      "0           11  48780\n",
      "1           11  32587\n",
      "2           11  54286\n",
      "3           11  49272\n",
      "4           11     47\n",
      "...        ...    ...\n",
      "313594  138493   3300\n",
      "313595  138493   5445\n",
      "313596  138493  33794\n",
      "313597  138493   6934\n",
      "313599  138493   6365\n",
      "\n",
      "[313600 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(new_ans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "new_ans.to_csv(\"/opt/ml/input/code/output/BERT4Rec.csv\",index= False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}