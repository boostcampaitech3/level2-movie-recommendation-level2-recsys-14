{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQG9hL8-UQUP"
      },
      "source": [
        "# Multi-VAE\n",
        "- Multi VAE에 감독 선호도를 반영한 노트북입니다. \n",
        "- Focal loss 역시 해당 노트북에서 실험해보았습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7CfnRw7U59C"
      },
      "source": [
        "## 1. 초기 세팅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWWEf1mPKdnX",
        "outputId": "c8bc42cc-45a7-4ff2-ed4a-d461f261f59b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas==1.0.1 in /opt/conda/lib/python3.8/site-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.0.1) (1.22.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas==1.0.1) (2021.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.8/site-packages (from pandas==1.0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas==1.0.1) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "## 전처리과정에서 pandas의 버전에 다르게 동작하는 경향이 보여, 이 미션에서는 아래 버전으로 사용하도록하겠습니다.\n",
        "!pip install pandas==1.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bQj6k1mSbxaz"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import matplotlib.pyplot as plt\n",
        "import bottleneck as bn\n",
        "import torch.nn.functional as F\n",
        "import adabound\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMWYLPFIjon8"
      },
      "source": [
        "## 데이터 다운로드\n",
        "이곳에 대회 사이트(AI Stages)에 있는 data의 URL을 입력해주세요. \n",
        "- 데이터 URL은 변경될 수 있습니다.\n",
        "- 예) `!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000176/data/data.tar.gz`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ3W0udmbxa3",
        "outputId": "c5e5b8ac-cc2c-4e9b-c388-49e550c510d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## 각종 파라미터 세팅\n",
        "parser = argparse.ArgumentParser(description='PyTorch Variational Autoencoders for Collaborative Filtering')\n",
        "\n",
        "\n",
        "parser.add_argument('--data', type=str, default='data/train/',\n",
        "                    help='Movielens dataset location')\n",
        "\n",
        "parser.add_argument('--lr', type=float, default=1e-4,\n",
        "                    help='initial learning rate')\n",
        "parser.add_argument('--wd', type=float, default=0.00,\n",
        "                    help='weight decay coefficient')\n",
        "parser.add_argument('--batch_size', type=int, default=1600,#500,\n",
        "                    help='batch size')\n",
        "parser.add_argument('--epochs', type=int, default=20, #원래 20\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--total_anneal_steps', type=int, default=200000,\n",
        "                    help='the total number of gradient updates for annealing')\n",
        "parser.add_argument('--anneal_cap', type=float, default=0.2,\n",
        "                    help='largest annealing parameter')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_true',\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--log_interval', type=int, default=100, metavar='N',\n",
        "                    help='report interval')\n",
        "parser.add_argument('--save', type=str, default='model.pt',\n",
        "                    help='path to save the final model')\n",
        "args = parser.parse_args([])\n",
        "\n",
        "# Set the random seed manually for reproductibility.\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#만약 GPU가 사용가능한 환경이라면 GPU를 사용\n",
        "if torch.cuda.is_available():\n",
        "    args.cuda = True\n",
        "\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o1fvXqFWE_G"
      },
      "source": [
        "## 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cgvNoy1Ybxa6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "\n",
        "def get_count(tp, id):\n",
        "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
        "    count = playcount_groupbyid.size()\n",
        "\n",
        "    return count\n",
        "\n",
        "# 특정한 횟수 이상의 리뷰가 존재하는(사용자의 경우 min_uc 이상, 아이템의 경우 min_sc이상) \n",
        "# 데이터만을 추출할 때 사용하는 함수입니다.\n",
        "# 현재 데이터셋에서는 결과적으로 원본그대로 사용하게 됩니다.\n",
        "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
        "    if min_sc > 0:\n",
        "        itemcount = get_count(tp, 'item')\n",
        "        tp = tp[tp['item'].isin(itemcount.index[itemcount >= min_sc])]\n",
        "\n",
        "    if min_uc > 0:\n",
        "        usercount = get_count(tp, 'user')\n",
        "        tp = tp[tp['user'].isin(usercount.index[usercount >= min_uc])]\n",
        "\n",
        "    usercount, itemcount = get_count(tp, 'user'), get_count(tp, 'item')\n",
        "    return tp, usercount, itemcount\n",
        "\n",
        "#훈련된 모델을 이용해 검증할 데이터를 분리하는 함수입니다.\n",
        "#100개의 액션이 있다면, 그중에 test_prop 비율 만큼을 비워두고, 그것을 모델이 예측할 수 있는지를\n",
        "#확인하기 위함입니다.\n",
        "def split_train_test_proportion(data, test_prop=0.2): #원래 0.2\n",
        "    data_grouped_by_user = data.groupby('user')\n",
        "    tr_list, te_list = list(), list()\n",
        "\n",
        "    np.random.seed(98765)\n",
        "    \n",
        "    for _, group in data_grouped_by_user:\n",
        "        n_items_u = len(group)\n",
        "        \n",
        "        if n_items_u >= 5:\n",
        "            idx = np.zeros(n_items_u, dtype='bool')\n",
        "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "\n",
        "            tr_list.append(group[np.logical_not(idx)])\n",
        "            te_list.append(group[idx])\n",
        "        \n",
        "        else:\n",
        "            tr_list.append(group)\n",
        "    \n",
        "    data_tr = pd.concat(tr_list)\n",
        "    data_te = pd.concat(te_list)\n",
        "\n",
        "    return data_tr, data_te\n",
        "\n",
        "def numerize(tp, profile2id, show2id):\n",
        "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
        "    sid = tp['item'].apply(lambda x: show2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVFoRHrmVQsp",
        "outputId": "c67154f0-0240-420a-ab16-e6aff09ee8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load and Preprocess Movielens dataset\n",
            "원본 데이터\n",
            "            user   item        time\n",
            "0            11   4643  1230782529\n",
            "1            11    170  1230782534\n",
            "2            11    531  1230782539\n",
            "3            11    616  1230782542\n",
            "4            11   2140  1230782563\n",
            "...         ...    ...         ...\n",
            "5154466  138493  44022  1260209449\n",
            "5154467  138493   4958  1260209482\n",
            "5154468  138493  68319  1260209720\n",
            "5154469  138493  40819  1260209726\n",
            "5154470  138493  27311  1260209807\n",
            "\n",
            "[5154471 rows x 3 columns]\n",
            "5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\n",
            "            user   item        time\n",
            "0            11   4643  1230782529\n",
            "1            11    170  1230782534\n",
            "2            11    531  1230782539\n",
            "3            11    616  1230782542\n",
            "4            11   2140  1230782563\n",
            "...         ...    ...         ...\n",
            "5154466  138493  44022  1260209449\n",
            "5154467  138493   4958  1260209482\n",
            "5154468  138493  68319  1260209720\n",
            "5154469  138493  40819  1260209726\n",
            "5154470  138493  27311  1260209807\n",
            "\n",
            "[5154471 rows x 3 columns]\n",
            "유저별 리뷰수\n",
            " user\n",
            "11        376\n",
            "14        180\n",
            "18         77\n",
            "25         91\n",
            "31        154\n",
            "         ... \n",
            "138473     63\n",
            "138475    124\n",
            "138486    137\n",
            "138492     68\n",
            "138493    314\n",
            "Length: 31360, dtype: int64\n",
            "아이템별 리뷰수\n",
            " item\n",
            "1         12217\n",
            "2          3364\n",
            "3           734\n",
            "4            43\n",
            "5           590\n",
            "          ...  \n",
            "118700       54\n",
            "118900       60\n",
            "118997       52\n",
            "119141      122\n",
            "119145       78\n",
            "Length: 6807, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Load and Preprocess Movielens dataset\")\n",
        "# Load Data\n",
        "DATA_DIR = args.data\n",
        "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train_ratings.csv'), header=0)\n",
        "print(\"원본 데이터\\n\", raw_data)\n",
        "\n",
        "# Filter Data\n",
        "raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=5, min_sc=0)\n",
        "#제공된 훈련데이터의 유저는 모두 5개 이상의 리뷰가 있습니다.\n",
        "print(\"5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\\n\",raw_data)\n",
        "\n",
        "print(\"유저별 리뷰수\\n\",user_activity)\n",
        "print(\"아이템별 리뷰수\\n\",item_popularity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T1dTsWUrffP",
        "outputId": "3a91a399-cd45-4173-d7f6-cc734fd04bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(BEFORE) unique_uid: Int64Index([    11,     14,     18,     25,     31,     35,     43,     50,\n",
            "                58,     60,\n",
            "            ...\n",
            "            138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486,\n",
            "            138492, 138493],\n",
            "           dtype='int64', name='user', length=31360)\n",
            "(AFTER) unique_uid: Int64Index([ 27968,  67764,   2581,  82969, 137831,  48639,  97870,  40424,\n",
            "             46835,  79570,\n",
            "            ...\n",
            "            114284,   9009,  21165,  33920,  22054, 135379, 125855,  41891,\n",
            "             15720,  17029],\n",
            "           dtype='int64', name='user', length=31360)\n",
            "훈련 데이터에 사용될 사용자 수: 25088\n",
            "검증 데이터에 사용될 사용자 수: 3136\n",
            "테스트 데이터에 사용될 사용자 수: 3136\n"
          ]
        }
      ],
      "source": [
        "# Shuffle User Indices\n",
        "unique_uid = user_activity.index\n",
        "print(\"(BEFORE) unique_uid:\",unique_uid)\n",
        "np.random.seed(98765)\n",
        "idx_perm = np.random.permutation(unique_uid.size)\n",
        "unique_uid = unique_uid[idx_perm]\n",
        "print(\"(AFTER) unique_uid:\",unique_uid)\n",
        "\n",
        "n_users = unique_uid.size #31360\n",
        "n_heldout_users = 3136#3000\n",
        "\n",
        "\n",
        "# Split Train/Validation/Test User Indices\n",
        "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
        "te_users = unique_uid[(n_users - n_heldout_users):]\n",
        "\n",
        "#주의: 데이터의 수가 아닌 사용자의 수입니다!\n",
        "print(\"훈련 데이터에 사용될 사용자 수:\", len(tr_users))\n",
        "print(\"검증 데이터에 사용될 사용자 수:\", len(vd_users))\n",
        "print(\"테스트 데이터에 사용될 사용자 수:\", len(te_users))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "##훈련 데이터에 해당하는 아이템들\n",
        "#Train에는 전체 데이터를 사용합니다.\n",
        "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
        "\n",
        "##아이템 ID\n",
        "unique_sid = pd.unique(train_plays['item'])\n",
        "\n",
        "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
        "\n",
        "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "\n",
        "if not os.path.exists(pro_dir):\n",
        "    os.makedirs(pro_dir)\n",
        "\n",
        "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "    for sid in unique_sid:\n",
        "        f.write('%s\\n' % sid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yBsRCRqtPz6",
        "outputId": "251683e7-c614-440e-bf83-d3854849a278"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# #Validation과 Test에는 input으로 사용될 tr 데이터와 정답을 확인하기 위한 te 데이터로 분리되었습니다.\n",
        "# vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
        "# vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
        "# vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
        "\n",
        "# test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
        "# test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
        "# test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
        "\n",
        "\n",
        "\n",
        "# train_data = numerize(train_plays, profile2id, show2id)\n",
        "# train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
        "\n",
        "\n",
        "# vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
        "# vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
        "\n",
        "# vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
        "# vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
        "\n",
        "# test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
        "# test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
        "\n",
        "# test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
        "# test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
        "\n",
        "# print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMiq9leyWWL1"
      },
      "source": [
        "##3. 데이터 로더 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nxUADr9ibxa8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataLoader():\n",
        "    '''\n",
        "    Load Movielens dataset\n",
        "    '''\n",
        "    def __init__(self, path):\n",
        "        \n",
        "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
        "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
        "\n",
        "        self.n_items = self.load_n_items()\n",
        "    \n",
        "    def load_data(self, datatype='train'):\n",
        "        if datatype == 'train':\n",
        "            return self._load_train_data()\n",
        "        elif datatype == 'validation':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        elif datatype == 'test':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        else:\n",
        "            raise ValueError(\"datatype should be in [train, validation, test]\")\n",
        "        \n",
        "    def load_n_items(self):\n",
        "        unique_sid = list()\n",
        "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                unique_sid.append(line.strip())\n",
        "        n_items = len(unique_sid)\n",
        "        return n_items\n",
        "    \n",
        "    def _load_train_data(self):\n",
        "        path = os.path.join(self.pro_dir, 'train.csv')\n",
        "        \n",
        "        tp = pd.read_csv(path)\n",
        "        n_users = tp['uid'].max() + 1\n",
        "\n",
        "        rows, cols = tp['uid'], tp['sid']\n",
        "        data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                                 (rows, cols)), dtype='float64',\n",
        "                                 shape=(n_users, self.n_items))\n",
        "        return data\n",
        "    \n",
        "    def _load_tr_te_data(self, datatype='test'):\n",
        "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
        "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
        "\n",
        "        tp_tr = pd.read_csv(tr_path)\n",
        "        tp_te = pd.read_csv(te_path)\n",
        "\n",
        "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        return data_tr, data_te"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHhwKqXWaUZ"
      },
      "source": [
        "##4. 모델정의\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "QYlGPJTYU0ii"
      },
      "outputs": [],
      "source": [
        "#이미 완성된 MultiDAE의 코드를 참고하여 그 아래 MultiVAE의 코드를 완성해보세요!\n",
        "class MultiDAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-DAE.\n",
        "\n",
        "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
        "        super(MultiDAE, self).__init__()\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "\n",
        "        self.dims = self.q_dims + self.p_dims[1:]\n",
        "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        \n",
        "        self.init_weights()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        h = F.normalize(input)\n",
        "        h = self.drop(h)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "        return h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "\n",
        "\n",
        "class MultiVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-VAE.\n",
        "\n",
        "    Multi-VAE : Variational Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
        "        super(MultiVAE, self).__init__()\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "\n",
        "        # Last dimension of q- network is for mean and variance\n",
        "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
        "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
        "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
        "        \n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.init_weights()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        mu, logvar = self.encode(input)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "    \n",
        "    def encode(self, input):\n",
        "        h = F.normalize(input)\n",
        "        h = self.drop(h)\n",
        "        \n",
        "        for i, layer in enumerate(self.q_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.q_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "            else:\n",
        "                mu = h[:, :self.q_dims[-1]]\n",
        "                logvar = h[:, self.q_dims[-1]:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "    \n",
        "    def decode(self, z):\n",
        "        h = z\n",
        "        for i, layer in enumerate(self.p_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.p_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "        return h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.q_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "        \n",
        "        for layer in self.p_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    binary focal loss\n",
        "    https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/13\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=5):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        # self.weight = torch.tensor([alpha, 1-alpha])\n",
        "        self.gamma = gamma\n",
        "        self.alpha= alpha\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        # BCE_loss = -torch.mean(torch.sum(F.log_softmax(input, 1) * target, -1))\n",
        "        BCE_loss= F.cross_entropy(input, target)\n",
        "        pt = torch.exp(-BCE_loss) # prevents nans when probability 0\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return F_loss.mean()\n",
        "\n",
        "def loss_function_vae(recon_x, x, mu, logvar, anneal=1.0, focal=True):\n",
        "    if focal:\n",
        "        floss= FocalLoss()\n",
        "        BCE= floss(recon_x, x)\n",
        "        # BCE= BCE.to(device)\n",
        "    else:\n",
        "        BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
        "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
        "\n",
        "    return BCE + anneal * KLD\n",
        "\n",
        "def loss_function_dae(recon_x, x, focal= True):\n",
        "    if focal:\n",
        "        floss= FocalLoss()\n",
        "        BCE= floss(recon_x, x)\n",
        "        # BCE= BCE.to(device)\n",
        "    else:\n",
        "        BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
        "\n",
        "    return BCE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.3976,  0.2283, -0.3871,  ..., -2.3925,  1.3741, -6.7644],\n",
              "        [ 1.7043,  0.7015,  2.1623,  ...,  0.2044, -3.1941,  1.0873],\n",
              "        [ 0.5015,  1.2869,  1.0603,  ..., -5.4036, -4.6506, -1.9421],\n",
              "        ...,\n",
              "        [ 2.5130, -0.0793,  3.9166,  ..., -5.7970, -4.6783, -2.8499],\n",
              "        [ 1.3124,  1.5735, -0.0126,  ..., -8.0362, -4.4553, -2.1399],\n",
              "        [-0.3488,  1.6826,  2.2812,  ..., -5.2224, -5.1640, -3.3567]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recon_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "7nEfVTktbxa8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sparse2torch_sparse(data):\n",
        "    \"\"\"\n",
        "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
        "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
        "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
        "    \"\"\"\n",
        "    samples = data.shape[0]\n",
        "    features = data.shape[1]\n",
        "    coo_data = data.tocoo()\n",
        "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
        "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
        "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
        "    values = np.array([row2val[r] for r in coo_data.row])\n",
        "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
        "    return t\n",
        "\n",
        "def naive_sparse2tensor(data):\n",
        "    return torch.FloatTensor(data.toarray())\n",
        "\n",
        "\n",
        "def train(model, criterion, optimizer, is_VAE = False):\n",
        "    # Turn on training mode\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    global update_count\n",
        "    global recon_test\n",
        "    np.random.shuffle(idxlist)\n",
        "    \n",
        "    for batch_idx, start_idx in enumerate(range(0, N, args.batch_size)):\n",
        "        end_idx = min(start_idx + args.batch_size, N)\n",
        "        data = train_data[idxlist[start_idx:end_idx]]\n",
        "        data = naive_sparse2tensor(data).to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if is_VAE:\n",
        "          if args.total_anneal_steps > 0:\n",
        "            anneal = min(args.anneal_cap, \n",
        "                            1. * update_count / args.total_anneal_steps)\n",
        "          else:\n",
        "              anneal = args.anneal_cap\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          recon_batch, mu, logvar = model(data)\n",
        "        \n",
        "          loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
        "        else:\n",
        "          recon_batch = model(data)\n",
        "          # recon_test= recon_batch\n",
        "          loss = criterion(recon_batch, data)\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        update_count += 1\n",
        "\n",
        "        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
        "                    'loss {:4.2f}'.format(\n",
        "                        epoch, batch_idx, len(range(0, N, args.batch_size)),\n",
        "                        elapsed * 1000 / args.log_interval,\n",
        "                        train_loss / args.log_interval))\n",
        "            \n",
        "\n",
        "            start_time = time.time()\n",
        "            train_loss = 0.0\n",
        "    train_loss /= len(range(0, N, args.batch_size))\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, data_tr, data_te, is_VAE=False):\n",
        "    # Turn on evaluation mode\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    global update_count\n",
        "    e_idxlist = list(range(data_tr.shape[0]))\n",
        "    e_N = data_tr.shape[0]\n",
        "    n100_list = []\n",
        "    r10_list= []\n",
        "    r20_list = []\n",
        "    r50_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for start_idx in range(0, e_N, args.batch_size):\n",
        "            end_idx = min(start_idx + args.batch_size, N)\n",
        "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
        "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
        "\n",
        "            data_tensor = naive_sparse2tensor(data).to(device)\n",
        "            if is_VAE :\n",
        "              \n",
        "              if args.total_anneal_steps > 0:\n",
        "                  anneal = min(args.anneal_cap, \n",
        "                                1. * update_count / args.total_anneal_steps)\n",
        "              else:\n",
        "                  anneal = args.anneal_cap\n",
        "\n",
        "              recon_batch, mu, logvar = model(data_tensor)\n",
        "\n",
        "              loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
        "\n",
        "            else :\n",
        "              recon_batch = model(data_tensor)\n",
        "              loss = criterion(recon_batch, data_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Exclude examples from training set\n",
        "            recon_batch = recon_batch.cpu().numpy()\n",
        "            recon_batch[data.nonzero()] = -np.inf\n",
        "\n",
        "            n100 = NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
        "            r20 = Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
        "            r10 = Recall_at_k_batch(recon_batch, heldout_data, 10)\n",
        "            r50 = Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
        "\n",
        "            n100_list.append(n100)\n",
        "            r20_list.append(r20)\n",
        "            r10_list.append(r10)\n",
        "            r50_list.append(r50)\n",
        " \n",
        "    total_loss /= len(range(0, e_N, args.batch_size))\n",
        "    n100_list = np.concatenate(n100_list)\n",
        "    r20_list = np.concatenate(r20_list)\n",
        "    r10_list = np.concatenate(r10_list)\n",
        "    r50_list = np.concatenate(r50_list)\n",
        "\n",
        "    return total_loss, np.mean(n100_list), np.mean(r10_list), np.mean(r20_list), np.mean(r50_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOsCJbb_X9gl"
      },
      "source": [
        "## Metric 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "zxNtit6vbxa-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    Normalized Discounted Cumulative Gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "    batch_users = X_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "    return DCG / IDCG\n",
        "\n",
        "\n",
        "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    batch_users = X_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
        "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
        "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    X_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDD7lD7sHcnH"
      },
      "source": [
        "## MultiDAE 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "WLYyTwToX4fm"
      },
      "outputs": [],
      "source": [
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "loader = DataLoader(args.data)\n",
        "\n",
        "n_items = loader.load_n_items()\n",
        "train_data = loader.load_data('train')\n",
        "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "N = train_data.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "#p_dims = [200, 600, 1600, 3200, n_items]\n",
        "p_dims = [200, 3000, n_items]\n",
        "model = MultiDAE(p_dims).to(device)\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
        "#https://github.com/Luolc/AdaBound\n",
        "#optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
        "criterion = loss_function_dae\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "best_r10 = -np.inf\n",
        "update_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0kLNpLo2f9A",
        "outputId": "ed5674ce-b002-4e39-e5d2-b1fae67d34e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 2.41s | valid loss 255.45 | n100 0.264 | r10 0.205 | r20 0.193 | r50 0.240\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 2.41s | valid loss 247.93 | n100 0.309 | r10 0.245 | r20 0.228 | r50 0.284\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 2.39s | valid loss 244.88 | n100 0.334 | r10 0.272 | r20 0.250 | r50 0.305\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 2.48s | valid loss 241.80 | n100 0.359 | r10 0.296 | r20 0.271 | r50 0.327\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 2.46s | valid loss 239.73 | n100 0.372 | r10 0.309 | r20 0.284 | r50 0.340\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 2.47s | valid loss 238.12 | n100 0.385 | r10 0.317 | r20 0.293 | r50 0.352\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 2.46s | valid loss 237.25 | n100 0.389 | r10 0.321 | r20 0.297 | r50 0.355\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 2.48s | valid loss 236.56 | n100 0.392 | r10 0.324 | r20 0.299 | r50 0.360\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 2.49s | valid loss 236.02 | n100 0.397 | r10 0.328 | r20 0.303 | r50 0.365\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 2.47s | valid loss 235.48 | n100 0.401 | r10 0.332 | r20 0.307 | r50 0.368\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 2.54s | valid loss 234.98 | n100 0.404 | r10 0.338 | r20 0.311 | r50 0.371\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 2.50s | valid loss 234.59 | n100 0.405 | r10 0.335 | r20 0.311 | r50 0.372\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 2.49s | valid loss 234.25 | n100 0.408 | r10 0.341 | r20 0.314 | r50 0.376\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 2.48s | valid loss 233.93 | n100 0.410 | r10 0.345 | r20 0.318 | r50 0.378\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 2.52s | valid loss 233.60 | n100 0.413 | r10 0.344 | r20 0.317 | r50 0.381\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 2.46s | valid loss 233.30 | n100 0.414 | r10 0.345 | r20 0.319 | r50 0.383\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 2.47s | valid loss 233.10 | n100 0.418 | r10 0.353 | r20 0.323 | r50 0.385\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 2.46s | valid loss 232.83 | n100 0.421 | r10 0.356 | r20 0.327 | r50 0.387\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 2.47s | valid loss 232.56 | n100 0.418 | r10 0.349 | r20 0.323 | r50 0.387\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 2.59s | valid loss 232.36 | n100 0.420 | r10 0.353 | r20 0.323 | r50 0.388\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 2.55s | valid loss 232.20 | n100 0.420 | r10 0.350 | r20 0.324 | r50 0.390\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 2.53s | valid loss 232.00 | n100 0.422 | r10 0.351 | r20 0.327 | r50 0.391\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 2.47s | valid loss 231.86 | n100 0.422 | r10 0.350 | r20 0.326 | r50 0.392\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 2.56s | valid loss 231.67 | n100 0.424 | r10 0.354 | r20 0.329 | r50 0.393\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 2.54s | valid loss 231.53 | n100 0.424 | r10 0.353 | r20 0.329 | r50 0.394\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 2.59s | valid loss 231.38 | n100 0.426 | r10 0.358 | r20 0.333 | r50 0.395\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 2.55s | valid loss 231.26 | n100 0.429 | r10 0.363 | r20 0.334 | r50 0.397\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 2.50s | valid loss 231.12 | n100 0.430 | r10 0.363 | r20 0.335 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 2.54s | valid loss 231.01 | n100 0.429 | r10 0.363 | r20 0.334 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 2.53s | valid loss 230.89 | n100 0.431 | r10 0.368 | r20 0.337 | r50 0.399\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 2.53s | valid loss 230.77 | n100 0.428 | r10 0.358 | r20 0.332 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 2.54s | valid loss 230.66 | n100 0.428 | r10 0.357 | r20 0.333 | r50 0.397\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 2.52s | valid loss 230.55 | n100 0.431 | r10 0.365 | r20 0.338 | r50 0.400\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 2.56s | valid loss 230.44 | n100 0.431 | r10 0.364 | r20 0.337 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 2.52s | valid loss 230.32 | n100 0.433 | r10 0.367 | r20 0.339 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 2.56s | valid loss 230.23 | n100 0.431 | r10 0.362 | r20 0.336 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 2.52s | valid loss 230.13 | n100 0.432 | r10 0.364 | r20 0.340 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 2.48s | valid loss 230.05 | n100 0.432 | r10 0.365 | r20 0.338 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 2.49s | valid loss 229.95 | n100 0.433 | r10 0.368 | r20 0.339 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 2.55s | valid loss 229.87 | n100 0.432 | r10 0.364 | r20 0.339 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 2.50s | valid loss 229.79 | n100 0.433 | r10 0.363 | r20 0.337 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 2.54s | valid loss 229.71 | n100 0.436 | r10 0.371 | r20 0.343 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 2.50s | valid loss 229.62 | n100 0.435 | r10 0.370 | r20 0.341 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 2.50s | valid loss 229.53 | n100 0.435 | r10 0.368 | r20 0.340 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 2.46s | valid loss 229.48 | n100 0.438 | r10 0.373 | r20 0.344 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 2.47s | valid loss 229.38 | n100 0.434 | r10 0.365 | r20 0.341 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 2.46s | valid loss 229.31 | n100 0.436 | r10 0.370 | r20 0.342 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 2.47s | valid loss 229.25 | n100 0.438 | r10 0.374 | r20 0.343 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 2.51s | valid loss 229.20 | n100 0.438 | r10 0.374 | r20 0.345 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 2.51s | valid loss 229.11 | n100 0.438 | r10 0.375 | r20 0.345 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "=========================================================================================\n",
            "| End of training | test loss 226.07 | n100 0.44 | r10 0.37 | r20 0.35 | r50 0.41\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "############batch 1600\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "r10_fin_list = []\n",
        "new_epochs = 50\n",
        "\n",
        "for epoch in range(1, new_epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_loss = train(model, criterion, optimizer, is_VAE=False)\n",
        "    val_loss, n100, r10, r20, r50 = evaluate(model, criterion, vad_data_tr, vad_data_te, is_VAE=False)\n",
        "    \n",
        "    train_loss_list.append(train_loss)\n",
        "    val_loss_list.append(val_loss)\n",
        "    r10_fin_list.append(r10)\n",
        "\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
        "            'n100 {:5.3f} | r10 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
        "                epoch, time.time() - epoch_start_time, val_loss,\n",
        "                n100, r10, r20, r50))\n",
        "    print('-' * 89)\n",
        "\n",
        "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
        "\n",
        "\n",
        "    # Save the model if the n100 is the best we've seen so far.\n",
        "    if n100 > best_r10:\n",
        "        with open(args.save, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_r10 = n100\n",
        "        print(\"Better performance! save best model...\")\n",
        "\n",
        "\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss, n100, r10, r20, r50 = evaluate(model, criterion, test_data_tr, test_data_te, is_VAE=False)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r10 {:4.2f} | r20 {:4.2f} | '\n",
        "        'r50 {:4.2f}'.format(test_loss, n100, r10, r20, r50))\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "tiq3-LkR2qIh",
        "outputId": "6d728e63-3c86-4bc0-b544-3aa61425c18a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, (loss_ax, recall_ax) = plt.subplots(1,2, figsize=(12,4))\n",
        "\n",
        "loss_ax.plot(range(1,new_epochs+1), train_loss_list, 'b', label='train loss')\n",
        "loss_ax.plot(range(1,new_epochs+1), val_loss_list, 'r', label='valid loss')\n",
        "recall_ax.plot(range(1,new_epochs+1), r10_fin_list, 'r', label='valid recall@10')\n",
        "\n",
        "loss_ax.set_xticks(range(0, new_epochs+1, 10))\n",
        "loss_ax.set_xlabel('Epoch')\n",
        "loss_ax.set_ylabel('Average Loss')\n",
        "loss_ax.legend()\n",
        "\n",
        "recall_ax.set_xticks(range(0, new_epochs+1, 10))\n",
        "recall_ax.set_xlabel('Epoch')\n",
        "recall_ax.set_ylabel('Recall@10')\n",
        "recall_ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.37461759259259253"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(r10_fin_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1QjCbMBXw4v"
      },
      "source": [
        "## MultiVAE 테스트 (TODO)\n",
        "\n",
        "위의 MultiVAE 모델 코드, train, evaluate 함수를 완성하여, 아래 훈련 코드가 정상적으로 동작하도록 해보세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "78zFFNzgbxa_"
      },
      "outputs": [],
      "source": [
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "loader = DataLoader(args.data)\n",
        "\n",
        "n_items = loader.load_n_items()\n",
        "train_data = loader.load_data('train')\n",
        "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "N = train_data.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "p_dims = [200, 3000, n_items]\n",
        "model2 = MultiVAE(p_dims).to(device)\n",
        "\n",
        "optimizer = adabound.AdaBound(model2.parameters(), lr=1e-3, final_lr=0.1)\n",
        "#optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
        "criterion2 = loss_function_vae\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "best_n100 = -np.inf\n",
        "update_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T6oN-FEZHwT",
        "outputId": "f7940c92-aeda-446f-9cbc-34c1ef400313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 2.29s | valid loss 255.91 | n100 0.256 | r10 0.192 | r20 0.189 | r50 0.239\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 2.43s | valid loss 249.05 | n100 0.302 | r10 0.243 | r20 0.227 | r50 0.279\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 2.38s | valid loss 245.01 | n100 0.337 | r10 0.273 | r20 0.254 | r50 0.309\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 2.43s | valid loss 241.80 | n100 0.362 | r10 0.300 | r20 0.274 | r50 0.330\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 2.36s | valid loss 240.36 | n100 0.371 | r10 0.308 | r20 0.285 | r50 0.338\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 2.41s | valid loss 238.89 | n100 0.383 | r10 0.320 | r20 0.292 | r50 0.349\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 2.40s | valid loss 238.04 | n100 0.388 | r10 0.324 | r20 0.296 | r50 0.352\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 2.39s | valid loss 237.15 | n100 0.393 | r10 0.328 | r20 0.301 | r50 0.359\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 2.41s | valid loss 236.46 | n100 0.396 | r10 0.326 | r20 0.303 | r50 0.365\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 2.41s | valid loss 235.84 | n100 0.401 | r10 0.334 | r20 0.308 | r50 0.367\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 2.41s | valid loss 235.36 | n100 0.403 | r10 0.333 | r20 0.309 | r50 0.371\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 2.41s | valid loss 234.85 | n100 0.407 | r10 0.338 | r20 0.311 | r50 0.373\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 2.43s | valid loss 234.45 | n100 0.410 | r10 0.338 | r20 0.313 | r50 0.377\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 2.40s | valid loss 234.11 | n100 0.412 | r10 0.343 | r20 0.317 | r50 0.379\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 2.43s | valid loss 233.82 | n100 0.413 | r10 0.348 | r20 0.321 | r50 0.382\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 2.47s | valid loss 233.55 | n100 0.416 | r10 0.348 | r20 0.322 | r50 0.384\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 2.45s | valid loss 233.31 | n100 0.419 | r10 0.352 | r20 0.322 | r50 0.386\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 2.44s | valid loss 233.08 | n100 0.418 | r10 0.351 | r20 0.323 | r50 0.387\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 2.46s | valid loss 232.89 | n100 0.420 | r10 0.354 | r20 0.325 | r50 0.389\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 2.45s | valid loss 232.73 | n100 0.421 | r10 0.354 | r20 0.326 | r50 0.390\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 2.42s | valid loss 232.59 | n100 0.423 | r10 0.355 | r20 0.328 | r50 0.391\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 2.43s | valid loss 232.40 | n100 0.424 | r10 0.355 | r20 0.328 | r50 0.392\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 2.43s | valid loss 232.27 | n100 0.426 | r10 0.360 | r20 0.331 | r50 0.393\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 2.42s | valid loss 232.10 | n100 0.425 | r10 0.355 | r20 0.331 | r50 0.395\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 2.41s | valid loss 231.97 | n100 0.427 | r10 0.360 | r20 0.331 | r50 0.396\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 2.46s | valid loss 231.85 | n100 0.426 | r10 0.357 | r20 0.331 | r50 0.396\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 2.44s | valid loss 231.76 | n100 0.428 | r10 0.360 | r20 0.333 | r50 0.397\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 2.47s | valid loss 231.64 | n100 0.428 | r10 0.362 | r20 0.334 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 2.45s | valid loss 231.57 | n100 0.430 | r10 0.363 | r20 0.335 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 2.42s | valid loss 231.46 | n100 0.429 | r10 0.363 | r20 0.335 | r50 0.400\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 2.49s | valid loss 231.37 | n100 0.432 | r10 0.366 | r20 0.338 | r50 0.400\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 2.45s | valid loss 231.28 | n100 0.430 | r10 0.360 | r20 0.335 | r50 0.399\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 2.48s | valid loss 231.21 | n100 0.432 | r10 0.362 | r20 0.336 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 2.40s | valid loss 231.16 | n100 0.433 | r10 0.367 | r20 0.339 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 2.42s | valid loss 231.06 | n100 0.431 | r10 0.364 | r20 0.338 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 2.43s | valid loss 231.00 | n100 0.435 | r10 0.371 | r20 0.340 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "Better performance! save best model...\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 2.45s | valid loss 230.93 | n100 0.433 | r10 0.365 | r20 0.339 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 2.39s | valid loss 230.86 | n100 0.434 | r10 0.365 | r20 0.339 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 2.39s | valid loss 230.81 | n100 0.435 | r10 0.370 | r20 0.341 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 2.45s | valid loss 230.75 | n100 0.432 | r10 0.362 | r20 0.337 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 2.46s | valid loss 230.69 | n100 0.435 | r10 0.366 | r20 0.339 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 2.47s | valid loss 230.63 | n100 0.436 | r10 0.367 | r20 0.341 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 2.43s | valid loss 230.56 | n100 0.436 | r10 0.366 | r20 0.341 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 2.42s | valid loss 230.53 | n100 0.433 | r10 0.365 | r20 0.341 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 2.46s | valid loss 230.46 | n100 0.435 | r10 0.368 | r20 0.341 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 2.51s | valid loss 230.42 | n100 0.437 | r10 0.367 | r20 0.340 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 2.46s | valid loss 230.37 | n100 0.437 | r10 0.370 | r20 0.341 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 2.46s | valid loss 230.32 | n100 0.437 | r10 0.365 | r20 0.340 | r50 0.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 2.46s | valid loss 230.28 | n100 0.437 | r10 0.370 | r20 0.341 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 2.47s | valid loss 230.25 | n100 0.437 | r10 0.369 | r20 0.341 | r50 0.406\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 228.28 | n100 0.44 | r10 0.37 | r20 0.34 | r50 0.40\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "r10_fin_list = []\n",
        "\n",
        "best_r10 = -np.inf\n",
        "# save best model as r10\n",
        "for epoch in range(1, 50 + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_loss = train(model2, criterion2, optimizer, is_VAE=True)\n",
        "    val_loss, n100, r10, r20, r50 = evaluate(model2, criterion2, vad_data_tr, vad_data_te, is_VAE=True)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
        "            'n100 {:5.3f} | r10 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
        "                epoch, time.time() - epoch_start_time, val_loss,\n",
        "                n100, r10, r20, r50))\n",
        "    print('-' * 89)\n",
        "\n",
        "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
        "    train_loss_list.append(train_loss)\n",
        "    val_loss_list.append(val_loss)\n",
        "    r10_fin_list.append(r10)\n",
        "\n",
        "    # Save the model if the r10 is the best we've seen so far.\n",
        "    if r10 > best_r10:\n",
        "        with open(args.save, 'wb') as f:\n",
        "            torch.save(model2, f)\n",
        "        best_r10 = r10\n",
        "        print(\"Better performance! save best model...\")\n",
        "\n",
        "\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model2 = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss, n100, r10, r20, r50 = evaluate(model2, criterion2, test_data_tr, test_data_te, is_VAE=True)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r10 {:4.2f} | r20 {:4.2f} | '\n",
        "        'r50 {:4.2f}'.format(test_loss, n100, r10, r20, r50))\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ZHfVkdl11p2e",
        "outputId": "7e841562-f668-4fa8-cbff-2421dee67b51"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, (loss_ax, recall_ax) = plt.subplots(1,2, figsize=(12,4))\n",
        "\n",
        "loss_ax.plot(range(1,new_epochs+1), train_loss_list, 'b', label='train loss')\n",
        "loss_ax.plot(range(1,new_epochs+1), val_loss_list, 'r', label='valid loss')\n",
        "recall_ax.plot(range(1,new_epochs+1), r10_fin_list, 'r', label='valid recall@10')\n",
        "\n",
        "loss_ax.set_xticks(range(0, new_epochs+1, 10))\n",
        "loss_ax.set_xlabel('Epoch')\n",
        "loss_ax.set_ylabel('Average Loss')\n",
        "loss_ax.legend()\n",
        "\n",
        "recall_ax.set_xticks(range(0, new_epochs+1, 10))\n",
        "recall_ax.set_xlabel('Epoch')\n",
        "recall_ax.set_ylabel('Recall@10')\n",
        "recall_ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test에 먼저 적용해보고 recall값 확인\n",
        "N = test_data_tr.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "model.eval()\n",
        "model2.eval()\n",
        "total_loss = 0.0\n",
        "e_idxlist = list(range(test_data_tr.shape[0]))\n",
        "e_N = test_data_tr.shape[0]\n",
        "pred_list = None\n",
        "\n",
        "total_loss2 = 0\n",
        "with torch.no_grad():\n",
        "    for start_idx in range(0, e_N, args.batch_size):\n",
        "        data_batch = test_data_tr\n",
        "\n",
        "        data_tensor = naive_sparse2tensor(data_batch).to(device)\n",
        "        data_tensor2 = naive_sparse2tensor(data_batch).to(device)\n",
        "\n",
        "        if args.total_anneal_steps > 0:\n",
        "            anneal = min(args.anneal_cap, 1. * update_count / args.total_anneal_steps)\n",
        "        else:\n",
        "            anneal = args.anneal_cap\n",
        "\n",
        "        recon_batch2, mu, logvar = model2(data_tensor2)\n",
        "        loss2 = criterion2(recon_batch2, data_tensor2, mu, logvar, anneal)\n",
        "        total_loss2 += loss2.item()\n",
        "        \n",
        "        recon_batch = model(data_tensor)\n",
        "        loss = criterion(recon_batch, data_tensor)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Exclude examples from training set\n",
        "        recon_batch2 = recon_batch2.cpu().numpy()\n",
        "        recon_batch = recon_batch.cpu().numpy()\n",
        "        \n",
        "\n",
        "        recon_batch = np.add(recon_batch, recon_batch2) # 1:1로 앙상블\n",
        "        recon_batch[data_batch.nonzero()] = -np.inf\n",
        "\n",
        "        # 여기에 감독 선호도 추가하면 될듯!\n",
        "  \n",
        "        ##Recall\n",
        "        batch_users = recon_batch.shape[0]\n",
        "        idx = bn.argpartition(-recon_batch, 10, axis=1)[:, :10]\n",
        "        if start_idx == 0:\n",
        "            pred_list = idx\n",
        "        else:\n",
        "            pred_list = np.append(pred_list, idx, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3745814814814815\n"
          ]
        }
      ],
      "source": [
        "print(np.mean(Recall_at_k_batch(recon_batch, test_data_te, k=10)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 감독선호도 반영 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('pref_item0.npy','rb') as f:\n",
        "    pref_item0= np.load(f)\n",
        "with open('pref_test0.npy','rb') as f:\n",
        "    pref_test0= np.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.38673203656462585\n"
          ]
        }
      ],
      "source": [
        "# test에 먼저 적용해보고 recall값 확인\n",
        "\n",
        "N = test_data_tr.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "model.eval()\n",
        "model2.eval()\n",
        "total_loss = 0.0\n",
        "e_idxlist = list(range(test_data_tr.shape[0]))\n",
        "e_N = test_data_tr.shape[0]\n",
        "pred_list = None\n",
        "\n",
        "total_loss2 = 0\n",
        "with torch.no_grad():\n",
        "    data_batch = test_data_tr\n",
        "\n",
        "    data_tensor = naive_sparse2tensor(data_batch).to(device)\n",
        "    data_tensor2 = naive_sparse2tensor(data_batch).to(device)\n",
        "\n",
        "    if args.total_anneal_steps > 0:\n",
        "        anneal = min(args.anneal_cap, 1. * update_count / args.total_anneal_steps)\n",
        "    else:\n",
        "        anneal = args.anneal_cap\n",
        "\n",
        "    recon_batch2, mu, logvar = model2(data_tensor2)\n",
        "    # loss2 = criterion2(recon_batch2, data_tensor2, mu, logvar, anneal)\n",
        "    # total_loss2 += loss2.item()\n",
        "    \n",
        "    recon_batch = model(data_tensor)\n",
        "    # loss = criterion(recon_batch, data_tensor)\n",
        "    # total_loss += loss.item()\n",
        "\n",
        "    # Exclude examples from training set\n",
        "    recon_batch2 = recon_batch2.cpu().numpy()\n",
        "    recon_batch = recon_batch.cpu().numpy()\n",
        "    \n",
        "\n",
        "    recon_batch = np.add(recon_batch, recon_batch2) # 1:1로 앙상블\n",
        "    recon_batch[data_batch.nonzero()] = -np.inf\n",
        "    # recon_batch= recon_batch**2\n",
        "\n",
        "    # 여기에 감독 선호도 추가하면 될듯!\n",
        "    recon_batch *= pref_test0\n",
        "    \n",
        "    ##Recall\n",
        "    batch_users = recon_batch.shape[0]\n",
        "    idx = bn.argpartition(-recon_batch, 10, axis=1)[:, :10]\n",
        "    pred_list= idx\n",
        "\n",
        "print(np.mean(Recall_at_k_batch(recon_batch, test_data_te, k=10)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "M2d-C8jH2OCE"
      },
      "outputs": [],
      "source": [
        "## 배치사이즈 포함\n",
        "def numerize_for_infer(tp, profile2id, show2id):\n",
        "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
        "    sid = tp['item'].apply(lambda x: show2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
        "\n",
        "### 데이터 준비    \n",
        "infer_df = numerize_for_infer(raw_data, profile2id, show2id)\n",
        "\n",
        "loader = DataLoader(args.data)\n",
        "n_items = loader.load_n_items()\n",
        "\n",
        "n_users = infer_df['uid'].max() + 1\n",
        "\n",
        "rows, cols = infer_df['uid'], infer_df['sid']\n",
        "data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                                 (rows, cols)), dtype='float64',\n",
        "                                 shape=(n_users, n_items))\n",
        "\n",
        "N = data.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "model.eval()\n",
        "model2.eval()\n",
        "total_loss = 0.0\n",
        "e_idxlist = list(range(data.shape[0]))\n",
        "e_N = data.shape[0]\n",
        "pred_list = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmLtUTWdwn_H",
        "outputId": "cb3f2097-0647-47d8-d70b-cd2c3043c298"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        }
      ],
      "source": [
        "total_loss2 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    data_batch = data\n",
        "\n",
        "    data_tensor = naive_sparse2tensor(data_batch).to(device)\n",
        "    data_tensor2 = naive_sparse2tensor(data_batch).to(device)\n",
        "\n",
        "    if args.total_anneal_steps > 0:\n",
        "        anneal = min(args.anneal_cap, 1. * update_count / args.total_anneal_steps)\n",
        "    else:\n",
        "        anneal = args.anneal_cap\n",
        "\n",
        "    recon_batch2, mu, logvar = model2(data_tensor2)\n",
        "    # loss2 = criterion2(recon_batch2, data_tensor2, mu, logvar, anneal)\n",
        "    # total_loss2 += loss2.item()\n",
        "    \n",
        "    recon_batch = model(data_tensor)\n",
        "    # loss = criterion(recon_batch, data_tensor)\n",
        "    # total_loss += loss.item()\n",
        "\n",
        "    # Exclude examples from training set\n",
        "    recon_batch2 = recon_batch2.cpu().numpy()\n",
        "    recon_batch = recon_batch.cpu().numpy()\n",
        "    \n",
        "\n",
        "    recon_batch = np.add(recon_batch, recon_batch2) # 1:1로 앙상블\n",
        "    recon_batch[data_batch.nonzero()] = -np.inf\n",
        "\n",
        "    # 여기에 감독 선호도 추가하면 될듯!\n",
        "    recon_batch *= pref_item0\n",
        "    \n",
        "    ##Recall\n",
        "    batch_users = recon_batch.shape[0]\n",
        "    idx = bn.argpartition(-recon_batch, 10, axis=1)[:, :10]\n",
        "    pred_list= idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(31360, 10)"
            ]
          },
          "execution_count": 322,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_list.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "331xLIQGJ4O2"
      },
      "outputs": [],
      "source": [
        "## sample_submission에 맞게끔 바꾸기\n",
        "user2 = []\n",
        "item2 = []\n",
        "for i_idx, arr_10 in enumerate(pred_list):\n",
        "    user2.extend([i_idx]*10)\n",
        "    item2.extend(arr_10)\n",
        "\n",
        "u2 = pd.DataFrame(user2, columns=['user'])\n",
        "i2 = pd.DataFrame(item2, columns=['item'])\n",
        "all2 = pd.concat([u2, i2], axis=1)\n",
        "\n",
        "re_p2id = dict((v, k) for k, v in profile2id.items())\n",
        "re_s2id = dict((v, k) for k, v in show2id.items())\n",
        "\n",
        "def de_numerize(tp, re_p2id, re_s2id):\n",
        "    uid2 = tp['user'].apply(lambda x: re_p2id[x])\n",
        "    sid2 = tp['item'].apply(lambda x: re_s2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid2, 'sid': sid2}, columns=['uid', 'sid'])\n",
        "\n",
        "ans2 = de_numerize(all2, re_p2id, re_s2id)\n",
        "ans2.columns = ['user', 'item']\n",
        "new_ans2 = ans2.sort_values('user')\n",
        "\n",
        "### 확인용\n",
        "# submit_data = pd.read_csv('/content/code/output/sample_submission.csv', sep='\\t')\n",
        "# sum(new_ans2.user.values == submit_data.user.values)\n",
        "new_ans2.reset_index(drop=True, inplace=True)\n",
        "new_ans2.to_csv('./code/output/answer/dir_vae.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>item</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>4370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>3156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>8961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>53464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313595</th>\n",
              "      <td>138493</td>\n",
              "      <td>33615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313596</th>\n",
              "      <td>138493</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313597</th>\n",
              "      <td>138493</td>\n",
              "      <td>2762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313598</th>\n",
              "      <td>138493</td>\n",
              "      <td>1258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313599</th>\n",
              "      <td>138493</td>\n",
              "      <td>2340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>313600 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          user   item\n",
              "0           11   4370\n",
              "1           11   3156\n",
              "2           11   8961\n",
              "3           11  53464\n",
              "4           11      2\n",
              "...        ...    ...\n",
              "313595  138493  33615\n",
              "313596  138493     32\n",
              "313597  138493   2762\n",
              "313598  138493   1258\n",
              "313599  138493   2340\n",
              "\n",
              "[313600 rows x 2 columns]"
            ]
          },
          "execution_count": 324,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_ans2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Mission2_Multi-VAE-정답.ipynb의 사본",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
